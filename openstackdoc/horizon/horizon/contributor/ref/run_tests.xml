<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/horizon/doc/source/contributor/ref/run_tests.rst">
    <section ids="the-run-tests-sh-script" names="the\ run_tests.sh\ script">
        <title>The run_tests.sh Script</title>
        <warning>
            <paragraph>This script is deprecated as of Newton (11.0), and will be removed in
                Queens (13.0), in favor of tox. The tox docs can be found at
                <reference refuri="https://tox.readthedocs.io/en/latest/">https://tox.readthedocs.io/en/latest/</reference></paragraph>
        </warning>
        <paragraph>Horizon ships with a script called <literal>run_tests.sh</literal> at the root of the
            repository. This script provides many crucial functions for the project,
            and also makes several otherwise complex tasks trivial for you as a
            developer.</paragraph>
        <section ids="first-run" names="first\ run">
            <title>First Run</title>
            <paragraph>If you start with a clean copy of the Horizon repository, the first thing
                you should do is to run <literal>./run_tests.sh</literal> from the root of the repository.
                This will do two things for you:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Set up a virtual environment for both the <literal>horizon</literal> module and
                        the <literal>openstack_dashboard</literal> project using <literal>./tools/install_venv.py</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Run the tests for both <literal>horizon</literal> and <literal>openstack_dashboard</literal> using
                        their respective environments and verify that everything is working.</paragraph>
                </list_item>
            </enumerated_list>
            <paragraph>Setting up the environment the first time can take several minutes, but only
                needs to be done once. If dependencies are added in the future, updating the
                environments will be necessary but not as time consuming.</paragraph>
        </section>
        <section ids="i-just-want-to-run-the-tests" names="i\ just\ want\ to\ run\ the\ tests!">
            <title>I just want to run the tests!</title>
            <paragraph>Running the full set of unit tests quickly and easily is the main goal of this
                script. All you need to do is:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh</literal_block>
            <paragraph>Yep, that’s it. However, for a more thorough test run you can include the
                Selenium tests by using the <literal>--with-selenium</literal> flag:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --with-selenium</literal_block>
            <paragraph>If you run horizon in a minimal installation VM, you will probably need
                the following (steps for Fedora 18 minimal installation):</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Install these packages in the VM:
                        <literal>yum install xorg-x11-xauth xorg-x11-fonts-Type1.noarch</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Install firefox in the VM:
                        <literal>yum install firefox</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Connect to the VM by <literal>ssh -X</literal>
                        (if you run <literal>set|grep DISP</literal>, you should see that the DISPLAY is set).</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Run <literal>./run_tests.sh --with-selenium</literal>.</paragraph>
                </list_item>
            </enumerated_list>
            <section ids="running-a-subset-of-tests" names="running\ a\ subset\ of\ tests">
                <title>Running a subset of tests</title>
                <paragraph>Instead of running all tests, you can specify an individual directory, file,
                    class, or method that contains test code.</paragraph>
                <paragraph>To run the tests in the <literal>horizon/test/tests/tables.py</literal> file:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh horizon.test.tests.tables</literal_block>
                <paragraph>To run the tests in the <title_reference>WorkflowsTests</title_reference> class in
                    <literal>horizon/test/tests/workflows</literal>:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh horizon.test.tests.workflows:WorkflowsTests</literal_block>
                <paragraph>To run just the <title_reference>WorkflowsTests.test_workflow_view</title_reference> test method:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh horizon.test.tests.workflows:WorkflowsTests.test_workflow_view</literal_block>
            </section>
            <section ids="running-the-integration-tests" names="running\ the\ integration\ tests">
                <title>Running the integration tests</title>
                <paragraph>The Horizon integration tests treat Horizon as a black box, and similar
                    to Tempest must be run against an existing OpenStack system. These
                    tests are not run by default.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Update the configuration file
                            <title_reference>openstack_dashboard/test/integration_tests/horizon.conf</title_reference> as
                            required (the format is similar to the Tempest configuration file).</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Run the tests with the following command:</paragraph>
                        <literal_block xml:space="preserve">$ ./run_tests.sh --integration</literal_block>
                    </list_item>
                </enumerated_list>
                <paragraph>Like for the unit tests, you can choose to only run a subset.</paragraph>
                <literal_block xml:space="preserve">$ ./run_tests.sh --integration openstack_dashboard.test.integration_tests.tests.test_login</literal_block>
            </section>
        </section>
        <section ids="using-dashboard-and-panel-templates" names="using\ dashboard\ and\ panel\ templates">
            <title>Using Dashboard and Panel Templates</title>
            <paragraph>Horizon has a set of convenient management commands for creating new
                dashboards and panels based on basic templates.</paragraph>
            <section ids="dashboards" names="dashboards">
                <title>Dashboards</title>
                <paragraph>To create a new dashboard, run the following:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh -m startdash &lt;dash_name&gt;</literal_block>
                <paragraph>This will create a directory with the given dashboard name, a <literal>dashboard.py</literal>
                    module with the basic dashboard code filled in, and various other common
                    “boilerplate” code.</paragraph>
                <paragraph>Available options:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph><literal>--target</literal>: the directory in which the dashboard files should be created.
                            Default: A new directory within the current directory.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="panels" names="panels">
                <title>Panels</title>
                <paragraph>To create a new panel, run the following:</paragraph>
                <literal_block xml:space="preserve">./run_tests -m startpanel &lt;panel_name&gt;</literal_block>
                <paragraph>This will create a directory with the given panel name, and <literal>panel.py</literal>
                    module with the basic panel code filled in, and various other common
                    “boilerplate” code.</paragraph>
                <paragraph>Available options:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph><literal>-d</literal>, <literal>--dashboard</literal>: The dotted python path to your dashboard app (the
                            module which contains the <literal>dashboard.py</literal> file.). If not specified, the
                            target dashboard should be specified in a pluggable settings file for the
                            panel.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><literal>--target</literal>: the directory in which the panel files should be created.
                            If the value is <literal>auto</literal> the panel will be created as a new directory inside
                            the dashboard module’s directory structure. Default: A new directory within
                            the current directory.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="javascript-tests" names="javascript\ tests">
                <title>JavaScript Tests</title>
                <paragraph>You can also run JavaScript unit tests using Karma.  Karma is a test
                    environment that allows for multiple test runners and reporters, including
                    such features as code coverage.  Karma allows developer to run tests live,
                    as it can watch source and test files for changes.</paragraph>
                <paragraph>The default configuration also performs coverage reports, which are saved
                    to <literal>cover/horizon/</literal> and <literal>cover/openstack_dashboard/</literal>.</paragraph>
                <paragraph>To run the Karma tests for Horizon and Dashboard using the <title_reference>run_tests.sh</title_reference>
                    script:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --karma</literal_block>
                <paragraph>To run the Karma tests for Horizon and Dashboard using <title_reference>npm</title_reference>:</paragraph>
                <literal_block xml:space="preserve">npm install # You only need to execute this once.
npm test</literal_block>
                <note>
                    <paragraph>These two methods are equivalent. The former merely executes
                        the latter.</paragraph>
                </note>
            </section>
            <section ids="javascript-code-style-checks" names="javascript\ code\ style\ checks">
                <title>JavaScript Code Style Checks</title>
                <paragraph>You can run the JavaScript code style checks, or linting, using eslint.
                    ESLint is a permissively licensed, sophisticated language parser and
                    linter that confirms both our style guidelines, and checks the code for
                    common errors that may create unexpected behavior.</paragraph>
                <paragraph>To run eslint for Horizon and Dashboard using the <title_reference>run_tests.sh</title_reference>
                    script:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --eslint</literal_block>
                <paragraph>To run eslint for Horizon and Dashboard using <title_reference>npm</title_reference>:</paragraph>
                <literal_block xml:space="preserve">npm install # You only need to execute this once.
npm run lint</literal_block>
                <note>
                    <paragraph>These two methods are equivalent. The former merely executes
                        the latter.</paragraph>
                </note>
            </section>
        </section>
        <section ids="give-me-metrics" names="give\ me\ metrics!">
            <title>Give me metrics!</title>
            <paragraph>You can generate various reports and metrics using command line arguments
                to <literal>run_tests.sh</literal>.</paragraph>
            <section ids="eslint" names="eslint">
                <title>ESLint</title>
                <paragraph>To run ESLint, a JavaScript code style checker:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --eslint</literal_block>
            </section>
            <section ids="coverage" names="coverage">
                <title>Coverage</title>
                <paragraph>To run coverage reports:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --coverage</literal_block>
                <paragraph>The reports are saved to <literal>./reports/</literal> and <literal>./coverage.xml</literal>.</paragraph>
            </section>
            <section ids="pep8" names="pep8">
                <title>PEP8</title>
                <paragraph>You can check for PEP8 violations as well:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --pep8</literal_block>
                <paragraph>The results are saved to <literal>./pep8.txt</literal>.</paragraph>
            </section>
            <section ids="pylint" names="pylint">
                <title>PyLint</title>
                <paragraph>For more detailed code analysis you can run:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --pylint</literal_block>
                <paragraph>The output will be saved in <literal>./pylint.txt</literal>.</paragraph>
            </section>
            <section ids="tab-characters" names="tab\ characters">
                <title>Tab Characters</title>
                <paragraph>For those who dislike having a mix of tab characters and spaces for indentation
                    there’s a command to check for that in Python, CSS, JavaScript and HTML files:</paragraph>
                <literal_block xml:space="preserve">./run_tests.sh --tabs</literal_block>
                <paragraph>This will output a total “tab count” and a list of the offending files.</paragraph>
            </section>
        </section>
        <section ids="running-the-development-server" names="running\ the\ development\ server">
            <title>Running the development server</title>
            <paragraph>As an added bonus, you can run Django’s development server directly from
                the root of the repository with <literal>run_tests.sh</literal> like so:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --runserver</literal_block>
            <paragraph>This is effectively just an alias for:</paragraph>
            <literal_block xml:space="preserve">./tools/with_venv.sh ./manage.py runserver</literal_block>
        </section>
        <section ids="generating-the-documentation" names="generating\ the\ documentation">
            <title>Generating the documentation</title>
            <paragraph>You can build Horizon’s documentation automatically by running:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --docs</literal_block>
            <paragraph>The output is stored in <literal>./doc/build/html/</literal>.</paragraph>
        </section>
        <section ids="updating-the-translation-files" names="updating\ the\ translation\ files">
            <title>Updating the translation files</title>
            <paragraph>You can update all of the translation files for both the <literal>horizon</literal> app and
                <literal>openstack_dashboard</literal> project with a single command:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --makemessages</literal_block>
            <paragraph>or, more compactly:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --m</literal_block>
        </section>
        <section ids="starting-clean" names="starting\ clean">
            <title>Starting clean</title>
            <paragraph>If you ever want to start clean with a new environment for Horizon, you can
                run:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --force</literal_block>
            <paragraph>That will blow away the existing environments and create new ones for you.</paragraph>
        </section>
        <section ids="non-interactive-mode" names="non-interactive\ mode">
            <title>Non-interactive Mode</title>
            <paragraph>There is an optional flag which will run the script in a non-interactive
                (and eventually less verbose) mode:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --quiet</literal_block>
            <paragraph>This will automatically take the default action for actions which would
                normally prompt for user input such as installing/updating the environment.</paragraph>
        </section>
        <section ids="environment-backups" names="environment\ backups">
            <title>Environment Backups</title>
            <paragraph>To speed up the process of doing clean checkouts, running continuous
                integration tests, etc. there are options for backing up the current
                environment and restoring from a backup:</paragraph>
            <literal_block xml:space="preserve">./run_tests.sh --restore-environment
./run_tests.sh --backup-environment</literal_block>
            <paragraph>The environment backup is stored in <literal>/tmp/.horizon_environment/</literal>.</paragraph>
        </section>
        <section ids="environment-versioning" names="environment\ versioning">
            <title>Environment Versioning</title>
            <paragraph>Horizon keeps track of changes to the environment by comparing the
                current requirements files (<literal>requirements.txt</literal> and
                <literal>test-requirements.txt</literal>) and the files last time the virtual
                environment was created or updated. If there is any difference,
                the virtual environment will be update automatically when you run
                <literal>run_tests.sh</literal>.</paragraph>
        </section>
    </section>
</document>
