<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/horizon/doc/source/contributor/topics/testing.rst">
    <target refid="topics-testing"></target>
    <section ids="testing-overview topics-testing" names="testing\ overview topics-testing">
        <title>Testing Overview</title>
        <paragraph>Having good tests in place is absolutely critical for ensuring a stable,
            maintainable codebase. Hopefully that doesn’t need any more explanation.</paragraph>
        <paragraph>However, what defines a “good” test is not always obvious, and there are
            a lot of common pitfalls that can easily shoot your test suite in the
            foot.</paragraph>
        <paragraph>If you already know everything about testing but are fed up with trying to
            debug why a specific test failed, you can skip the intro and jump
            straight to <reference internal="True" refid="debugging-unit-tests"><inline classes="std std-ref">Debugging Unit Tests</inline></reference>.</paragraph>
        <compound classes="toctree-wrapper">
            <compact_paragraph toctree="True"><bullet_list><list_item classes="toctree-l1"><compact_paragraph classes="toctree-l1"><reference anchorname="" internal="True" refuri="javascript_testing">Angular specific testing</reference></compact_paragraph></list_item></bullet_list></compact_paragraph>
        </compound>
        <section ids="an-overview-of-testing" names="an\ overview\ of\ testing">
            <title>An overview of testing</title>
            <paragraph>There are three main types of tests, each with their associated pros and cons:</paragraph>
            <section ids="unit-tests" names="unit\ tests">
                <title>Unit tests</title>
                <paragraph>These are isolated, stand-alone tests with no external dependencies. They are
                    written from the perspective of “knowing the code”, and test the assumptions
                    of the codebase and the developer.</paragraph>
                <paragraph>Pros:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph>Generally lightweight and fast.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Can be run anywhere, anytime since they have no external dependencies.</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>Cons:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph>Easy to be lax in writing them, or lazy in constructing them.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Can’t test interactions with live external services.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="functional-tests" names="functional\ tests">
                <title>Functional tests</title>
                <paragraph>These are generally also isolated tests, though sometimes they may interact
                    with other services running locally. The key difference between functional
                    tests and unit tests, however, is that functional tests are written from the
                    perspective of the user (who knows nothing about the code) and only knows
                    what they put in and what they get back. Essentially this is a higher-level
                    testing of “does the result match the spec?”.</paragraph>
                <paragraph>Pros:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph>Ensures that your code <emphasis>always</emphasis> meets the stated functional requirements.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Verifies things from an “end user” perspective, which helps to ensure
                            a high-quality experience.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Designing your code with a functional testing perspective in mind helps
                            keep a higher-level viewpoint in mind.</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>Cons:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph>Requires an additional layer of thinking to define functional requirements
                            in terms of inputs and outputs.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Often requires writing a separate set of tests and/or using a different
                            testing framework from your unit tests.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Doesn’t offer any insight into the quality or status of the underlying code,
                            only verifies that it works or it doesn’t.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="integration-tests" names="integration\ tests">
                <title>Integration Tests</title>
                <paragraph>This layer of testing involves testing all of the components that your
                    codebase interacts with or relies on in conjunction. This is equivalent to
                    “live” testing, but in a repeatable manner.</paragraph>
                <paragraph>Pros:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph>Catches <emphasis>many</emphasis> bugs that unit and functional tests will not.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Doesn’t rely on assumptions about the inputs and outputs.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Will warn you when changes in external components break your code.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Will take screenshot of the current page on test fail for easy debug</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>Cons:</paragraph>
                <bullet_list bullet="*">
                    <list_item>
                        <paragraph>Difficult and time-consuming to create a repeatable test environment.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Did I mention that setting it up is a pain?</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>Screenshot directory could be set through horizon.conf file, default value:
                    <literal>./integration_tests_screenshots</literal></paragraph>
            </section>
            <section ids="so-what-should-i-write" names="so\ what\ should\ i\ write?">
                <title>So what should I write?</title>
                <paragraph>A few simple guidelines:</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Every bug fix should have a regression test. Period.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>When writing a new feature, think about writing unit tests to verify
                            the behavior step-by-step as you write the feature. Every time you’d
                            go to run your code by hand and verify it manually, think “could I
                            write a test to do this instead?”. That way when the feature is done
                            and you’re ready to commit it you’ve already got a whole set of tests
                            that are more thorough than anything you’d write after the fact.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Write tests that hit every view in your application. Even if they
                            don’t assert a single thing about the code, it tells you that your
                            users aren’t getting fatal errors just by interacting with your code.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
        </section>
        <section ids="what-makes-a-good-unit-test" names="what\ makes\ a\ good\ unit\ test?">
            <title>What makes a good unit test?</title>
            <paragraph>Limiting our focus just to unit tests, there are a number of things you can
                do to make your unit tests as useful, maintainable, and unburdensome as
                possible.</paragraph>
            <section ids="test-data" names="test\ data">
                <title>Test data</title>
                <paragraph>Use a single, consistent set of test data. Grow it over time, but do everything
                    you can not to fragment it. It quickly becomes unmaintainable and perniciously
                    out-of-sync with reality.</paragraph>
                <paragraph>Make your test data as accurate to reality as possible. Supply <emphasis>all</emphasis> the
                    attributes of an object, provide objects in all the various states you may want
                    to test.</paragraph>
                <paragraph>If you do the first suggestion above <emphasis>first</emphasis> it makes the second one far less
                    painful. Write once, use everywhere.</paragraph>
                <paragraph>To make your life even easier, if your codebase doesn’t have a built-in
                    ORM-like function to manage your test data you can consider building (or
                    borrowing) one yourself. Being able to do simple retrieval queries on your
                    test data is incredibly valuable.</paragraph>
            </section>
            <section ids="mocking" names="mocking">
                <title>Mocking</title>
                <paragraph>Mocking is the practice of providing stand-ins for objects or pieces of code
                    you don’t need to test. While convenient, they should be used with <emphasis>extreme</emphasis>
                    caution.</paragraph>
                <paragraph>Why? Because overuse of mocks can rapidly land you in a situation where you’re
                    not testing any real code. All you’ve done is verified that your mocking
                    framework returns what you tell it to. This problem can be very tricky to
                    recognize, since you may be mocking things in <literal>setUp</literal> methods, other modules,
                    etc.</paragraph>
                <paragraph>A good rule of thumb is to mock as close to the source as possible. If you have
                    a function call that calls an external API in a view , mock out the external
                    API, not the whole function. If you mock the whole function you’ve suddenly
                    lost test coverage for an entire chunk of code <emphasis>inside</emphasis> your codebase. Cut the
                    ties cleanly right where your system ends and the external world begins.</paragraph>
                <paragraph>Similarly, don’t mock return values when you could construct a real return
                    value of the correct type with the correct attributes. You’re just adding
                    another point of potential failure by exercising your mocking framework instead
                    of real code. Following the suggestions for testing above will make this a lot
                    less burdensome.</paragraph>
            </section>
            <section ids="assertions-and-verification" names="assertions\ and\ verification">
                <title>Assertions and verification</title>
                <paragraph>Think long and hard about what you really want to verify in your unit test. In
                    particular, think about what custom logic your code executes.</paragraph>
                <paragraph>A common pitfall is to take a known test object, pass it through your code,
                    and then verify the properties of that object on the output. This is all well
                    and good, except if you’re verifying properties that were untouched by your
                    code. What you want to check are the pieces that were <emphasis>changed</emphasis>, <emphasis>added</emphasis>, or
                    <emphasis>removed</emphasis>. Don’t check the object’s id attribute unless you have reason to
                    suspect it’s not the object you started with. But if you added a new attribute
                    to it, be damn sure you verify that came out right.</paragraph>
                <paragraph>It’s also very common to avoid testing things you really care about because
                    it’s more difficult. Verifying that the proper messages were displayed to the
                    user after an action, testing for form errors, making sure exception handling
                    is tested… these types of things aren’t always easy, but they’re extremely
                    necessary.</paragraph>
                <paragraph>To that end, Horizon includes several custom assertions to make these tasks
                    easier. <reference internal="True" reftitle="openstack_dashboard.test.helpers.TestCase.assertNoFormErrors" refuri="../ref/test#openstack_dashboard.test.helpers.TestCase.assertNoFormErrors"><literal classes="xref py py-meth">assertNoFormErrors()</literal></reference>,
                    <reference internal="True" reftitle="horizon.test.helpers.TestCase.assertMessageCount" refuri="../ref/test#horizon.test.helpers.TestCase.assertMessageCount"><literal classes="xref py py-meth">assertMessageCount()</literal></reference>, and
                    <reference internal="True" reftitle="horizon.test.helpers.TestCase.assertNoMessages" refuri="../ref/test#horizon.test.helpers.TestCase.assertNoMessages"><literal classes="xref py py-meth">assertNoMessages()</literal></reference> all exist for exactly
                    these purposes. Moreover, they provide useful output when things go wrong so
                    you’re not left scratching your head wondering why your view test didn’t
                    redirect as expected when you posted a form.</paragraph>
                <target refid="debugging-unit-tests"></target>
            </section>
        </section>
        <section ids="debugging-unit-tests id1" names="debugging\ unit\ tests debugging_unit_tests">
            <title>Debugging Unit Tests</title>
            <section ids="tips-and-tricks" names="tips\ and\ tricks">
                <title>Tips and tricks</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Use <reference internal="True" reftitle="openstack_dashboard.test.helpers.TestCase.assertNoFormErrors" refuri="../ref/test#openstack_dashboard.test.helpers.TestCase.assertNoFormErrors"><literal classes="xref py py-meth">assertNoFormErrors()</literal></reference>
                            immediately after your <literal>client.post</literal> call for tests that handle form views.
                            This will immediately fail if your form POST failed due to a validation error
                            and tell you what the error was.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Use <reference internal="True" reftitle="horizon.test.helpers.TestCase.assertMessageCount" refuri="../ref/test#horizon.test.helpers.TestCase.assertMessageCount"><literal classes="xref py py-meth">assertMessageCount()</literal></reference> and
                            <reference internal="True" reftitle="horizon.test.helpers.TestCase.assertNoMessages" refuri="../ref/test#horizon.test.helpers.TestCase.assertNoMessages"><literal classes="xref py py-meth">assertNoMessages()</literal></reference> when a piece of code
                            is failing inexplicably. Since the core error handlers attach user-facing
                            error messages (and since the core logging is silenced during test runs)
                            these methods give you the dual benefit of verifying the output you expect
                            while clearly showing you the problematic error message if they fail.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Use Python’s <literal>pdb</literal> module liberally. Many people don’t realize it works
                            just as well in a test case as it does in a live view. Simply inserting
                            <literal>import pdb; pdb.set_trace()</literal> anywhere in your codebase will drop the
                            interpreter into an interactive shell so you can explore your test
                            environment and see which of your assumptions about the code isn’t,
                            in fact, flawlessly correct.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>If the error is in the Selenium test suite, you’re likely getting very little
                            information about the error. To increase the information provided to you,
                            edit <literal>horizon/test/settings.py</literal> to set <literal>DEBUG = True</literal> and set the logging
                            level to ‘DEBUG’ for the default ‘test’ logger. Also, add a logger config
                            for Django:</paragraph>
                        <literal_block xml:space="preserve">     },
     'loggers': {
+        'django': {
+            'handlers': ['test'],
+            'propagate': False,
+        },
         'django.db.backends': {</literal_block>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="coverage-reports" names="coverage\ reports">
                <title>Coverage reports</title>
                <paragraph>It is possible for tests to fail on your patch due to the npm-run-test not
                    passing the minimum threshold. This is not necessarily related directly to the
                    functions in the patch that have failed, but more that there are not enough
                    tests across horizon that are related to your patch.</paragraph>
                <paragraph>The coverage reports may be found in the ‘cover’ directory. There’s a
                    subdirectory for horizon and openstack_dashboard, and then under a directory
                    for the browser used to run the tests you should find an <literal>index.html</literal>. This
                    can then be viewed to see the coverage details.</paragraph>
                <paragraph>In this scenario you may need to submit a secondary patch to address test
                    coverage for another function within horizon to ensure tests rise above the
                    coverage threshold and your original patch can pass the necessary tests.</paragraph>
            </section>
            <section ids="common-pitfalls" names="common\ pitfalls">
                <title>Common pitfalls</title>
                <paragraph>There are a number of typical (and non-obvious) ways to break the unit tests.
                    Some common things to look for:</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Make sure you stub out the method exactly as it’s called in the code
                            being tested. For example, if your real code calls
                            <literal>api.keystone.tenant_get</literal>, stubbing out <literal>api.tenant_get</literal> (available
                            for legacy reasons) will fail.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>When defining the expected input to a stubbed call, make sure the
                            arguments are <emphasis>identical</emphasis>, this includes <literal>str</literal> vs. <literal>int</literal> differences.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Make sure your test data are completely in line with the expected inputs.
                            Again, <literal>str</literal> vs. <literal>int</literal> or missing properties on test objects will
                            kill your tests.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Make sure there’s nothing amiss in your templates (particularly the
                            <literal>{% url %}</literal> tag and its arguments). This often comes up when refactoring
                            views or renaming context variables. It can easily result in errors that
                            you might not stumble across while clicking around the development server.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Make sure you’re not redirecting to views that no longer exist, e.g.
                            the <literal>index</literal> view for a panel that got combined (such as instances &amp;
                            volumes).</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Make sure your mock calls are in order before calling <literal>mox.ReplayAll</literal>.
                            The order matters.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Make sure you repeat any stubbed out method calls that happen more than
                            once. They don’t automatically repeat, you have to explicitly define them.
                            While this is a nuisance, it makes you acutely aware of how many API
                            calls are involved in a particular function.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="understanding-the-output-from-mox" names="understanding\ the\ output\ from\ mox">
                <title>Understanding the output from <literal>mox</literal></title>
                <paragraph>Horizon uses <literal>mox</literal> as its mocking framework of choice, and while it
                    offers many nice features, its output when a test fails can be quite
                    mysterious.</paragraph>
                <section ids="unexpected-method-call" names="unexpected\ method\ call">
                    <title>Unexpected Method Call</title>
                    <paragraph>This occurs when you stubbed out a piece of code, and it was subsequently
                        called in a way that you didn’t specify it would be. There are two reasons
                        this tends to come up:</paragraph>
                    <enumerated_list enumtype="arabic" prefix="" suffix=".">
                        <list_item>
                            <paragraph>You defined the expected call, but a subtle difference crept in. This
                                may be a string versus integer difference, a string versus unicode
                                difference, a slightly off date/time, or passing a name instead of an id.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>The method is actually being called <emphasis>multiple times</emphasis>. Since mox uses
                                a call stack internally, it simply pops off the expected method calls to
                                verify them. That means once a call is used once, it’s gone. An easy way
                                to see if this is the case is simply to copy and paste your method call a
                                second time to see if the error changes. If it does, that means your method
                                is being called more times than you think it is.</paragraph>
                        </list_item>
                    </enumerated_list>
                </section>
                <section ids="expected-method-never-called" names="expected\ method\ never\ called">
                    <title>Expected Method Never Called</title>
                    <paragraph>This one is the opposite of the unexpected method call. This one means you
                        told mox to expect a call and it didn’t happen. This is almost always the
                        result of an error in the conditions of the test. Using the
                        <reference internal="True" reftitle="openstack_dashboard.test.helpers.TestCase.assertNoFormErrors" refuri="../ref/test#openstack_dashboard.test.helpers.TestCase.assertNoFormErrors"><literal classes="xref py py-meth">assertNoFormErrors()</literal></reference> and
                        <reference internal="True" reftitle="horizon.test.helpers.TestCase.assertMessageCount" refuri="../ref/test#horizon.test.helpers.TestCase.assertMessageCount"><literal classes="xref py py-meth">assertMessageCount()</literal></reference> will make it readily
                        apparent what the problem is in the majority of cases. If not, then use <literal>pdb</literal>
                        and start interrupting the code flow to see where things are getting off track.</paragraph>
                </section>
            </section>
        </section>
    </section>
</document>
