<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/cinder/doc/source/configuration/block-storage/drivers/ibm-gpfs-volume-driver.rst">
    <section ids="ibm-spectrum-scale-volume-driver" names="ibm\ spectrum\ scale\ volume\ driver">
        <title>IBM Spectrum Scale volume driver</title>
        <paragraph>IBM Spectrum Scale is a flexible software-defined storage that can be
            deployed as high performance file storage or a cost optimized
            large-scale content repository. IBM Spectrum Scale, previously known as
            IBM General Parallel File System (GPFS), is designed to scale performance
            and capacity with no bottlenecks. IBM Spectrum Scale is a cluster file system
            that provides concurrent access to file systems from multiple nodes. The
            storage provided by these nodes can be direct attached, network attached,
            SAN attached, or a combination of these methods. Spectrum Scale provides
            many features beyond common data access, including data replication,
            policy based storage management, and space efficient file snapshot and
            clone operations.</paragraph>
        <section ids="how-the-spectrum-scale-volume-driver-works" names="how\ the\ spectrum\ scale\ volume\ driver\ works">
            <title>How the Spectrum Scale volume driver works</title>
            <paragraph>The Spectrum Scale volume driver, named <literal>gpfs.py</literal>, enables the use of
                Spectrum Scale in a fashion similar to that of the NFS driver. With
                the Spectrum Scale driver, instances do not actually access a storage
                device at the block level. Instead, volume backing files are created
                in a Spectrum Scale file system and mapped to instances, which emulate
                a block device.</paragraph>
            <note>
                <paragraph>Spectrum Scale must be installed and cluster has to be created on the
                    storage nodes in the OpenStack environment. A file system must also be
                    created and mounted on these nodes before configuring the cinder service
                    to use Spectrum Scale storage.For more details, please refer to
                    <reference name="Spectrum Scale product documentation" refuri="https://ibm.biz/Bdi84g">Spectrum Scale product documentation</reference><target ids="spectrum-scale-product-documentation" names="spectrum\ scale\ product\ documentation" refuri="https://ibm.biz/Bdi84g"></target>.</paragraph>
            </note>
            <paragraph>Optionally, the Image service can be configured to store glance images
                in a Spectrum Scale file system. When a Block Storage volume is created
                from an image, if both image data and volume data reside in the same
                Spectrum Scale file system, the data from image file is moved efficiently
                to the volume file using copy-on-write optimization strategy.</paragraph>
        </section>
        <section ids="supported-operations" names="supported\ operations">
            <title>Supported operations</title>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>Create, delete, attach, and detach volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create, delete volume snapshots.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create a volume from a snapshot.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create cloned volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Extend a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Migrate a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Retype a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create, delete consistency groups.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create, delete consistency group snapshots.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Copy an image to a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Copy a volume to an image.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Backup and restore volumes.</paragraph>
                </list_item>
            </bullet_list>
        </section>
        <section ids="driver-configurations" names="driver\ configurations">
            <title>Driver configurations</title>
            <paragraph>The Spectrum Scale volume driver supports three modes of deployment.</paragraph>
            <section ids="mode-1-pervasive-spectrum-scale-client" names="mode\ 1\ –\ pervasive\ spectrum\ scale\ client">
                <title>Mode 1 – Pervasive Spectrum Scale Client</title>
                <paragraph>When Spectrum Scale is running on compute nodes as well as on the cinder node.
                    For example, Spectrum Scale filesystem is available to both Compute and
                    Block Storage services as a local filesystem.</paragraph>
                <paragraph>To use Spectrum Scale driver in this deployment mode, set the <literal>volume_driver</literal>
                    in the <literal>cinder.conf</literal> as:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">volume_driver = cinder.volume.drivers.ibm.gpfs.GPFSDriver</literal_block>
                <paragraph>The following table contains the configuration options supported by the
                    Spectrum Scale driver in this deployment mode.</paragraph>
                <comment xml:space="preserve">Warning: Do not edit this file. It is automatically generated from the
software project's code and your changes will be overwritten.

The tool to generate this file lives in openstack-doc-tools repository.

Please make any changes needed in the code, then run the
autogenerate-config-doc tool from the openstack-doc-tools repository, or
ask for help on the documentation mailing list, IRC channel or meeting.</comment>
                <target refid="cinder-ibm-gpfs"></target>
                <table classes="config-ref-table" ids="id1 cinder-ibm-gpfs" names="cinder-ibm_gpfs">
                    <title>Description of Spectrum Scale volume driver configuration options</title>
                    <tgroup cols="2">
                        <colspec colwidth="50"></colspec>
                        <colspec colwidth="50"></colspec>
                        <thead>
                            <row>
                                <entry>
                                    <paragraph>Configuration option = Default value</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>Description</paragraph>
                                </entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>
                                    <paragraph><strong>[DEFAULT]</strong></paragraph>
                                </entry>
                                <entry>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_images_dir</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the path of the Image service repository in GPFS. Leave undefined if not storing images in GPFS.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_images_share_mode</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the type of image copy to be used. Set this when the Image service repository also uses GPFS so that image files can be transferred efficiently from the Image service to the Block Storage service. There are two valid values: “copy” specifies that a full copy of the image is made; “copy_on_write” specifies that copy-on-write optimization strategy is used and unmodified blocks of the image file are shared efficiently.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_max_clone_depth</literal> = <literal>0</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Integer) Specifies an upper limit on the number of indirections required to reach a specific block due to snapshots or clones. A lengthy chain of copy-on-write snapshots or clones can have a negative impact on performance, but improves space utilization. 0 indicates unlimited clone depth.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_mount_point_base</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the path of the GPFS directory where Block Storage volume and snapshot files are stored.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_sparse_volumes</literal> = <literal>True</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Boolean) Specifies that volumes are created as sparse files which initially consume no space. If set to False, the volume is created as a fully allocated file, in which case, creation may take a significantly longer time.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_storage_pool</literal> = <literal>system</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the storage pool that volumes are assigned to. By default, the system storage pool is used.</paragraph>
                                </entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <note>
                    <paragraph>The <literal>gpfs_images_share_mode</literal> flag is only valid if the Image
                        Service is configured to use Spectrum Scale with the
                        <literal>gpfs_images_dir</literal> flag. When the value of this flag is
                        <literal>copy_on_write</literal>, the paths specified by the <literal>gpfs_mount_point_base</literal>
                        and <literal>gpfs_images_dir</literal> flags must both reside in the same GPFS
                        file system and in the same GPFS file set.</paragraph>
                </note>
            </section>
            <section ids="mode-2-remote-spectrum-scale-driver-with-local-compute-access" names="mode\ 2\ –\ remote\ spectrum\ scale\ driver\ with\ local\ compute\ access">
                <title>Mode 2 – Remote Spectrum Scale Driver with Local Compute Access</title>
                <paragraph>When Spectrum Scale is running on compute nodes, but not on the Block Storage
                    node. For example, Spectrum Scale filesystem is only available to Compute
                    service as Local filesystem where as Block Storage service accesses Spectrum
                    Scale remotely. In this case, <literal>cinder-volume</literal> service running Spectrum Scale
                    driver access storage system over SSH and creates volume backing files to make
                    them available on the compute nodes. This mode is typically deployed when the
                    cinder and glance services are running inside a Linux container. The container
                    host should have Spectrum Scale client running and GPFS filesystem mount path
                    should be bind mounted into the Linux containers.</paragraph>
                <note>
                    <paragraph>Note that the user IDs present in the containers should match as that in the
                        host machines. For example, the containers running cinder and glance
                        services should be priviledged containers.</paragraph>
                </note>
                <paragraph>To use Spectrum Scale driver in this deployment mode, set the <literal>volume_driver</literal>
                    in the <literal>cinder.conf</literal> as:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">volume_driver = cinder.volume.drivers.ibm.gpfs.GPFSRemoteDriver</literal_block>
                <paragraph>The following table contains the configuration options supported by the
                    Spectrum Scale driver in this deployment mode.</paragraph>
                <comment xml:space="preserve">Warning: Do not edit this file. It is automatically generated from the
software project's code and your changes will be overwritten.

The tool to generate this file lives in openstack-doc-tools repository.

Please make any changes needed in the code, then run the
autogenerate-config-doc tool from the openstack-doc-tools repository, or
ask for help on the documentation mailing list, IRC channel or meeting.</comment>
                <target refid="cinder-ibm-gpfs-remote"></target>
                <table classes="config-ref-table" ids="id2 cinder-ibm-gpfs-remote" names="cinder-ibm_gpfs_remote">
                    <title>Description of Spectrum Scale Remote volume driver configuration options</title>
                    <tgroup cols="2">
                        <colspec colwidth="50"></colspec>
                        <colspec colwidth="50"></colspec>
                        <thead>
                            <row>
                                <entry>
                                    <paragraph>Configuration option = Default value</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>Description</paragraph>
                                </entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>
                                    <paragraph><strong>[DEFAULT]</strong></paragraph>
                                </entry>
                                <entry>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_hosts</literal> =</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(List) Comma-separated list of IP address or hostnames of GPFS nodes.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_hosts_key_file</literal> = <literal>$state_path/ssh_known_hosts</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) File containing SSH host keys for the gpfs nodes with which driver needs to communicate. Default=$state_path/ssh_known_hosts</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_images_dir</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the path of the Image service repository in GPFS. Leave undefined if not storing images in GPFS.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_images_share_mode</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the type of image copy to be used. Set this when the Image service repository also uses GPFS so that image files can be transferred efficiently from the Image service to the Block Storage service. There are two valid values: “copy” specifies that a full copy of the image is made; “copy_on_write” specifies that copy-on-write optimization strategy is used and unmodified blocks of the image file are shared efficiently.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_max_clone_depth</literal> = <literal>0</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Integer) Specifies an upper limit on the number of indirections required to reach a specific block due to snapshots or clones. A lengthy chain of copy-on-write snapshots or clones can have a negative impact on performance, but improves space utilization. 0 indicates unlimited clone depth.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_mount_point_base</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the path of the GPFS directory where Block Storage volume and snapshot files are stored.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_private_key</literal> =</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Filename of private key to use for SSH authentication.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_sparse_volumes</literal> = <literal>True</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Boolean) Specifies that volumes are created as sparse files which initially consume no space. If set to False, the volume is created as a fully allocated file, in which case, creation may take a significantly longer time.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_ssh_port</literal> = <literal>22</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Port number) SSH port to use.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_storage_pool</literal> = <literal>system</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the storage pool that volumes are assigned to. By default, the system storage pool is used.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_strict_host_key_policy</literal> = <literal>False</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Boolean) Option to enable strict gpfs host key checking while connecting to gpfs nodes. Default=False</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_user_login</literal> = <literal>root</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Username for GPFS nodes.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_user_password</literal> =</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Password for GPFS node user.</paragraph>
                                </entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <note>
                    <paragraph>The <literal>gpfs_images_share_mode</literal> flag is only valid if the Image
                        Service is configured to use Spectrum Scale with the
                        <literal>gpfs_images_dir</literal> flag. When the value of this flag is
                        <literal>copy_on_write</literal>, the paths specified by the <literal>gpfs_mount_point_base</literal>
                        and <literal>gpfs_images_dir</literal> flags must both reside in the same GPFS
                        file system and in the same GPFS file set.</paragraph>
                </note>
            </section>
            <section ids="mode-3-remote-spectrum-scale-access" names="mode\ 3\ –\ remote\ spectrum\ scale\ access">
                <title>Mode 3 – Remote Spectrum Scale Access</title>
                <paragraph>When both Compute and Block Storage nodes are not running Spectrum Scale
                    software and do not have access to Spectrum Scale file system directly as
                    local filesystem. In this case, we create an NFS export on the volume path
                    and make it available on the cinder node and on compute nodes.</paragraph>
                <paragraph>Optionally, if one wants to use the copy-on-write optimization to create
                    bootable volumes from glance images, one need to also export the glance
                    images path and mount it on the nodes where glance and cinder services
                    are running. The cinder and glance services will access the GPFS
                    filesystem through NFS.</paragraph>
                <paragraph>To use Spectrum Scale driver in this deployment mode, set the <literal>volume_driver</literal>
                    in the <literal>cinder.conf</literal> as:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">volume_driver = cinder.volume.drivers.ibm.gpfs.GPFSNFSDriver</literal_block>
                <paragraph>The following table contains the configuration options supported by the
                    Spectrum Scale driver in this deployment mode.</paragraph>
                <comment xml:space="preserve">Warning: Do not edit this file. It is automatically generated from the
software project's code and your changes will be overwritten.

The tool to generate this file lives in openstack-doc-tools repository.

Please make any changes needed in the code, then run the
autogenerate-config-doc tool from the openstack-doc-tools repository, or
ask for help on the documentation mailing list, IRC channel or meeting.</comment>
                <target refid="cinder-ibm-gpfs-nfs"></target>
                <table classes="config-ref-table" ids="id3 cinder-ibm-gpfs-nfs" names="cinder-ibm_gpfs_nfs">
                    <title>Description of Spectrum Scale NFS volume driver configuration options</title>
                    <tgroup cols="2">
                        <colspec colwidth="50"></colspec>
                        <colspec colwidth="50"></colspec>
                        <thead>
                            <row>
                                <entry>
                                    <paragraph>Configuration option = Default value</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>Description</paragraph>
                                </entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>
                                    <paragraph><strong>[DEFAULT]</strong></paragraph>
                                </entry>
                                <entry>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_images_dir</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the path of the Image service repository in GPFS. Leave undefined if not storing images in GPFS.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_images_share_mode</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the type of image copy to be used. Set this when the Image service repository also uses GPFS so that image files can be transferred efficiently from the Image service to the Block Storage service. There are two valid values: “copy” specifies that a full copy of the image is made; “copy_on_write” specifies that copy-on-write optimization strategy is used and unmodified blocks of the image file are shared efficiently.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_max_clone_depth</literal> = <literal>0</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Integer) Specifies an upper limit on the number of indirections required to reach a specific block due to snapshots or clones. A lengthy chain of copy-on-write snapshots or clones can have a negative impact on performance, but improves space utilization. 0 indicates unlimited clone depth.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_mount_point_base</literal> = <literal>None</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the path of the GPFS directory where Block Storage volume and snapshot files are stored.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_sparse_volumes</literal> = <literal>True</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Boolean) Specifies that volumes are created as sparse files which initially consume no space. If set to False, the volume is created as a fully allocated file, in which case, creation may take a significantly longer time.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>gpfs_storage_pool</literal> = <literal>system</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Specifies the storage pool that volumes are assigned to. By default, the system storage pool is used.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nas_host</literal> =</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) IP address or Hostname of NAS system.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nas_login</literal> = <literal>admin</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) User name to connect to NAS system.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nas_password</literal> =</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Password to connect to NAS system.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nas_private_key</literal> =</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Filename of private key to use for SSH authentication.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nas_ssh_port</literal> = <literal>22</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(Port number) SSH port to use to connect to NAS system.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nfs_mount_point_base</literal> = <literal>$state_path/mnt</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) Base dir containing mount points for NFS shares.</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph><literal>nfs_shares_config</literal> = <literal>/etc/cinder/nfs_shares</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>(String) File with the list of available NFS shares.</paragraph>
                                </entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <paragraph>Additionally, all the options of the base NFS driver are applicable
                    for GPFSNFSDriver. The above table lists the basic configuration
                    options which are needed for initialization of the driver.</paragraph>
                <note>
                    <paragraph>The <literal>gpfs_images_share_mode</literal> flag is only valid if the Image
                        Service is configured to use Spectrum Scale with the
                        <literal>gpfs_images_dir</literal> flag. When the value of this flag is
                        <literal>copy_on_write</literal>, the paths specified by the <literal>gpfs_mount_point_base</literal>
                        and <literal>gpfs_images_dir</literal> flags must both reside in the same GPFS
                        file system and in the same GPFS file set.</paragraph>
                </note>
            </section>
        </section>
        <section ids="volume-creation-options" names="volume\ creation\ options">
            <title>Volume creation options</title>
            <paragraph>It is possible to specify additional volume configuration options on a
                per-volume basis by specifying volume metadata. The volume is created
                using the specified options. Changing the metadata after the volume is
                created has no effect. The following table lists the volume creation
                options supported by the GPFS volume driver.</paragraph>
            <table classes="colwidths-given" ids="id4">
                <title><strong>Volume Create Options for Spectrum Scale Volume Drivers</strong></title>
                <tgroup cols="2">
                    <colspec colwidth="10"></colspec>
                    <colspec colwidth="25"></colspec>
                    <thead>
                        <row>
                            <entry>
                                <paragraph>Metadata Item Name</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Description</paragraph>
                            </entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                <paragraph>fstype</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Specifies whether to create a file system or a swap area on the new volume. If fstype=swap is specified, the mkswap command is used to create a swap area. Otherwise the mkfs command is passed the specified file system type, for example ext3, ext4 or ntfs.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>fslabel</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Sets the file system label for the file system specified by fstype option. This value is only used if fstype is specified.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>data_pool_name</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Specifies the GPFS storage pool to which the volume is to be assigned. Note: The GPFS storage pool must already have been created.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>replicas</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Specifies how many copies of the volume file to create. Valid values are 1, 2, and, for Spectrum Scale V3.5.0.7 and later, 3. This value cannot be greater than the value of the MaxDataReplicasattribute of the file system.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>dio</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Enables or disables the Direct I/O caching policy for the volume file. Valid values are yes and no.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>write_affinity_depth</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Specifies the allocation policy to be used for the volume file. Note: This option only works if allow-write-affinity is set for the GPFS data pool.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>block_group_factor</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Specifies how many blocks are laid out sequentially in the volume file to behave as a single large block. Note: This option only works if allow-write-affinity is set for the GPFS data pool.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>write_affinity_failure_group</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Specifies the range of nodes (in GPFS shared nothing architecture) where replicas of blocks in the volume file are to be written. See Spectrum Scale documentation for more details about this option.</paragraph>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <paragraph>This example shows the creation of a 50GB volume with an <literal>ext4</literal> file
                system labeled <literal>newfs</literal> and direct IO enabled:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume create --property fstype=ext4 fslabel=newfs dio=yes \
  --size 50 VOLUME</literal_block>
            <paragraph>Note that if the metadata for the volume is changed later, the changes
                do not reflect in the backend. User will have to manually change the
                volume attributes corresponding to metadata on Spectrum Scale filesystem.</paragraph>
        </section>
        <section ids="operational-notes-for-gpfs-driver" names="operational\ notes\ for\ gpfs\ driver">
            <title>Operational notes for GPFS driver</title>
            <paragraph>Volume snapshots are implemented using the GPFS file clone feature.
                Whenever a new snapshot is created, the snapshot file is efficiently
                created as a read-only clone parent of the volume, and the volume file
                uses copy-on-write optimization strategy to minimize data movement.</paragraph>
            <paragraph>Similarly when a new volume is created from a snapshot or from an
                existing volume, the same approach is taken. The same approach is also
                used when a new volume is created from an Image service image, if the
                source image is in raw format, and <literal>gpfs_images_share_mode</literal> is set to
                <literal>copy_on_write</literal>.</paragraph>
            <paragraph>The Spectrum Scale driver supports encrypted volume back end feature.
                To encrypt a volume at rest, specify the extra specification
                <literal>gpfs_encryption_rest = True</literal>.</paragraph>
        </section>
    </section>
</document>
