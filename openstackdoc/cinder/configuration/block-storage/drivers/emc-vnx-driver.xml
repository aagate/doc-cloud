<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/cinder/doc/source/configuration/block-storage/drivers/emc-vnx-driver.rst">
    <section ids="dell-emc-vnx-driver" names="dell\ emc\ vnx\ driver">
        <title>Dell EMC VNX driver</title>
        <paragraph>EMC VNX driver interacts with configured VNX array. It supports
            both iSCSI and FC protocol.</paragraph>
        <paragraph>The VNX cinder driver performs the volume operations by
            executing Navisphere CLI (NaviSecCLI) which is a command-line interface used
            for management, diagnostics, and reporting functions for VNX. It also
            supports both iSCSI and FC protocol.</paragraph>
        <section ids="system-requirements" names="system\ requirements">
            <title>System requirements</title>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>VNX Operational Environment for Block version 5.32 or higher.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>VNX Snapshot and Thin Provisioning license should be activated for VNX.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Python library <literal>storops</literal> version 0.5.7 or higher to interact with VNX.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Navisphere CLI v7.32 or higher is installed along with the driver.</paragraph>
                </list_item>
            </bullet_list>
        </section>
        <section ids="supported-operations" names="supported\ operations">
            <title>Supported operations</title>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>Create, delete, attach, and detach volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create, list, and delete volume snapshots.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create a volume from a snapshot.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Copy an image to a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Clone a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Extend a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Migrate a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Retype a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Get volume statistics.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create and delete consistency groups.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create, list, and delete consistency group snapshots.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Modify consistency groups.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Efficient non-disruptive volume backup.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create a cloned consistency group.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create a consistency group from consistency group snapshots.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Replication v2.1 support.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Generic Group support.</paragraph>
                </list_item>
            </bullet_list>
        </section>
        <section ids="preparation" names="preparation">
            <title>Preparation</title>
            <paragraph>This section contains instructions to prepare the Block Storage nodes to
                use the EMC VNX driver. You should install the Navisphere CLI and ensure you
                have correct zoning configurations.</paragraph>
            <section ids="install-navisphere-cli" names="install\ navisphere\ cli">
                <title>Install Navisphere CLI</title>
                <paragraph>Navisphere CLI needs to be installed on all Block Storage nodes within
                    an OpenStack deployment. You need to download different versions for
                    different platforms:</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>For Ubuntu x64, DEB is available at <reference name="EMC OpenStack Github" refuri="https://github.com/emc-openstack/naviseccli">EMC OpenStack
                                Github</reference><target ids="emc-openstack-github" names="emc\ openstack\ github" refuri="https://github.com/emc-openstack/naviseccli"></target>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>For all other variants of Linux, Navisphere CLI is available at
                            <reference name="Downloads for VNX2 Series" refuri="https://support.emc.com/downloads/36656_VNX2-Series">Downloads for VNX2
                                Series</reference><target ids="downloads-for-vnx2-series" names="downloads\ for\ vnx2\ series" refuri="https://support.emc.com/downloads/36656_VNX2-Series"></target> or
                            <reference name="Downloads for VNX1 Series" refuri="https://support.emc.com/downloads/12781_VNX1-Series">Downloads for VNX1
                                Series</reference><target ids="downloads-for-vnx1-series" names="downloads\ for\ vnx1\ series" refuri="https://support.emc.com/downloads/12781_VNX1-Series"></target>.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="install-python-library-storops" names="install\ python\ library\ storops">
                <title>Install Python library storops</title>
                <paragraph><literal>storops</literal> is a Python library that interacts with VNX array through
                    Navisphere CLI.
                    Use the following command to install the <literal>storops</literal> library:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ pip install storops</literal_block>
            </section>
            <section ids="check-array-software" names="check\ array\ software">
                <title>Check array software</title>
                <paragraph>Make sure your have the following software installed for certain features:</paragraph>
                <table>
                    <tgroup cols="2">
                        <colspec colwidth="44"></colspec>
                        <colspec colwidth="21"></colspec>
                        <thead>
                            <row>
                                <entry>
                                    <paragraph>Feature</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>Software Required</paragraph>
                                </entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>
                                    <paragraph>All</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>ThinProvisioning</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph>All</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>VNXSnapshots</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph>FAST cache support</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>FASTCache</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph>Create volume with type <literal>compressed</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>Compression</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph>Create volume with type <literal>deduplicated</literal></paragraph>
                                </entry>
                                <entry>
                                    <paragraph>Deduplication</paragraph>
                                </entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <paragraph><strong>Required software</strong></paragraph>
                <paragraph>You can check the status of your array software in the <inline classes="guilabel" rawtext=":guilabel:`Software`">Software</inline>
                    page of <inline classes="guilabel" rawtext=":guilabel:`Storage System Properties`">Storage System Properties</inline>. Here is how it looks like:</paragraph>
                <figure>
                    <image candidates="{'*': 'configuration/block-storage/drivers/../../figures/emc-enabler.png'}" uri="configuration/block-storage/drivers/../../figures/emc-enabler.png"></image>
                </figure>
            </section>
            <section ids="network-configuration" names="network\ configuration">
                <title>Network configuration</title>
                <paragraph>For the FC Driver, FC zoning is properly configured between the hosts and
                    the VNX. Check <reference internal="True" refid="register-fc-port-with-vnx"><inline classes="std std-ref">Register FC port with VNX</inline></reference> for reference.</paragraph>
                <paragraph>For the iSCSI Driver, make sure your VNX iSCSI port is accessible by
                    your hosts. Check <reference internal="True" refid="register-iscsi-port-with-vnx"><inline classes="std std-ref">Register iSCSI port with VNX</inline></reference> for reference.</paragraph>
                <paragraph>You can use <literal>initiator_auto_registration = True</literal> configuration to avoid
                    registering the ports manually. Check the detail of the configuration in
                    <reference internal="True" refid="emc-vnx-conf"><inline classes="std std-ref">Back-end configuration</inline></reference> for reference.</paragraph>
                <paragraph>If you are trying to setup multipath, refer to <reference internal="True" refid="multipath-setup"><inline classes="std std-ref">Multipath setup</inline></reference>.</paragraph>
                <target refid="emc-vnx-conf"></target>
            </section>
        </section>
        <section ids="back-end-configuration emc-vnx-conf" names="back-end\ configuration emc-vnx-conf">
            <title>Back-end configuration</title>
            <paragraph>Make the following changes in the <literal>/etc/cinder/cinder.conf</literal> file.</paragraph>
            <section ids="minimum-configuration" names="minimum\ configuration">
                <title>Minimum configuration</title>
                <paragraph>Here is a sample of minimum back-end configuration. See the following sections
                    for the detail of each option.
                    Set <literal>storage_protocol = iscsi</literal> if iSCSI protocol is used.</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
enabled_backends = vnx_array1

[vnx_array1]
san_ip = 10.10.72.41
san_login = sysadmin
san_password = sysadmin
naviseccli_path = /opt/Navisphere/bin/naviseccli
volume_driver = cinder.volume.drivers.dell_emc.vnx.driver.VNXDriver
initiator_auto_registration = True
storage_protocol = fc</literal_block>
            </section>
            <section ids="multiple-back-end-configuration" names="multiple\ back-end\ configuration">
                <title>Multiple back-end configuration</title>
                <paragraph>Here is a sample of a minimum back-end configuration. See following sections
                    for the detail of each option.
                    Set <literal>storage_protocol = iscsi</literal> if iSCSI protocol is used.</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
enabled_backends = backendA, backendB

[backendA]
storage_vnx_pool_names = Pool_01_SAS, Pool_02_FLASH
san_ip = 10.10.72.41
storage_vnx_security_file_dir = /etc/secfile/array1
naviseccli_path = /opt/Navisphere/bin/naviseccli
volume_driver = cinder.volume.drivers.dell_emc.vnx.driver.VNXDriver
initiator_auto_registration = True
storage_protocol = fc

[backendB]
storage_vnx_pool_names = Pool_02_SAS
san_ip = 10.10.26.101
san_login = username
san_password = password
naviseccli_path = /opt/Navisphere/bin/naviseccli
volume_driver = cinder.volume.drivers.dell_emc.vnx.driver.VNXDriver
initiator_auto_registration = True
storage_protocol = fc</literal_block>
                <paragraph>The value of the option <literal>storage_protocol</literal> can be either <literal>fc</literal> or <literal>iscsi</literal>,
                    which is case insensitive.</paragraph>
                <paragraph>For more details on multiple back ends, see <reference name="Configure multiple-storage back ends" refuri="https://docs.openstack.org/admin-guide/blockstorage-multi-backend.html">Configure multiple-storage
                        back ends</reference><target ids="configure-multiple-storage-back-ends" names="configure\ multiple-storage\ back\ ends" refuri="https://docs.openstack.org/admin-guide/blockstorage-multi-backend.html"></target></paragraph>
            </section>
            <section ids="required-configurations" names="required\ configurations">
                <title>Required configurations</title>
                <paragraph><strong>IP of the VNX Storage Processors</strong></paragraph>
                <paragraph>Specify SP A or SP B IP to connect:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">san_ip = &lt;IP of VNX Storage Processor&gt;</literal_block>
                <paragraph><strong>VNX login credentials</strong></paragraph>
                <paragraph>There are two ways to specify the credentials.</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Use plain text username and password.</paragraph>
                        <paragraph>Supply for plain username and password:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">san_login = &lt;VNX account with administrator role&gt;
san_password = &lt;password for VNX account&gt;
storage_vnx_authentication_type = global</literal_block>
                        <paragraph>Valid values for <literal>storage_vnx_authentication_type</literal> are: <literal>global</literal>
                            (default), <literal>local</literal>, and <literal>ldap</literal>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Use Security file.</paragraph>
                        <paragraph>This approach avoids the plain text password in your cinder
                            configuration file. Supply a security file as below:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">storage_vnx_security_file_dir = &lt;path to security file&gt;</literal_block>
                    </list_item>
                </bullet_list>
                <paragraph>Check Unisphere CLI user guide or <reference internal="True" refid="authenticate-by-security-file"><inline classes="std std-ref">Authenticate by security file</inline></reference>
                    for how to create a security file.</paragraph>
                <paragraph><strong>Path to your Unisphere CLI</strong></paragraph>
                <paragraph>Specify the absolute path to your naviseccli:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">naviseccli_path = /opt/Navisphere/bin/naviseccli</literal_block>
                <paragraph><strong>Driver’s storage protocol</strong></paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>For the FC Driver, add the following option:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">volume_driver = cinder.volume.drivers.dell_emc.vnx.driver.VNXDriver
storage_protocol = fc</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>For iSCSI Driver, add the following option:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">volume_driver = cinder.volume.drivers.dell_emc.vnx.driver.VNXDriver
storage_protocol = iscsi</literal_block>
                    </list_item>
                </bullet_list>
            </section>
        </section>
        <section ids="optional-configurations" names="optional\ configurations">
            <title>Optional configurations</title>
            <section ids="vnx-pool-names" names="vnx\ pool\ names">
                <title>VNX pool names</title>
                <paragraph>Specify the list of pools to be managed, separated by commas. They should
                    already exist in VNX.</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">storage_vnx_pool_names = pool 1, pool 2</literal_block>
                <paragraph>If this value is not specified, all pools of the array will be used.</paragraph>
                <paragraph><strong>Initiator auto registration</strong></paragraph>
                <paragraph>When <literal>initiator_auto_registration</literal> is set to <literal>True</literal>, the driver will
                    automatically register initiators to all working target ports of the VNX array
                    during volume attaching (The driver will skip those initiators that have
                    already been registered) if the option <literal>io_port_list</literal> is not specified in
                    the <literal>cinder.conf</literal> file.</paragraph>
                <paragraph>If the user wants to register the initiators with some specific ports but not
                    register with the other ports, this functionality should be disabled.</paragraph>
                <paragraph>When a comma-separated list is given to <literal>io_port_list</literal>, the driver will only
                    register the initiator to the ports specified in the list and only return
                    target port(s) which belong to the target ports in the <literal>io_port_list</literal> instead
                    of all target ports.</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Example for FC ports:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">io_port_list = a-1,B-3</literal_block>
                        <paragraph><literal>a</literal> or <literal>B</literal> is <emphasis>Storage Processor</emphasis>, number <literal>1</literal> and <literal>3</literal> are
                            <emphasis>Port ID</emphasis>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Example for iSCSI ports:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">io_port_list = a-1-0,B-3-0</literal_block>
                        <paragraph><literal>a</literal> or <literal>B</literal> is <emphasis>Storage Processor</emphasis>, the first numbers <literal>1</literal> and <literal>3</literal> are
                            <emphasis>Port ID</emphasis> and the second number <literal>0</literal> is <emphasis>Virtual Port ID</emphasis></paragraph>
                    </list_item>
                </bullet_list>
                <note>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>Rather than de-registered, the registered ports will be simply
                                bypassed whatever they are in <literal>io_port_list</literal> or not.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>The driver will raise an exception if ports in <literal>io_port_list</literal>
                                do not exist in VNX during startup.</paragraph>
                        </list_item>
                    </bullet_list>
                </note>
            </section>
            <section ids="force-delete-volumes-in-storage-group" names="force\ delete\ volumes\ in\ storage\ group">
                <title>Force delete volumes in storage group</title>
                <paragraph>Some <literal>available</literal> volumes may remain in storage group on the VNX array due to
                    some OpenStack timeout issue. But the VNX array do not allow the user to delete
                    the volumes which are in storage group. Option
                    <literal>force_delete_lun_in_storagegroup</literal> is introduced to allow the user to delete
                    the <literal>available</literal> volumes in this tricky situation.</paragraph>
                <paragraph>When <literal>force_delete_lun_in_storagegroup</literal> is set to <literal>True</literal> in the back-end
                    section, the driver will move the volumes out of the storage groups and then
                    delete them if the user tries to delete the volumes that remain in the storage
                    group on the VNX array.</paragraph>
                <paragraph>The default value of <literal>force_delete_lun_in_storagegroup</literal> is <literal>False</literal>.</paragraph>
            </section>
            <section ids="over-subscription-in-thin-provisioning" names="over\ subscription\ in\ thin\ provisioning">
                <title>Over subscription in thin provisioning</title>
                <paragraph>Over subscription allows that the sum of all volume’s capacity (provisioned
                    capacity) to be larger than the pool’s total capacity.</paragraph>
                <paragraph><literal>max_over_subscription_ratio</literal> in the back-end section is the ratio of
                    provisioned capacity over total capacity.</paragraph>
                <paragraph>The default value of <literal>max_over_subscription_ratio</literal> is 20.0, which means
                    the provisioned capacity can be 20 times of the total capacity.
                    If the value of this ratio is set larger than 1.0, the provisioned
                    capacity can exceed the total capacity.</paragraph>
            </section>
            <section ids="storage-group-automatic-deletion" names="storage\ group\ automatic\ deletion">
                <title>Storage group automatic deletion</title>
                <paragraph>For volume attaching, the driver has a storage group on VNX for each compute
                    node hosting the vm instances which are going to consume VNX Block Storage
                    (using compute node’s host name as storage group’s name).  All the volumes
                    attached to the VM instances in a compute node will be put into the storage
                    group. If <literal>destroy_empty_storage_group</literal> is set to <literal>True</literal>, the driver will
                    remove the empty storage group after its last volume is detached. For data
                    safety, it does not suggest to set <literal>destroy_empty_storage_group=True</literal> unless
                    the VNX is exclusively managed by one Block Storage node because consistent
                    <literal>lock_path</literal> is required for operation synchronization for this behavior.</paragraph>
            </section>
            <section ids="initiator-auto-deregistration" names="initiator\ auto\ deregistration">
                <title>Initiator auto deregistration</title>
                <paragraph>Enabling storage group automatic deletion is the precondition of this function.
                    If <literal>initiator_auto_deregistration</literal> is set to <literal>True</literal> is set, the driver will
                    deregister all FC and iSCSI initiators of the host after its storage group is
                    deleted.</paragraph>
            </section>
            <section ids="fc-san-auto-zoning" names="fc\ san\ auto\ zoning">
                <title>FC SAN auto zoning</title>
                <paragraph>The EMC VNX driver supports FC SAN auto zoning when <literal>ZoneManager</literal> is
                    configured and <literal>zoning_mode</literal> is set to <literal>fabric</literal> in <literal>cinder.conf</literal>.
                    For ZoneManager configuration, refer to <reference internal="True" refuri="../fc-zoning"><inline classes="doc">Fibre Channel Zone Manager</inline></reference>.</paragraph>
            </section>
            <section ids="volume-number-threshold" names="volume\ number\ threshold">
                <title>Volume number threshold</title>
                <paragraph>In VNX, there is a limitation on the number of pool volumes that can be created
                    in the system. When the limitation is reached, no more pool volumes can be
                    created even if there is remaining capacity in the storage pool. In other
                    words, if the scheduler dispatches a volume creation request to a back end that
                    has free capacity but reaches the volume limitation, the creation fails.</paragraph>
                <paragraph>The default value of <literal>check_max_pool_luns_threshold</literal> is <literal>False</literal>.  When
                    <literal>check_max_pool_luns_threshold=True</literal>, the pool-based back end will check the
                    limit and will report 0 free capacity to the scheduler if the limit is reached.
                    So the scheduler will be able to skip this kind of pool-based back end that
                    runs out of the pool volume number.</paragraph>
            </section>
            <section ids="iscsi-initiators" names="iscsi\ initiators">
                <title>iSCSI initiators</title>
                <paragraph><literal>iscsi_initiators</literal> is a dictionary of IP addresses of the iSCSI
                    initiator ports on OpenStack compute and block storage nodes which want to
                    connect to VNX via iSCSI. If this option is configured, the driver will
                    leverage this information to find an accessible iSCSI target portal for the
                    initiator when attaching volume. Otherwise, the iSCSI target portal will be
                    chosen in a relative random way.</paragraph>
                <note>
                    <paragraph>This option is only valid for iSCSI driver.</paragraph>
                </note>
                <paragraph>Here is an example. VNX will connect <literal>host1</literal> with <literal>10.0.0.1</literal> and
                    <literal>10.0.0.2</literal>. And it will connect <literal>host2</literal> with <literal>10.0.0.3</literal>.</paragraph>
                <paragraph>The key name (<literal>host1</literal> in the example) should be the output of
                    <literal_strong classes="command">hostname</literal_strong> command.</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">iscsi_initiators = {"host1":["10.0.0.1", "10.0.0.2"],"host2":["10.0.0.3"]}</literal_block>
            </section>
            <section ids="default-timeout" names="default\ timeout">
                <title>Default timeout</title>
                <paragraph>Specify the timeout in minutes for operations like LUN migration, LUN creation,
                    etc. For example, LUN migration is a typical long running operation, which
                    depends on the LUN size and the load of the array. An upper bound in the
                    specific deployment can be set to avoid unnecessary long wait.</paragraph>
                <paragraph>The default value for this option is <literal>infinite</literal>.</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">default_timeout = 60</literal_block>
            </section>
            <section ids="max-luns-per-storage-group" names="max\ luns\ per\ storage\ group">
                <title>Max LUNs per storage group</title>
                <paragraph>The <literal>max_luns_per_storage_group</literal> specify the maximum number of LUNs in a
                    storage group. Default value is 255. It is also the maximum value supported by
                    VNX.</paragraph>
            </section>
            <section ids="ignore-pool-full-threshold" names="ignore\ pool\ full\ threshold">
                <title>Ignore pool full threshold</title>
                <paragraph>If <literal>ignore_pool_full_threshold</literal> is set to <literal>True</literal>, driver will force LUN
                    creation even if the full threshold of pool is reached. Default to <literal>False</literal>.</paragraph>
            </section>
        </section>
        <section ids="extra-spec-options" names="extra\ spec\ options">
            <title>Extra spec options</title>
            <paragraph>Extra specs are used in volume types created in Block Storage as the preferred
                property of the volume.</paragraph>
            <paragraph>The Block Storage scheduler will use extra specs to find the suitable back end
                for the volume and the Block Storage driver will create the volume based on the
                properties specified by the extra spec.</paragraph>
            <paragraph>Use the following command to create a volume type:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create demoVolumeType</literal_block>
            <paragraph>Use the following command to update the extra spec of a volume type:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type set --property provisioning:type=thin thick_provisioning_support='&lt;is&gt; True' demoVolumeType</literal_block>
            <paragraph>The following sections describe the VNX extra keys.</paragraph>
            <section ids="provisioning-type" names="provisioning\ type">
                <title>Provisioning type</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Key: <literal>provisioning:type</literal></paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Possible Values:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph><literal>thick</literal></paragraph>
                                <paragraph>Volume is fully provisioned.</paragraph>
                                <paragraph>Run the following commands to create a <literal>thick</literal> volume type:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create ThickVolumeType
$ openstack volume type set --property provisioning:type=thick thick_provisioning_support='&lt;is&gt; True' ThickVolumeType</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph><literal>thin</literal></paragraph>
                                <paragraph>Volume is virtually provisioned.</paragraph>
                                <paragraph>Run the following commands to create a <literal>thin</literal> volume type:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create ThinVolumeType
$ openstack volume type set --property provisioning:type=thin thin_provisioning_support='&lt;is&gt; True' ThinVolumeType</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph><literal>deduplicated</literal></paragraph>
                                <paragraph>Volume is <literal>thin</literal> and deduplication is enabled. The administrator shall
                                    go to VNX to configure the system level deduplication settings. To
                                    create a deduplicated volume, the VNX Deduplication license must be
                                    activated on VNX, and specify <literal>deduplication_support=True</literal> to let Block
                                    Storage scheduler find the proper volume back end.</paragraph>
                                <paragraph>Run the following commands to create a <literal>deduplicated</literal> volume type:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create DeduplicatedVolumeType
$ openstack volume type set --property provisioning:type=deduplicated deduplicated_support='&lt;is&gt; True' DeduplicatedVolumeType</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph><literal>compressed</literal></paragraph>
                                <paragraph>Volume is <literal>thin</literal> and compression is enabled. The administrator shall go
                                    to the VNX to configure the system level compression settings. To create
                                    a compressed volume, the VNX Compression license must be activated on
                                    VNX, and use <literal>compression_support=True</literal> to let Block Storage scheduler
                                    find a volume back end. VNX does not support creating snapshots on a
                                    compressed volume.</paragraph>
                                <paragraph>Run the following commands to create a <literal>compressed</literal> volume type:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create CompressedVolumeType
$ openstack volume type set --property provisioning:type=compressed compression_support='&lt;is&gt; True' CompressedVolumeType</literal_block>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Default: <literal>thick</literal></paragraph>
                    </list_item>
                </bullet_list>
                <note>
                    <paragraph><literal>provisioning:type</literal> replaces the old spec key <literal>storagetype:provisioning</literal>.
                        The latter one is obsolete since the <emphasis>Mitaka</emphasis> release.</paragraph>
                </note>
            </section>
            <section ids="storage-tiering-support" names="storage\ tiering\ support">
                <title>Storage tiering support</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Key: <literal>storagetype:tiering</literal></paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Possible values:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph><literal>StartHighThenAuto</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph><literal>Auto</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph><literal>HighestAvailable</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph><literal>LowestAvailable</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph><literal>NoMovement</literal></paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Default: <literal>StartHighThenAuto</literal></paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>VNX supports fully automated storage tiering which requires the FAST license
                    activated on the VNX. The OpenStack administrator can use the extra spec key
                    <literal>storagetype:tiering</literal> to set the tiering policy of a volume and use the key
                    <literal>fast_support='&lt;is&gt; True'</literal> to let Block Storage scheduler find a volume back
                    end which manages a VNX with FAST license activated. Here are the five
                    supported values for the extra spec key <literal>storagetype:tiering</literal>:</paragraph>
                <paragraph>Run the following commands to create a volume type with tiering policy:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create ThinVolumeOnAutoTier
$ openstack volume type set --property provisioning:type=thin storagetype:tiering=Auto fast_support='&lt;is&gt; True' ThinVolumeOnAutoTier</literal_block>
                <note>
                    <paragraph>The tiering policy cannot be applied to a deduplicated volume. Tiering
                        policy of the deduplicated LUN align with the settings of the pool.</paragraph>
                </note>
            </section>
            <section ids="fast-cache-support" names="fast\ cache\ support">
                <title>FAST cache support</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Key: <literal>fast_cache_enabled</literal></paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Possible values:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph><literal>True</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph><literal>False</literal></paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Default: <literal>False</literal></paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>VNX has FAST Cache feature which requires the FAST Cache license activated on
                    the VNX. Volume will be created on the backend with FAST cache enabled when
                    <literal>&lt;is&gt; True</literal> is specified.</paragraph>
            </section>
            <section ids="pool-name" names="pool\ name">
                <title>Pool name</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Key: <literal>pool_name</literal></paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Possible values: name of the storage pool managed by cinder</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Default: None</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>If the user wants to create a volume on a certain storage pool in a back end
                    that manages multiple pools, a volume type with a extra spec specified storage
                    pool should be created first, then the user can use this volume type to create
                    the volume.</paragraph>
                <paragraph>Run the following commands to create the volume type:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create HighPerf
$ openstack volume type set --property pool_name=Pool_02_SASFLASH volume_backend_name=vnx_41 HighPerf</literal_block>
            </section>
            <section ids="obsolete-extra-specs" names="obsolete\ extra\ specs">
                <title>Obsolete extra specs</title>
                <note>
                    <paragraph><emphasis>DO NOT</emphasis> use the following obsolete extra spec keys:</paragraph>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph><literal>storagetype:provisioning</literal></paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><literal>storagetype:pool</literal></paragraph>
                        </list_item>
                    </bullet_list>
                </note>
            </section>
            <section ids="force-detach" names="force\ detach">
                <title>Force detach</title>
                <paragraph>The user could use <title_reference>os-force_detach</title_reference> action to detach a volume from all its attached hosts.
                    For more detail, please refer to
                    <reference refuri="https://developer.openstack.org/api-ref/block-storage/v2/?expanded=force-detach-volume-detail#force-detach-volume">https://developer.openstack.org/api-ref/block-storage/v2/?expanded=force-detach-volume-detail#force-detach-volume</reference></paragraph>
            </section>
        </section>
        <section ids="advanced-features" names="advanced\ features">
            <title>Advanced features</title>
            <section ids="snap-copy" names="snap\ copy">
                <title>Snap copy</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Metadata Key: <literal>snapcopy</literal></paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Possible Values:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph><literal>True</literal> or <literal>true</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph><literal>False</literal> or <literal>false</literal></paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Default: <title_reference>False</title_reference></paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>VNX driver supports snap copy which accelerates the process for
                    creating a copied volume.</paragraph>
                <paragraph>By default, the driver will use <reference name="asynchronous migration support" refid="asynchronous-migration-support">asynchronous migration support</reference>, which will
                    start a VNX migration session. When snap copy is used, driver creates a
                    snapshot and mounts it as a volume for the 2 kinds of operations which will be
                    instant even for large volumes.</paragraph>
                <paragraph>To enable this functionality, append <literal>--metadata snapcopy=True</literal>
                    when creating cloned volume or creating volume from snapshot.</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder create --source-volid &lt;source-void&gt; --name "cloned_volume" --metadata snapcopy=True</literal_block>
                <paragraph>Or</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder create --snapshot-id &lt;snapshot-id&gt; --name "vol_from_snapshot" --metadata snapcopy=True</literal_block>
                <paragraph>The newly created volume is a snap copy instead of
                    a full copy. If a full copy is needed, retype or migrate can be used
                    to convert the snap-copy volume to a full-copy volume which may be
                    time-consuming.</paragraph>
                <paragraph>You can determine whether the volume is a snap-copy volume or not by
                    showing its metadata. If the <literal>snapcopy</literal> in metadata is <literal>True</literal> or <literal>true</literal>,
                    the volume is a snap-copy volume. Otherwise, it is a full-copy volume.</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder metadata-show &lt;volume&gt;</literal_block>
                <paragraph><strong>Constraints</strong></paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>The number of snap-copy volumes created from a single source volume is
                            limited to 255 at one point in time.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>The source volume which has snap-copy volume can not be deleted or migrated.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>snapcopy volume will be change to full-copy volume after host-assisted or
                            storage-assisted migration.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>snapcopy volume can not be added to consisgroup because of VNX limitation.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="efficient-non-disruptive-volume-backup" names="efficient\ non-disruptive\ volume\ backup">
                <title>Efficient non-disruptive volume backup</title>
                <paragraph>The default implementation in Block Storage for non-disruptive volume backup is
                    not efficient since a cloned volume will be created during backup.</paragraph>
                <paragraph>The approach of efficient backup is to create a snapshot for the volume and
                    connect this snapshot (a mount point in VNX) to the Block Storage host for
                    volume backup. This eliminates migration time involved in volume clone.</paragraph>
                <paragraph><strong>Constraints</strong></paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Backup creation for a snap-copy volume is not allowed if the volume
                            status is <literal>in-use</literal> since snapshot cannot be taken from this volume.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="configurable-migration-rate" names="configurable\ migration\ rate">
                <title>Configurable migration rate</title>
                <paragraph>VNX cinder driver is leveraging the LUN migration from the VNX. LUN migration
                    is involved in cloning, migrating, retyping, and creating volume from snapshot.
                    When admin set <literal>migrate_rate</literal> in volume’s <literal>metadata</literal>, VNX driver can start
                    migration with specified rate. The available values for the <literal>migrate_rate</literal>
                    are <literal>high</literal>, <literal>asap</literal>, <literal>low</literal> and <literal>medium</literal>.</paragraph>
                <paragraph>The following is an example to set <literal>migrate_rate</literal> to <literal>asap</literal>:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder metadata &lt;volume-id&gt; set migrate_rate=asap</literal_block>
                <paragraph>After set, any cinder volume operations involving VNX LUN migration will
                    take the value as the migration rate. To restore the migration rate to
                    default, unset the metadata as following:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder metadata &lt;volume-id&gt; unset migrate_rate</literal_block>
                <note>
                    <paragraph>Do not use the <literal>asap</literal> migration rate when the system is in production, as the normal
                        host I/O may be interrupted. Use asap only when the system is offline
                        (free of any host-level I/O).</paragraph>
                </note>
            </section>
            <section ids="replication-v2-1-support" names="replication\ v2.1\ support">
                <title>Replication v2.1 support</title>
                <paragraph>Cinder introduces Replication v2.1 support in Mitaka, it supports
                    fail-over and fail-back replication for specific back end. In VNX cinder
                    driver, <strong>MirrorView</strong> is used to set up replication for the volume.</paragraph>
                <paragraph>To enable this feature, you need to set configuration in <literal>cinder.conf</literal> as
                    below:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">replication_device = backend_id:&lt;secondary VNX serial number&gt;,
                     san_ip:192.168.1.2,
                     san_login:admin,
                     san_password:admin,
                     naviseccli_path:/opt/Navisphere/bin/naviseccli,
                     storage_vnx_authentication_type:global,
                     storage_vnx_security_file_dir:</literal_block>
                <paragraph>Currently, only synchronized mode <strong>MirrorView</strong> is supported, and one volume
                    can only have 1 secondary storage system. Therefore, you can have only one
                    <literal>replication_device</literal> presented in driver configuration section.</paragraph>
                <paragraph>To create a replication enabled volume, you need to create a volume type:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create replication-type
$ openstack volume type set --property replication_enabled="&lt;is&gt; True" replication-type</literal_block>
                <paragraph>And then create volume with above volume type:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume create replication-volume --type replication-type --size 1</literal_block>
                <paragraph><strong>Supported operations</strong></paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Create volume</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Create cloned volume</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Create volume from snapshot</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Fail-over volume:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder failover-host --backend_id &lt;secondary VNX serial number&gt; &lt;hostname&gt;</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Fail-back volume:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder failover-host --backend_id default &lt;hostname&gt;</literal_block>
                    </list_item>
                </bullet_list>
                <paragraph><strong>Requirements</strong></paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>2 VNX systems must be in same domain.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>For iSCSI MirrorView, user needs to setup iSCSI connection before enable
                            replication in Cinder.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>For FC MirrorView, user needs to zone specific FC ports from 2
                            VNX system together.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>MirrorView Sync enabler( <strong>MirrorView/S</strong> ) installed on both systems.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Write intent log enabled on both VNX systems.</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>For more information on how to configure, please refer to: <reference name="MirrorView-Knowledgebook:-Releases-30-–-33" refuri="https://support.emc.com/docu32906_MirrorView-Knowledgebook:-Releases-30-%E2%80%93-33---A-Detailed-Review.pdf?language=en_US">MirrorView-Knowledgebook:-Releases-30-–-33</reference><target ids="mirrorview-knowledgebook-releases-30-33" names="mirrorview-knowledgebook:-releases-30-–-33" refuri="https://support.emc.com/docu32906_MirrorView-Knowledgebook:-Releases-30-%E2%80%93-33---A-Detailed-Review.pdf?language=en_US"></target></paragraph>
            </section>
            <section ids="asynchronous-migration-support" names="asynchronous\ migration\ support">
                <title>Asynchronous migration support</title>
                <paragraph>VNX Cinder driver now supports asynchronous migration during volume cloning.</paragraph>
                <paragraph>The driver now using asynchronous migration when creating a volume from source
                    as the default cloning method. The driver will return immediately after the
                    migration session starts on the VNX, which dramatically reduces the time before
                    a volume is available for use.</paragraph>
                <paragraph>To disable this feature, user can add <literal>--metadata async_migrate=False</literal> when
                    creating new volume from source.</paragraph>
            </section>
        </section>
        <section ids="best-practice" names="best\ practice">
            <title>Best practice</title>
            <target refid="multipath-setup"></target>
            <section ids="multipath-setup id1" names="multipath\ setup multipath-setup">
                <title>Multipath setup</title>
                <paragraph>Enabling multipath volume access is recommended for robust data access.
                    The major configuration includes:</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Install <literal>multipath-tools</literal>, <literal>sysfsutils</literal> and <literal>sg3-utils</literal> on the
                            nodes hosting compute and <literal>cinder-volume</literal> services. Check
                            the operating system manual for the system distribution for specific
                            installation steps. For Red Hat based distributions, they should be
                            <literal>device-mapper-multipath</literal>, <literal>sysfsutils</literal> and <literal>sg3_utils</literal>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Specify <literal>use_multipath_for_image_xfer=true</literal> in the <literal>cinder.conf</literal> file
                            for each FC/iSCSI back end.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Specify <literal>iscsi_use_multipath=True</literal> in <literal>libvirt</literal> section of the
                            <literal>nova.conf</literal> file. This option is valid for both iSCSI and FC driver.</paragraph>
                    </list_item>
                </enumerated_list>
                <paragraph>For multipath-tools, here is an EMC recommended sample of
                    <literal>/etc/multipath.conf</literal> file.</paragraph>
                <paragraph><literal>user_friendly_names</literal> is not specified in the configuration and thus
                    it will take the default value <literal>no</literal>. It is not recommended to set it
                    to <literal>yes</literal> because it may fail operations such as VM live migration.</paragraph>
                <literal_block highlight_args="{}" language="vim" linenos="False" xml:space="preserve">blacklist {
    # Skip the files under /dev that are definitely not FC/iSCSI devices
    # Different system may need different customization
    devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"
    devnode "^hd[a-z][0-9]*"
    devnode "^cciss!c[0-9]d[0-9]*[p[0-9]*]"

    # Skip LUNZ device from VNX
    device {
        vendor "DGC"
        product "LUNZ"
        }
}

defaults {
    user_friendly_names no
    flush_on_last_del yes
}

devices {
    # Device attributed for EMC CLARiiON and VNX series ALUA
    device {
        vendor "DGC"
        product ".*"
        product_blacklist "LUNZ"
        path_grouping_policy group_by_prio
        path_selector "round-robin 0"
        path_checker emc_clariion
        features "1 queue_if_no_path"
        hardware_handler "1 alua"
        prio alua
        failback immediate
    }
}</literal_block>
                <note>
                    <paragraph>When multipath is used in OpenStack, multipath faulty devices may
                        come out in Nova-Compute nodes due to different issues (<reference name="Bug 1336683" refuri="https://bugs.launchpad.net/nova/+bug/1336683">Bug
                            1336683</reference><target ids="bug-1336683" names="bug\ 1336683" refuri="https://bugs.launchpad.net/nova/+bug/1336683"></target> is a
                        typical example).</paragraph>
                </note>
                <paragraph>A solution to completely avoid faulty devices has not been found yet.
                    <literal>faulty_device_cleanup.py</literal> mitigates this issue when VNX iSCSI storage is
                    used. Cloud administrators can deploy the script in all Nova-Compute nodes and
                    use a CRON job to run the script on each Nova-Compute node periodically so that
                    faulty devices will not stay too long. Refer to: <reference name="VNX faulty device cleanup" refuri="https://github.com/emc-openstack/vnx-faulty-device-cleanup">VNX faulty device
                        cleanup</reference><target ids="vnx-faulty-device-cleanup" names="vnx\ faulty\ device\ cleanup" refuri="https://github.com/emc-openstack/vnx-faulty-device-cleanup"></target> for
                    detailed usage and the script.</paragraph>
            </section>
        </section>
        <section ids="restrictions-and-limitations" names="restrictions\ and\ limitations">
            <title>Restrictions and limitations</title>
            <section ids="iscsi-port-cache" names="iscsi\ port\ cache">
                <title>iSCSI port cache</title>
                <paragraph>EMC VNX iSCSI driver caches the iSCSI ports information, so that the user
                    should restart the <literal>cinder-volume</literal> service or wait for seconds (which is
                    configured by <literal>periodic_interval</literal> in the <literal>cinder.conf</literal> file) before any
                    volume attachment operation after changing the iSCSI port configurations.
                    Otherwise the attachment may fail because the old iSCSI port configurations
                    were used.</paragraph>
            </section>
            <section ids="no-extending-for-volume-with-snapshots" names="no\ extending\ for\ volume\ with\ snapshots">
                <title>No extending for volume with snapshots</title>
                <paragraph>VNX does not support extending the thick volume which has a snapshot. If the
                    user tries to extend a volume which has a snapshot, the status of the volume
                    would change to <literal>error_extending</literal>.</paragraph>
            </section>
            <section ids="limitations-for-deploying-cinder-on-computer-node" names="limitations\ for\ deploying\ cinder\ on\ computer\ node">
                <title>Limitations for deploying cinder on computer node</title>
                <paragraph>It is not recommended to deploy the driver on a compute node if <literal>cinder
upload-to-image --force True</literal> is used against an in-use volume. Otherwise,
                    <literal>cinder upload-to-image --force True</literal> will terminate the data access of the
                    vm instance to the volume.</paragraph>
            </section>
            <section ids="storage-group-with-host-names-in-vnx" names="storage\ group\ with\ host\ names\ in\ vnx">
                <title>Storage group with host names in VNX</title>
                <paragraph>When the driver notices that there is no existing storage group that has the
                    host name as the storage group name, it will create the storage group and also
                    add the compute node’s or Block Storage node’s registered initiators into the
                    storage group.</paragraph>
                <paragraph>If the driver notices that the storage group already exists, it will assume
                    that the registered initiators have also been put into it and skip the
                    operations above for better performance.</paragraph>
                <paragraph>It is recommended that the storage administrator does not create the storage
                    group manually and instead relies on the driver for the preparation. If the
                    storage administrator needs to create the storage group manually for some
                    special requirements, the correct registered initiators should be put into the
                    storage group as well (otherwise the following volume attaching operations will
                    fail).</paragraph>
            </section>
            <section ids="emc-storage-assisted-volume-migration" names="emc\ storage-assisted\ volume\ migration">
                <title>EMC storage-assisted volume migration</title>
                <paragraph>EMC VNX driver supports storage-assisted volume migration, when the user starts
                    migrating with <literal>cinder migrate --force-host-copy False &lt;volume_id&gt; &lt;host&gt;</literal> or
                    <literal>cinder migrate &lt;volume_id&gt; &lt;host&gt;</literal>, cinder will try to leverage the VNX’s
                    native volume migration functionality.</paragraph>
                <paragraph>In following scenarios, VNX storage-assisted volume migration will not be
                    triggered:</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph><literal>in-use</literal> volume migration between back ends with different storage
                            protocol, for example, FC and iSCSI.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Volume is to be migrated across arrays.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
        </section>
        <section ids="appendix" names="appendix">
            <title>Appendix</title>
            <target refid="authenticate-by-security-file"></target>
            <section ids="authenticate-by-security-file id2" names="authenticate\ by\ security\ file authenticate-by-security-file">
                <title>Authenticate by security file</title>
                <paragraph>VNX credentials are necessary when the driver connects to the VNX system.
                    Credentials in <literal>global</literal>, <literal>local</literal> and <literal>ldap</literal> scopes are supported. There
                    are two approaches to provide the credentials.</paragraph>
                <paragraph>The recommended one is using the Navisphere CLI security file to provide the
                    credentials which can get rid of providing the plain text credentials in the
                    configuration file. Following is the instruction on how to do this.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Find out the Linux user id of the <literal>cinder-volume</literal> processes. Assuming the
                            <literal>cinder-volume</literal> service is running by the account <literal>cinder</literal>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Run <literal>su</literal> as root user.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>In <literal>/etc/passwd</literal> file, change
                            <literal>cinder:x:113:120::/var/lib/cinder:/bin/false</literal>
                            to <literal>cinder:x:113:120::/var/lib/cinder:/bin/bash</literal> (This temporary change is
                            to make step 4 work.)</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Save the credentials on behalf of <literal>cinder</literal> user to a security file
                            (assuming the array credentials are <literal>admin/admin</literal> in <literal>global</literal> scope). In
                            the command below, the <literal>-secfilepath</literal> switch is used to specify the
                            location to save the security file.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># su -l cinder -c \
  '/opt/Navisphere/bin/naviseccli -AddUserSecurity -user admin -password admin -scope 0 -secfilepath &lt;location&gt;'</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Change <literal>cinder:x:113:120::/var/lib/cinder:/bin/bash</literal> back to
                            <literal>cinder:x:113:120::/var/lib/cinder:/bin/false</literal> in <literal>/etc/passwd</literal> file.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Remove the credentials options <literal>san_login</literal>, <literal>san_password</literal> and
                            <literal>storage_vnx_authentication_type</literal> from <literal>cinder.conf</literal> file. (normally
                            it is <literal>/etc/cinder/cinder.conf</literal> file). Add option
                            <literal>storage_vnx_security_file_dir</literal> and set its value to the directory path of
                            your security file generated in the above step. Omit this option if
                            <literal>-secfilepath</literal> is not used in the above step.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Restart the <literal>cinder-volume</literal> service to validate the change.</paragraph>
                    </list_item>
                </enumerated_list>
                <target refid="register-fc-port-with-vnx"></target>
            </section>
            <section ids="register-fc-port-with-vnx id3" names="register\ fc\ port\ with\ vnx register-fc-port-with-vnx">
                <title>Register FC port with VNX</title>
                <paragraph>This configuration is only required when <literal>initiator_auto_registration=False</literal>.</paragraph>
                <paragraph>To access VNX storage, the Compute nodes should be registered on VNX first if
                    initiator auto registration is not enabled.</paragraph>
                <paragraph>To perform <literal>Copy Image to Volume</literal> and <literal>Copy Volume to Image</literal> operations,
                    the nodes running the <literal>cinder-volume</literal> service (Block Storage nodes) must be
                    registered with the VNX as well.</paragraph>
                <paragraph>The steps mentioned below are for the compute nodes. Follow the same
                    steps for the Block Storage nodes also (The steps can be skipped if initiator
                    auto registration is enabled).</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Assume <literal>20:00:00:24:FF:48:BA:C2:21:00:00:24:FF:48:BA:C2</literal> is the WWN of a
                            FC initiator port name of the compute node whose host name and IP are
                            <literal>myhost1</literal> and <literal>10.10.61.1</literal>. Register
                            <literal>20:00:00:24:FF:48:BA:C2:21:00:00:24:FF:48:BA:C2</literal> in Unisphere:</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Log in to <inline classes="guilabel" rawtext=":guilabel:`Unisphere`">Unisphere</inline>, go to
                            <inline classes="menuselection" rawtext=":menuselection:`FNM0000000000 &gt; Hosts &gt; Initiators`">FNM0000000000 &gt; Hosts &gt; Initiators</inline>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Refresh and wait until the initiator
                            <literal>20:00:00:24:FF:48:BA:C2:21:00:00:24:FF:48:BA:C2</literal> with SP Port <literal>A-1</literal>
                            appears.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Click the <inline classes="guilabel" rawtext=":guilabel:`Register`">Register</inline> button, select <inline classes="guilabel" rawtext=":guilabel:`CLARiiON/VNX`">CLARiiON/VNX</inline>
                            and enter the host name (which is the output of the <literal_strong classes="command">hostname</literal_strong>
                            command) and IP address:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph>Hostname: <literal>myhost1</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph>IP: <literal>10.10.61.1</literal></paragraph>
                            </list_item>
                            <list_item>
                                <paragraph>Click <inline classes="guilabel" rawtext=":guilabel:`Register`">Register</inline>.</paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Then host <literal>10.10.61.1</literal> will appear under
                            <inline classes="menuselection" rawtext=":menuselection:`Hosts &gt; Host List`">Hosts &gt; Host List</inline> as well.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Register the <literal>wwn</literal> with more ports if needed.</paragraph>
                    </list_item>
                </enumerated_list>
                <target refid="register-iscsi-port-with-vnx"></target>
            </section>
            <section ids="register-iscsi-port-with-vnx id4" names="register\ iscsi\ port\ with\ vnx register-iscsi-port-with-vnx">
                <title>Register iSCSI port with VNX</title>
                <paragraph>This configuration is only required when <literal>initiator_auto_registration=False</literal>.</paragraph>
                <paragraph>To access VNX storage, the compute nodes should be registered on VNX first if
                    initiator auto registration is not enabled.</paragraph>
                <paragraph>To perform <literal>Copy Image to Volume</literal> and <literal>Copy Volume to Image</literal> operations,
                    the nodes running the <literal>cinder-volume</literal> service (Block Storage nodes) must be
                    registered with the VNX as well.</paragraph>
                <paragraph>The steps mentioned below are for the compute nodes. Follow the
                    same steps for the Block Storage nodes also (The steps can be skipped if
                    initiator auto registration is enabled).</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>On the compute node with IP address <literal>10.10.61.1</literal> and host name <literal>myhost1</literal>,
                            execute the following commands (assuming <literal>10.10.61.35</literal> is the iSCSI
                            target):</paragraph>
                        <enumerated_list enumtype="arabic" prefix="" suffix=".">
                            <list_item>
                                <paragraph>Start the iSCSI initiator service on the node:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># /etc/init.d/open-iscsi start</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph>Discover the iSCSI target portals on VNX:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m discovery -t st -p 10.10.61.35</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph>Change directory to <literal>/etc/iscsi</literal> :</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># cd /etc/iscsi</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph>Find out the <literal>iqn</literal> of the node:</paragraph>
                                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># more initiatorname.iscsi</literal_block>
                            </list_item>
                        </enumerated_list>
                    </list_item>
                    <list_item>
                        <paragraph>Log in to <inline classes="guilabel" rawtext=":guilabel:`VNX`">VNX</inline> from the compute node using the target
                            corresponding to the SPA port:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m node -T iqn.1992-04.com.emc:cx.apm01234567890.a0 -p 10.10.61.35 -l</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Assume <literal>iqn.1993-08.org.debian:01:1a2b3c4d5f6g</literal> is the initiator name of
                            the compute node. Register <literal>iqn.1993-08.org.debian:01:1a2b3c4d5f6g</literal> in
                            Unisphere:</paragraph>
                        <enumerated_list enumtype="arabic" prefix="" suffix=".">
                            <list_item>
                                <paragraph>Log in to <inline classes="guilabel" rawtext=":guilabel:`Unisphere`">Unisphere</inline>, go to
                                    <inline classes="menuselection" rawtext=":menuselection:`FNM0000000000 &gt; Hosts &gt; Initiators`">FNM0000000000 &gt; Hosts &gt; Initiators</inline>.</paragraph>
                            </list_item>
                            <list_item>
                                <paragraph>Refresh and wait until the initiator
                                    <literal>iqn.1993-08.org.debian:01:1a2b3c4d5f6g</literal> with SP Port <literal>A-8v0</literal>
                                    appears.</paragraph>
                            </list_item>
                            <list_item>
                                <paragraph>Click the <inline classes="guilabel" rawtext=":guilabel:`Register`">Register</inline> button, select <inline classes="guilabel" rawtext=":guilabel:`CLARiiON/VNX`">CLARiiON/VNX</inline>
                                    and enter the host name
                                    (which is the output of the <literal_strong classes="command">hostname</literal_strong> command) and IP address:</paragraph>
                                <bullet_list bullet="-">
                                    <list_item>
                                        <paragraph>Hostname: <literal>myhost1</literal></paragraph>
                                    </list_item>
                                    <list_item>
                                        <paragraph>IP: <literal>10.10.61.1</literal></paragraph>
                                    </list_item>
                                    <list_item>
                                        <paragraph>Click <inline classes="guilabel" rawtext=":guilabel:`Register`">Register</inline>.</paragraph>
                                    </list_item>
                                </bullet_list>
                            </list_item>
                            <list_item>
                                <paragraph>Then host <literal>10.10.61.1</literal> will appear under
                                    <inline classes="menuselection" rawtext=":menuselection:`Hosts &gt; Host List`">Hosts &gt; Host List</inline> as well.</paragraph>
                            </list_item>
                        </enumerated_list>
                    </list_item>
                    <list_item>
                        <paragraph>Log out <inline classes="guilabel" rawtext=":guilabel:`iSCSI`">iSCSI</inline> on the node:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m node -u</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Log in to <inline classes="guilabel" rawtext=":guilabel:`VNX`">VNX</inline> from the compute node using the target
                            corresponding to the SPB port:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m node -T iqn.1992-04.com.emc:cx.apm01234567890.b8 -p 10.10.61.36 -l</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>In <literal>Unisphere</literal>, register the initiator with the SPB port.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Log out <inline classes="guilabel" rawtext=":guilabel:`iSCSI`">iSCSI</inline> on the node:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m node -u</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Register the <literal>iqn</literal> with more ports if needed.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
        </section>
    </section>
</document>
