<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/nova/doc/source/admin/node-down.rst">
    <section ids="recover-from-a-failed-compute-node" names="recover\ from\ a\ failed\ compute\ node">
        <title>Recover from a failed compute node</title>
        <paragraph>If you deploy Compute with a shared file system, you can use several methods to
            quickly recover from a node failure. This section discusses manual recovery.</paragraph>
        <section ids="evacuate-instances" names="evacuate\ instances">
            <title>Evacuate instances</title>
            <paragraph>If a hardware malfunction or other error causes the cloud compute node to fail,
                you can use the <literal_strong classes="command">nova evacuate</literal_strong> command to evacuate instances.  See
                <reference internal="True" refuri="evacuate"><inline classes="doc">evacuate instances</inline></reference> for more information on using the command.</paragraph>
            <target refid="nova-compute-node-down-manual-recovery"></target>
        </section>
        <section ids="manual-recovery nova-compute-node-down-manual-recovery" names="manual\ recovery nova-compute-node-down-manual-recovery">
            <title>Manual recovery</title>
            <paragraph>To manually recover a failed compute node:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Identify the VMs on the affected hosts by using a combination of the
                        <literal_strong classes="command">openstack server list</literal_strong> and <literal_strong classes="command">openstack server show</literal_strong>
                        commands or the <literal_strong classes="command">euca-describe-instances</literal_strong> command.</paragraph>
                    <paragraph>For example, this command displays information about the i-000015b9 instance
                        that runs on the np-rcc54 node:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ euca-describe-instances
i-000015b9 at3-ui02 running nectarkey (376, np-rcc54) 0 m1.xxlarge 2012-06-19T00:48:11.000Z 115.146.93.60</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Query the Compute database for the status of the host. This example converts
                        an EC2 API instance ID to an OpenStack ID. If you use the <literal_strong classes="command">nova</literal_strong>
                        commands, you can substitute the ID directly. This example output is
                        truncated:</paragraph>
                    <literal_block highlight_args="{}" language="none" linenos="False" xml:space="preserve">mysql&gt; SELECT * FROM instances WHERE id = CONV('15b9', 16, 10) \G;
*************************** 1. row ***************************
created_at: 2012-06-19 00:48:11
updated_at: 2012-07-03 00:35:11
deleted_at: NULL
...
id: 5561
...
power_state: 5
vm_state: shutoff
...
hostname: at3-ui02
host: np-rcc54
...
uuid: 3f57699a-e773-4650-a443-b4b37eed5a06
...
task_state: NULL
...</literal_block>
                    <note>
                        <paragraph>Find the credentials for your database in <literal>/etc/nova.conf</literal> file.</paragraph>
                    </note>
                </list_item>
                <list_item>
                    <paragraph>Decide to which compute host to move the affected VM. Run this database
                        command to move the VM to that host:</paragraph>
                    <literal_block highlight_args="{}" language="mysql" linenos="False" xml:space="preserve">mysql&gt; UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';</literal_block>
                </list_item>
                <list_item>
                    <paragraph>If you use a hypervisor that relies on libvirt, such as KVM, update the
                        <literal>libvirt.xml</literal> file in <literal>/var/lib/nova/instances/[instance ID]</literal> with these
                        changes:</paragraph>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>Change the <literal>DHCPSERVER</literal> value to the host IP address of the new compute
                                host.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>Update the VNC IP to <literal>0.0.0.0</literal>.</paragraph>
                        </list_item>
                    </bullet_list>
                </list_item>
                <list_item>
                    <paragraph>Reboot the VM:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server reboot 3f57699a-e773-4650-a443-b4b37eed5a06</literal_block>
                </list_item>
            </enumerated_list>
            <paragraph>Typically, the database update and <literal_strong classes="command">openstack server reboot</literal_strong> command
                recover a VM from a failed host. However, if problems persist, try one of these
                actions:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>Use <literal_strong classes="command">virsh</literal_strong> to recreate the network filter configuration.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Restart Compute services.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Update the <literal>vm_state</literal> and <literal>power_state</literal> fields in the Compute database.</paragraph>
                </list_item>
            </bullet_list>
        </section>
        <section ids="recover-from-a-uid-gid-mismatch" names="recover\ from\ a\ uid/gid\ mismatch">
            <title>Recover from a UID/GID mismatch</title>
            <paragraph>Sometimes when you run Compute with a shared file system or an automated
                configuration tool, files on your compute node might use the wrong UID or GID.
                This UID or GID mismatch can prevent you from running live migrations or
                starting virtual machines.</paragraph>
            <paragraph>This procedure runs on <literal>nova-compute</literal> hosts, based on the KVM hypervisor:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Set the nova UID to the same number in <literal>/etc/passwd</literal> on all hosts. For
                        example, set the UID to <literal>112</literal>.</paragraph>
                    <note>
                        <paragraph>Choose UIDs or GIDs that are not in use for other users or groups.</paragraph>
                    </note>
                </list_item>
                <list_item>
                    <paragraph>Set the <literal>libvirt-qemu</literal> UID to the same number in the <literal>/etc/passwd</literal> file
                        on all hosts. For example, set the UID to <literal>119</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Set the <literal>nova</literal> group to the same number in the <literal>/etc/group</literal> file on all
                        hosts. For example, set the group to <literal>120</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Set the <literal>libvirtd</literal> group to the same number in the <literal>/etc/group</literal> file on
                        all hosts. For example, set the group to <literal>119</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Stop the services on the compute node.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Change all files that the nova user or group owns. For example:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># find / -uid 108 -exec chown nova {} \;
# note the 108 here is the old nova UID before the change
# find / -gid 120 -exec chgrp nova {} \;</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Repeat all steps for the <literal>libvirt-qemu</literal> files, if required.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Restart the services.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>To verify that all files use the correct IDs, run the <literal_strong classes="command">find</literal_strong>
                        command.</paragraph>
                </list_item>
            </enumerated_list>
        </section>
        <section ids="recover-cloud-after-disaster" names="recover\ cloud\ after\ disaster">
            <title>Recover cloud after disaster</title>
            <paragraph>This section describes how to manage your cloud after a disaster and back up
                persistent storage volumes. Backups are mandatory, even outside of disaster
                scenarios.</paragraph>
            <paragraph>For a definition of a disaster recovery plan (DRP), see
                <reference name="https://en.wikipedia.org/wiki/Disaster_Recovery_Plan" refuri="https://en.wikipedia.org/wiki/Disaster_Recovery_Plan">https://en.wikipedia.org/wiki/Disaster_Recovery_Plan</reference><target ids="https-en-wikipedia-org-wiki-disaster-recovery-plan" names="https://en.wikipedia.org/wiki/disaster_recovery_plan" refuri="https://en.wikipedia.org/wiki/Disaster_Recovery_Plan"></target>.</paragraph>
            <paragraph>A disk crash, network loss, or power failure can affect several components in
                your cloud architecture. The worst disaster for a cloud is a power loss. A
                power loss affects these components:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>A cloud controller (<literal>nova-api</literal>, <literal>nova-objectstore</literal>, <literal>nova-network</literal>)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>A compute node (<literal>nova-compute</literal>)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>A storage area network (SAN) used by OpenStack Block Storage
                        (<literal>cinder-volumes</literal>)</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>Before a power loss:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>Create an active iSCSI session from the SAN to the cloud controller (used
                        for the <literal>cinder-volumes</literal> LVM's VG).</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create an active iSCSI session from the cloud controller to the compute node
                        (managed by <literal>cinder-volume</literal>).</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create an iSCSI session for every volume (so 14 EBS volumes requires 14
                        iSCSI sessions).</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create <literal>iptables</literal> or <literal>ebtables</literal> rules from the cloud controller to the
                        compute node. This allows access from the cloud controller to the running
                        instance.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Save the current state of the database, the current state of the running
                        instances, and the attached volumes (mount point, volume ID, volume status,
                        etc), at least from the cloud controller to the compute node.</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>After power resumes and all hardware components restart:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>The iSCSI session from the SAN to the cloud no longer exists.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The iSCSI session from the cloud controller to the compute node no longer
                        exists.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>nova-network reapplies configurations on boot and, as a result, recreates
                        the iptables and ebtables from the cloud controller to the compute node.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Instances stop running.</paragraph>
                    <paragraph>Instances are not lost because neither <literal>destroy</literal> nor <literal>terminate</literal> ran.
                        The files for the instances remain on the compute node.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The database does not update.</paragraph>
                </list_item>
            </bullet_list>
            <rubric>Begin recovery</rubric>
            <warning>
                <paragraph>Do not add any steps or change the order of steps in this procedure.</paragraph>
            </warning>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Check the current relationship between the volume and its instance, so that
                        you can recreate the attachment.</paragraph>
                    <paragraph>Use the <literal_strong classes="command">openstack volume list</literal_strong> command to get this information.
                        Note that the <literal_strong classes="command">openstack</literal_strong> client can get volume information from
                        OpenStack Block Storage.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Update the database to clean the stalled state. Do this for every volume by
                        using these queries:</paragraph>
                    <literal_block highlight_args="{}" language="mysql" linenos="False" xml:space="preserve">mysql&gt; use cinder;
mysql&gt; update volumes set mountpoint=NULL;
mysql&gt; update volumes set status="available" where status &lt;&gt;"error_deleting";
mysql&gt; update volumes set attach_status="detached";
mysql&gt; update volumes set instance_id=0;</literal_block>
                    <paragraph>Use <literal_strong classes="command">openstack volume list</literal_strong> command to list all volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Restart the instances by using the <literal_strong classes="command">openstack server reboot
                            INSTANCE</literal_strong> command.</paragraph>
                    <important>
                        <paragraph>Some instances completely reboot and become reachable, while some might
                            stop at the plymouth stage. This is expected behavior. DO NOT reboot a
                            second time.</paragraph>
                        <paragraph>Instance state at this stage depends on whether you added an <title_reference>/etc/fstab</title_reference>
                            entry for that volume. Images built with the cloud-init package remain in
                            a <literal>pending</literal> state, while others skip the missing volume and start. You
                            perform this step to ask Compute to reboot every instance so that the
                            stored state is preserved. It does not matter if not all instances come
                            up successfully. For more information about cloud-init, see
                            <reference name="help.ubuntu.com/community/CloudInit/" refuri="https://help.ubuntu.com/community/CloudInit/">help.ubuntu.com/community/CloudInit/</reference>.</paragraph>
                    </important>
                </list_item>
                <list_item>
                    <paragraph>If required, run the <literal_strong classes="command">openstack server add volume</literal_strong> command to
                        reattach the volumes to their respective instances. This example uses a file
                        of listed volumes to reattach them:</paragraph>
                    <literal_block highlight_args="{}" language="bash" linenos="False" xml:space="preserve">#!/bin/bash

while read line; do
    volume=`echo $line | $CUT -f 1 -d " "`
    instance=`echo $line | $CUT -f 2 -d " "`
    mount_point=`echo $line | $CUT -f 3 -d " "`
        echo "ATTACHING VOLUME FOR INSTANCE - $instance"
    openstack server add volume $instance $volume $mount_point
    sleep 2
done &lt; $volumes_tmp_file</literal_block>
                    <paragraph>Instances that were stopped at the plymouth stage now automatically continue
                        booting and start normally. Instances that previously started successfully
                        can now see the volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Log in to the instances with SSH and reboot them.</paragraph>
                    <paragraph>If some services depend on the volume or if a volume has an entry in fstab,
                        you can now restart the instance. Restart directly from the instance itself
                        and not through <literal_strong classes="command">nova</literal_strong>:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># shutdown -r now</literal_block>
                    <paragraph>When you plan for and complete a disaster recovery, follow these tips:</paragraph>
                </list_item>
            </enumerated_list>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>Use the <literal>errors=remount</literal> option in the <literal>fstab</literal> file to prevent data
                        corruption.</paragraph>
                    <block_quote>
                        <paragraph>In the event of an I/O error, this option prevents writes to the disk. Add
                            this configuration option into the cinder-volume server that performs the
                            iSCSI connection to the SAN and into the instances' <literal>fstab</literal> files.</paragraph>
                    </block_quote>
                </list_item>
                <list_item>
                    <paragraph>Do not add the entry for the SAN's disks to the cinder-volume's <literal>fstab</literal>
                        file.</paragraph>
                    <block_quote>
                        <paragraph>Some systems hang on that step, which means you could lose access to your
                            cloud-controller. To re-run the session manually, run this command before
                            performing the mount:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l</literal_block>
                    </block_quote>
                </list_item>
                <list_item>
                    <paragraph>On your instances, if you have the whole <literal>/home/</literal> directory on the disk,
                        leave a user's directory with the user's bash files and the
                        <literal>authorized_keys</literal> file instead of emptying the <literal>/home/</literal> directory and
                        mapping the disk on it.</paragraph>
                    <paragraph>This action enables you to connect to the instance without the volume
                        attached, if you allow only connections through public keys.</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>To script the disaster recovery plan (DRP), use the <reference name="https://github.com/Razique" refuri="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh">https://github.com/Razique</reference><target ids="https-github-com-razique" names="https://github.com/razique" refuri="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh"></target>
                bash script.</paragraph>
            <paragraph>This script completes these steps:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Creates an array for instances and their attached volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Updates the MySQL database.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Restarts all instances with euca2ools.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Reattaches the volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Uses Compute credentials to make an SSH connection into every instance.</paragraph>
                </list_item>
            </enumerated_list>
            <paragraph>The script includes a <literal>test mode</literal>, which enables you to perform the sequence
                for only one instance.</paragraph>
            <paragraph>To reproduce the power loss, connect to the compute node that runs that
                instance and close the iSCSI session. Do not detach the volume by using the
                <literal_strong classes="command">openstack server remove volume</literal_strong> command. You must manually close the
                iSCSI session. This example closes an iSCSI session with the number <literal>15</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iscsiadm -m session -u -r 15</literal_block>
            <paragraph>Do not forget the <literal>-r</literal> option. Otherwise, all sessions close.</paragraph>
            <warning>
                <paragraph>There is potential for data loss while running instances during this
                    procedure. If you are using Liberty or earlier, ensure you have the correct
                    patch and set the options appropriately.</paragraph>
            </warning>
        </section>
    </section>
</document>
