<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/nova/doc/source/admin/configuring-migrations.rst">
    <target refid="section-configuring-compute-migrations"></target>
    <section ids="configure-live-migrations section-configuring-compute-migrations" names="configure\ live\ migrations section_configuring-compute-migrations">
        <title>Configure live migrations</title>
        <paragraph>Migration enables an administrator to move a virtual machine instance from one
            compute host to another. A typical scenario is planned maintenance on the
            source host, but migration can also be useful to redistribute the load when
            many VM instances are running on a specific physical machine.</paragraph>
        <paragraph>This document covers live migrations using the
            <reference internal="True" refid="configuring-migrations-kvm-libvirt"><inline classes="std std-ref">KVM-libvirt</inline></reference> and
            <reference internal="True" refid="configuring-migrations-xenserver"><inline classes="std std-ref">XenServer</inline></reference> hypervisors.</paragraph>
        <comment xml:space="preserve">:ref:`_configuring-migrations-kvm-libvirt`</comment>
        <comment xml:space="preserve">:ref:`_configuring-migrations-xenserver`</comment>
        <note>
            <paragraph>Not all Compute service hypervisor drivers support live-migration, or
                support all live-migration features.</paragraph>
            <paragraph>Consult the <reference name="Hypervisor Support Matrix" refuri="https://docs.openstack.org/developer/nova/support-matrix.html">Hypervisor Support Matrix</reference><target ids="hypervisor-support-matrix" names="hypervisor\ support\ matrix" refuri="https://docs.openstack.org/developer/nova/support-matrix.html"></target> to
                determine which hypervisors support live-migration.</paragraph>
            <paragraph>See the <reference name="Hypervisor configuration pages" refuri="https://docs.openstack.org/ocata/config-reference/compute/hypervisors.html">Hypervisor configuration pages</reference><target ids="hypervisor-configuration-pages" names="hypervisor\ configuration\ pages" refuri="https://docs.openstack.org/ocata/config-reference/compute/hypervisors.html"></target>
                for details on hypervisor-specific configuration settings.</paragraph>
        </note>
        <paragraph>The migration types are:</paragraph>
        <bullet_list bullet="-">
            <list_item>
                <paragraph><strong>Non-live migration</strong>, also known as cold migration or simply migration.</paragraph>
                <paragraph>The instance is shut down, then moved to another hypervisor and restarted.
                    The instance recognizes that it was rebooted, and the application running on
                    the instance is disrupted.</paragraph>
                <paragraph>This section does not cover cold migration.</paragraph>
            </list_item>
            <list_item>
                <paragraph><strong>Live migration</strong></paragraph>
                <paragraph>The instance keeps running throughout the migration.  This is useful when it
                    is not possible or desirable to stop the application running on the instance.</paragraph>
                <paragraph>Live migrations can be classified further by the way they treat instance
                    storage:</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph><strong>Shared storage-based live migration</strong>. The instance has ephemeral disks
                            that are located on storage shared between the source and destination
                            hosts.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Block live migration</strong>, or simply block migration.  The instance has
                            ephemeral disks that are not shared between the source and destination
                            hosts.  Block migration is incompatible with read-only devices such as
                            CD-ROMs and Configuration Drive (config_drive).</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Volume-backed live migration</strong>. Instances use volumes rather than
                            ephemeral disks.</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>Block live migration requires copying disks from the source to the
                    destination host. It takes more time and puts more load on the network.
                    Shared-storage and volume-backed live migration does not copy disks.</paragraph>
            </list_item>
        </bullet_list>
        <note>
            <paragraph>In a multi-cell cloud, instances can be live migrated to a
                different host in the same cell, but not across cells.</paragraph>
        </note>
        <paragraph>The following sections describe how to configure your hosts for live migrations
            using the KVM and XenServer hypervisors.</paragraph>
        <target refid="configuring-migrations-kvm-libvirt"></target>
        <section ids="kvm-libvirt configuring-migrations-kvm-libvirt" names="kvm-libvirt configuring-migrations-kvm-libvirt">
            <title>KVM-libvirt</title>
            <comment xml:space="preserve">:ref:`_configuring-migrations-kvm-general`</comment>
            <comment xml:space="preserve">:ref:`_configuring-migrations-kvm-block-and-volume-migration`</comment>
            <comment xml:space="preserve">:ref:`_configuring-migrations-kvm-shared-storage`</comment>
            <target refid="configuring-migrations-kvm-general"></target>
            <section ids="general-configuration configuring-migrations-kvm-general" names="general\ configuration configuring-migrations-kvm-general">
                <title>General configuration</title>
                <paragraph>To enable any type of live migration, configure the compute hosts according to
                    the instructions below:</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Set the following parameters in <literal>nova.conf</literal> on all compute hosts:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph><literal>vncserver_listen=0.0.0.0</literal></paragraph>
                                <paragraph>You must not make the VNC server listen to the IP address of its compute
                                    host, since that addresses changes when the instance is migrated.</paragraph>
                                <important>
                                    <paragraph>Since this setting allows VNC clients from any IP address to connect to
                                        instance consoles, you must take additional measures like secure
                                        networks or firewalls to prevent potential attackers from gaining
                                        access to instances.</paragraph>
                                </important>
                            </list_item>
                            <list_item>
                                <paragraph><literal>instances_path</literal> must have the same value for all compute hosts. In
                                    this guide, the value <literal>/var/lib/nova/instances</literal> is assumed.</paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Ensure that name resolution on all compute hosts is identical, so that they
                            can connect each other through their hostnames.</paragraph>
                        <paragraph>If you use <literal>/etc/hosts</literal> for name resolution and enable SELinux, ensure
                            that <literal>/etc/hosts</literal> has the correct SELinux context:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># restorecon /etc/hosts</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Enable password-less SSH so that root on one compute host can log on to any
                            other compute host without providing a password.  The <literal>libvirtd</literal> daemon,
                            which runs as root, uses the SSH protocol to copy the instance to the
                            destination and can't know the passwords of all compute hosts.</paragraph>
                        <paragraph>You may, for example, compile root's public SSH keys on all compute hosts
                            into an <literal>authorized_keys</literal> file and deploy that file to the compute hosts.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Configure the firewalls to allow libvirt to communicate between compute
                            hosts.</paragraph>
                        <paragraph>By default, libvirt uses the TCP port range from 49152 to 49261 for copying
                            memory and disk contents. Compute hosts must accept connections in this
                            range.</paragraph>
                        <paragraph>For information about ports used by libvirt, see the <reference name="libvirt documentation" refuri="http://libvirt.org/remote.html#Remote_libvirtd_configuration">libvirt documentation</reference><target ids="libvirt-documentation" names="libvirt\ documentation" refuri="http://libvirt.org/remote.html#Remote_libvirtd_configuration"></target>.</paragraph>
                        <important>
                            <paragraph>Be mindful of the security risks introduced by opening ports.</paragraph>
                        </important>
                    </list_item>
                </enumerated_list>
                <target refid="configuring-migrations-kvm-block-and-volume-migration"></target>
            </section>
            <section ids="block-migration-volume-based-live-migration configuring-migrations-kvm-block-and-volume-migration" names="block\ migration,\ volume-based\ live\ migration configuring-migrations-kvm-block-and-volume-migration">
                <title>Block migration, volume-based live migration</title>
                <paragraph>No additional configuration is required for block migration and volume-backed
                    live migration.</paragraph>
                <paragraph>Be aware that block migration adds load to the network and storage subsystems.</paragraph>
                <target refid="configuring-migrations-kvm-shared-storage"></target>
            </section>
            <section dupnames="shared\ storage" ids="shared-storage configuring-migrations-kvm-shared-storage" names="configuring-migrations-kvm-shared-storage">
                <title>Shared storage</title>
                <paragraph>Compute hosts have many options for sharing storage, for example NFS, shared
                    disk array LUNs, Ceph or GlusterFS.</paragraph>
                <paragraph>The next steps show how a regular Linux system might be configured as an NFS v4
                    server for live migration.  For detailed information and alternative ways to
                    configure NFS on Linux, see instructions for <reference name="Ubuntu" refuri="https://help.ubuntu.com/community/SettingUpNFSHowTo">Ubuntu</reference><target ids="ubuntu" names="ubuntu" refuri="https://help.ubuntu.com/community/SettingUpNFSHowTo"></target>, <reference name="RHEL and derivatives" refuri="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/nfs-serverconfig.html">RHEL and derivatives</reference><target ids="rhel-and-derivatives" names="rhel\ and\ derivatives" refuri="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/nfs-serverconfig.html"></target>
                    or <reference name="SLES and OpenSUSE" refuri="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_nfs_configuring-nfs-server.html">SLES and OpenSUSE</reference><target ids="sles-and-opensuse" names="sles\ and\ opensuse" refuri="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_nfs_configuring-nfs-server.html"></target>.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Ensure that UID and GID of the nova user are identical on the compute hosts
                            and the NFS server.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Create a directory with enough disk space for all instances in the cloud,
                            owned by user nova. In this guide, we assume <literal>/var/lib/nova/instances</literal>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Set the execute/search bit on the <literal>instances</literal> directory:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ chmod o+x /var/lib/nova/instances</literal_block>
                        <paragraph>This  allows qemu to access the <literal>instances</literal> directory tree.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Export <literal>/var/lib/nova/instances</literal> to the compute hosts. For example, add
                            the following line to <literal>/etc/exports</literal>:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">/var/lib/nova/instances *(rw,sync,fsid=0,no_root_squash)</literal_block>
                        <paragraph>The asterisk permits access to any NFS client. The option <literal>fsid=0</literal> exports
                            the instances directory as the NFS root.</paragraph>
                    </list_item>
                </enumerated_list>
                <paragraph>After setting up the NFS server, mount the remote filesystem on all compute
                    hosts.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Assuming the NFS server's hostname is <literal>nfs-server</literal>, add this line to
                            <literal>/etc/fstab</literal> to mount the NFS root:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">nfs-server:/ /var/lib/nova/instances nfs4 defaults 0 0</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Test NFS by mounting the instances directory and check access permissions
                            for the nova user:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ sudo mount -a -v
$ ls -ld /var/lib/nova/instances/
drwxr-xr-x. 2 nova nova 6 Mar 14 21:30 /var/lib/nova/instances/</literal_block>
                    </list_item>
                </enumerated_list>
                <target refid="configuring-migrations-kvm-advanced"></target>
            </section>
            <section ids="advanced-configuration-for-kvm-and-qemu configuring-migrations-kvm-advanced" names="advanced\ configuration\ for\ kvm\ and\ qemu configuring-migrations-kvm-advanced">
                <title>Advanced configuration for KVM and QEMU</title>
                <paragraph>Live migration copies the instance's memory from the source to the destination
                    compute host. After a memory page has been copied, the instance may write to it
                    again, so that it has to be copied again.  Instances that frequently write to
                    different memory pages can overwhelm the memory copy process and prevent the
                    live migration from completing.</paragraph>
                <paragraph>This section covers configuration settings that can help live migration of
                    memory-intensive instances succeed.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph><strong>Live migration completion timeout</strong></paragraph>
                        <paragraph>The Compute service aborts a migration when it has been running for too
                            long.  The timeout is calculated based on the instance size, which is the
                            instance's memory size in GiB. In the case of block migration, the size of
                            ephemeral storage in GiB is added.</paragraph>
                        <paragraph>The timeout in seconds is the instance size multiplied by the configurable
                            parameter <literal>live_migration_completion_timeout</literal>, whose default is 800. For
                            example, shared-storage live migration of an instance with 8GiB memory will
                            time out after 6400 seconds.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Live migration progress timeout</strong></paragraph>
                        <paragraph>The Compute service also aborts a live migration when it detects that memory
                            copy is not making progress for a certain time. You can set this time, in
                            seconds, through the configurable parameter
                            <literal>live_migration_progress_timeout</literal>.</paragraph>
                        <paragraph>In Ocata, the default value of <literal>live_migration_progress_timeout</literal> is 0,
                            which disables progress timeouts. You should not change this value, since
                            the algorithm that detects memory copy progress has been determined to be
                            unreliable. It may be re-enabled in future releases.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Instance downtime</strong></paragraph>
                        <paragraph>Near the end of the memory copy, the instance is paused for a short time so
                            that the remaining few pages can be copied without interference from
                            instance memory writes. The Compute service initializes this time to a small
                            value that depends on the instance size, typically around 50 milliseconds.
                            When it notices that the memory copy does not make sufficient progress, it
                            increases the time gradually.</paragraph>
                        <paragraph>You can influence the instance downtime algorithm with the help of three
                            configuration variables on the compute hosts:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">live_migration_downtime = 500
live_migration_downtime_steps = 10
live_migration_downtime_delay = 75</literal_block>
                        <paragraph><literal>live_migration_downtime</literal> sets the maximum permitted downtime for a live
                            migration, in <emphasis>milliseconds</emphasis>.  The default is 500.</paragraph>
                        <paragraph><literal>live_migration_downtime_steps</literal> sets the total number of adjustment steps
                            until <literal>live_migration_downtime</literal> is reached.  The default is 10 steps.</paragraph>
                        <paragraph><literal>live_migration_downtime_delay</literal> sets the time interval between two
                            adjustment steps in <emphasis>seconds</emphasis>. The default is 75.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Auto-convergence</strong></paragraph>
                        <paragraph>One strategy for a successful live migration of a memory-intensive instance
                            is slowing the instance down. This is called auto-convergence.  Both libvirt
                            and QEMU implement this feature by automatically throttling the instance's
                            CPU when memory copy delays are detected.</paragraph>
                        <paragraph>Auto-convergence is disabled by default.  You can enable it by setting
                            <literal>live_migration_permit_auto_convergence=true</literal>.</paragraph>
                        <caution>
                            <paragraph>Before enabling auto-convergence, make sure that the instance's
                                application tolerates a slow-down.</paragraph>
                            <paragraph>Be aware that auto-convergence does not guarantee live migration success.</paragraph>
                        </caution>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Post-copy</strong></paragraph>
                        <paragraph>Live migration of a memory-intensive instance is certain to succeed when you
                            enable post-copy. This feature, implemented by libvirt and QEMU, activates
                            the virtual machine on the destination host before all of its memory has
                            been copied.  When the virtual machine accesses a page that is missing on
                            the destination host, the resulting page fault is resolved by copying the
                            page from the source host.</paragraph>
                        <paragraph>Post-copy is disabled by default. You can enable it by setting
                            <literal>live_migration_permit_post_copy=true</literal>.</paragraph>
                        <paragraph>When you enable both auto-convergence and post-copy, auto-convergence
                            remains disabled.</paragraph>
                        <caution>
                            <paragraph>The page faults introduced by post-copy can slow the instance down.</paragraph>
                            <paragraph>When the network connection between source and destination host is
                                interrupted, page faults cannot be resolved anymore and the instance is
                                rebooted.</paragraph>
                        </caution>
                    </list_item>
                </enumerated_list>
                <comment xml:space="preserve">TODO Bernd: I *believe* that it is certain to succeed,</comment>
                <comment xml:space="preserve">but perhaps I am missing something.</comment>
                <paragraph>The full list of live migration configuration parameters is documented in the
                    <reference internal="True" refuri="../configuration/config"><inline classes="doc">Nova Configuration Options</inline></reference></paragraph>
                <target refid="configuring-migrations-xenserver"></target>
            </section>
        </section>
        <section ids="xenserver configuring-migrations-xenserver" names="xenserver configuring-migrations-xenserver">
            <title>XenServer</title>
            <comment xml:space="preserve">:ref:Shared Storage</comment>
            <comment xml:space="preserve">:ref:Block migration</comment>
            <target refid="configuring-migrations-xenserver-shared-storage"></target>
            <section dupnames="shared\ storage" ids="configuring-migrations-xenserver-shared-storage id1" names="configuring-migrations-xenserver-shared-storage">
                <title>Shared storage</title>
                <paragraph><strong>Prerequisites</strong></paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph><strong>Compatible XenServer hypervisors</strong>.</paragraph>
                        <paragraph>For more information, see the <reference name="Requirements for Creating Resource Pools" refuri="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#pooling_homogeneity_requirements">Requirements for Creating Resource Pools</reference><target ids="requirements-for-creating-resource-pools" names="requirements\ for\ creating\ resource\ pools" refuri="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#pooling_homogeneity_requirements"></target>
                            section of the XenServer Administrator's Guide.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph><strong>Shared storage</strong>.</paragraph>
                        <paragraph>An NFS export, visible to all XenServer hosts.</paragraph>
                        <block_quote>
                            <note>
                                <paragraph>For the supported NFS versions, see the <reference name="NFS VHD" refuri="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701">NFS VHD</reference><target ids="nfs-vhd" names="nfs\ vhd" refuri="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701"></target>
                                    section of the XenServer Administrator's Guide.</paragraph>
                            </note>
                        </block_quote>
                    </list_item>
                </bullet_list>
                <paragraph>To use shared storage live migration with XenServer hypervisors, the hosts must
                    be joined to a XenServer pool. To create that pool, a host aggregate must be
                    created with specific metadata. This metadata is used by the XAPI plug-ins to
                    establish the pool.</paragraph>
                <rubric>Using shared storage live migrations with XenServer Hypervisors</rubric>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Add an NFS VHD storage to your master XenServer, and set it as the default
                            storage repository. For more information, see NFS VHD in the XenServer
                            Administrator's Guide.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Configure all compute nodes to use the default storage repository (<literal>sr</literal>)
                            for pool operations. Add this line to your <literal>nova.conf</literal> configuration files
                            on all compute nodes:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">sr_matching_filter=default-sr:true</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create a host aggregate. This command creates the aggregate, and then
                            displays a table that contains the ID of the new aggregate</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack aggregate create --zone AVAILABILITY_ZONE POOL_NAME</literal_block>
                        <paragraph>Add metadata to the aggregate, to mark it as a hypervisor pool</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack aggregate set --property hypervisor_pool=true AGGREGATE_ID

$ openstack aggregate set --property operational_state=created AGGREGATE_ID</literal_block>
                        <paragraph>Make the first compute node part of that aggregate</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack aggregate add host AGGREGATE_ID MASTER_COMPUTE_NAME</literal_block>
                        <paragraph>The host is now part of a XenServer pool.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Add hosts to the pool</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack aggregate add host AGGREGATE_ID COMPUTE_HOST_NAME</literal_block>
                        <note>
                            <paragraph>The added compute node and the host will shut down to join the host to
                                the XenServer pool. The operation will fail if any server other than the
                                compute node is running or suspended on the host.</paragraph>
                        </note>
                    </list_item>
                </enumerated_list>
                <target refid="configuring-migrations-xenserver-block-migration"></target>
            </section>
            <section ids="block-migration configuring-migrations-xenserver-block-migration" names="block\ migration configuring-migrations-xenserver-block-migration">
                <title>Block migration</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph><strong>Compatible XenServer hypervisors</strong>.</paragraph>
                        <paragraph>The hypervisors must support the Storage XenMotion feature.  See your
                            XenServer manual to make sure your edition has this feature.</paragraph>
                        <block_quote>
                            <note>
                                <bullet_list bullet="-">
                                    <list_item>
                                        <paragraph>To use block migration, you must use the <literal>--block-migrate</literal> parameter
                                            with the live migration command.</paragraph>
                                    </list_item>
                                    <list_item>
                                        <paragraph>Block migration works only with EXT local storage storage repositories,
                                            and the server must not have any volumes attached.</paragraph>
                                    </list_item>
                                </bullet_list>
                            </note>
                        </block_quote>
                    </list_item>
                </bullet_list>
            </section>
        </section>
    </section>
</document>
