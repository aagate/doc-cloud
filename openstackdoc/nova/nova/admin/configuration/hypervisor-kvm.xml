<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/nova/doc/source/admin/configuration/hypervisor-kvm.rst">
    <section ids="kvm" names="kvm">
        <title>KVM</title>
        <target refid="index-0"></target>
        <todo_node classes="admonition-todo" ids="index-0">
            <title>Todo</title>
            <paragraph>This is really installation guide material and should probably be
                moved.</paragraph>
        </todo_node>
        <paragraph>KVM is configured as the default hypervisor for Compute.</paragraph>
        <note>
            <paragraph>This document contains several sections about hypervisor selection.  If you
                are reading this document linearly, you do not want to load the KVM module
                before you install <literal>nova-compute</literal>.  The <literal>nova-compute</literal> service depends
                on qemu-kvm, which installs <literal>/lib/udev/rules.d/45-qemu-kvm.rules</literal>, which
                sets the correct permissions on the <literal>/dev/kvm</literal> device node.</paragraph>
        </note>
        <paragraph>To enable KVM explicitly, add the following configuration options to the
            <literal>/etc/nova/nova.conf</literal> file:</paragraph>
        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">compute_driver = libvirt.LibvirtDriver

[libvirt]
virt_type = kvm</literal_block>
        <paragraph>The KVM hypervisor supports the following virtual machine image formats:</paragraph>
        <bullet_list bullet="*">
            <list_item>
                <paragraph>Raw</paragraph>
            </list_item>
            <list_item>
                <paragraph>QEMU Copy-on-write (QCOW2)</paragraph>
            </list_item>
            <list_item>
                <paragraph>QED Qemu Enhanced Disk</paragraph>
            </list_item>
            <list_item>
                <paragraph>VMware virtual machine disk format (vmdk)</paragraph>
            </list_item>
        </bullet_list>
        <paragraph>This section describes how to enable KVM on your system.  For more information,
            see the following distribution-specific documentation:</paragraph>
        <bullet_list bullet="*">
            <list_item>
                <paragraph><reference name="Fedora: Virtualization Getting Started Guide" refuri="http://docs.fedoraproject.org/en-US/Fedora/22/html/Virtualization_Getting_Started_Guide/index.html">Fedora: Virtualization Getting Started Guide</reference><target ids="fedora-virtualization-getting-started-guide" names="fedora:\ virtualization\ getting\ started\ guide" refuri="http://docs.fedoraproject.org/en-US/Fedora/22/html/Virtualization_Getting_Started_Guide/index.html"></target>
                    from the Fedora 22 documentation.</paragraph>
            </list_item>
            <list_item>
                <paragraph><reference name="Ubuntu: KVM/Installation" refuri="https://help.ubuntu.com/community/KVM/Installation">Ubuntu: KVM/Installation</reference><target ids="ubuntu-kvm-installation" names="ubuntu:\ kvm/installation" refuri="https://help.ubuntu.com/community/KVM/Installation"></target> from the Community Ubuntu documentation.</paragraph>
            </list_item>
            <list_item>
                <paragraph><reference name="Debian: Virtualization with KVM" refuri="http://static.debian-handbook.info/browse/stable/sect.virtualization.html#idp11279352">Debian: Virtualization with KVM</reference><target ids="debian-virtualization-with-kvm" names="debian:\ virtualization\ with\ kvm" refuri="http://static.debian-handbook.info/browse/stable/sect.virtualization.html#idp11279352"></target> from the Debian handbook.</paragraph>
            </list_item>
            <list_item>
                <paragraph><reference name="Red Hat Enterprise Linux: Installing virtualization packages on an existing Red Hat Enterprise Linux system" refuri="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Host_Installation-Installing_KVM_packages_on_an_existing_Red_Hat_Enterprise_Linux_system.html">Red Hat Enterprise Linux: Installing virtualization packages on an existing
                        Red Hat Enterprise Linux system</reference><target ids="red-hat-enterprise-linux-installing-virtualization-packages-on-an-existing-red-hat-enterprise-linux-system" names="red\ hat\ enterprise\ linux:\ installing\ virtualization\ packages\ on\ an\ existing\ red\ hat\ enterprise\ linux\ system" refuri="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Host_Installation-Installing_KVM_packages_on_an_existing_Red_Hat_Enterprise_Linux_system.html"></target> from the <literal>Red Hat Enterprise Linux
Virtualization Host Configuration and Guest Installation Guide</literal>.</paragraph>
            </list_item>
            <list_item>
                <paragraph><reference name="openSUSE: Installing KVM" refuri="http://doc.opensuse.org/documentation/html/openSUSE/opensuse-kvm/cha.kvm.requires.html#sec.kvm.requires.install">openSUSE: Installing KVM</reference><target ids="opensuse-installing-kvm" names="opensuse:\ installing\ kvm" refuri="http://doc.opensuse.org/documentation/html/openSUSE/opensuse-kvm/cha.kvm.requires.html#sec.kvm.requires.install"></target>
                    from the openSUSE Virtualization with KVM manual.</paragraph>
            </list_item>
            <list_item>
                <paragraph><reference name="SLES: Installing KVM" refuri="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_installation_kvm.html">SLES: Installing KVM</reference><target ids="sles-installing-kvm" names="sles:\ installing\ kvm" refuri="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_installation_kvm.html"></target> from the SUSE Linux Enterprise Server
                    <literal>Virtualization Guide</literal>.</paragraph>
            </list_item>
        </bullet_list>
        <target refid="enable-kvm"></target>
        <section ids="enable-kvm id1" names="enable\ kvm enable-kvm">
            <title>Enable KVM</title>
            <paragraph>The following sections outline how to enable KVM based hardware virtualization
                on different architectures and platforms.  To perform these steps, you must be
                logged in as the <literal>root</literal> user.</paragraph>
            <section ids="for-x86-based-systems" names="for\ x86\ based\ systems">
                <title>For x86 based systems</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>To determine whether the <literal>svm</literal> or <literal>vmx</literal> CPU extensions are present, run
                            this command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># grep -E 'svm|vmx' /proc/cpuinfo</literal_block>
                        <paragraph>This command generates output if the CPU is capable of
                            hardware-virtualization. Even if output is shown, you might still need to
                            enable virtualization in the system BIOS for full support.</paragraph>
                        <paragraph>If no output appears, consult your system documentation to ensure that your
                            CPU and motherboard support hardware virtualization.  Verify that any
                            relevant hardware virtualization options are enabled in the system BIOS.</paragraph>
                        <paragraph>The BIOS for each manufacturer is different. If you must enable
                            virtualization in the BIOS, look for an option containing the words
                            <literal>virtualization</literal>, <literal>VT</literal>, <literal>VMX</literal>, or <literal>SVM</literal>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>To list the loaded kernel modules and verify that the <literal>kvm</literal> modules are
                            loaded, run this command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># lsmod | grep kvm</literal_block>
                        <paragraph>If the output includes <literal>kvm_intel</literal> or <literal>kvm_amd</literal>, the <literal>kvm</literal> hardware
                            virtualization modules are loaded and your kernel meets the module
                            requirements for OpenStack Compute.</paragraph>
                        <paragraph>If the output does not show that the <literal>kvm</literal> module is loaded, run this
                            command to load it:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe -a kvm</literal_block>
                        <paragraph>Run the command for your CPU. For Intel, run this command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe -a kvm-intel</literal_block>
                        <paragraph>For AMD, run this command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe -a kvm-amd</literal_block>
                        <paragraph>Because a KVM installation can change user group membership, you might need
                            to log in again for changes to take effect.</paragraph>
                        <paragraph>If the kernel modules do not load automatically, use the procedures listed
                            in these subsections.</paragraph>
                    </list_item>
                </enumerated_list>
                <paragraph>If the checks indicate that required hardware virtualization support or kernel
                    modules are disabled or unavailable, you must either enable this support on the
                    system or find a system with this support.</paragraph>
                <note>
                    <paragraph>Some systems require that you enable VT support in the system BIOS.  If you
                        believe your processor supports hardware acceleration but the previous
                        command did not produce output, reboot your machine, enter the system BIOS,
                        and enable the VT option.</paragraph>
                </note>
                <paragraph>If KVM acceleration is not supported, configure Compute to use a different
                    hypervisor, such as <literal>QEMU</literal> or <literal>Xen</literal>. See <reference internal="True" refuri="hypervisor-qemu#compute-qemu"><inline classes="std std-ref">QEMU</inline></reference> or
                    <reference internal="True" refuri="hypervisor-xen-api#compute-xen-api"><inline classes="std std-ref">XenServer (and other XAPI based Xen variants)</inline></reference> for details.</paragraph>
                <paragraph>These procedures help you load the kernel modules for Intel-based and AMD-based
                    processors if they do not load automatically during KVM installation.</paragraph>
                <rubric>Intel-based processors</rubric>
                <paragraph>If your compute host is Intel-based, run these commands as root to load the
                    kernel modules:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe kvm
# modprobe kvm-intel</literal_block>
                <paragraph>Add these lines to the <literal>/etc/modules</literal> file so that these modules load on
                    reboot:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">kvm
kvm-intel</literal_block>
                <rubric>AMD-based processors</rubric>
                <paragraph>If your compute host is AMD-based, run these commands as root to load the
                    kernel modules:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe kvm
# modprobe kvm-amd</literal_block>
                <paragraph>Add these lines to <literal>/etc/modules</literal> file so that these modules load on reboot:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">kvm
kvm-amd</literal_block>
            </section>
            <section ids="for-power-based-systems" names="for\ power\ based\ systems">
                <title>For POWER based systems</title>
                <paragraph>KVM as a hypervisor is supported on POWER system's PowerNV platform.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>To determine if your POWER platform supports KVM based virtualization run
                            the following command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># cat /proc/cpuinfo | grep PowerNV</literal_block>
                        <paragraph>If the previous command generates the following output, then CPU supports
                            KVM based virtualization.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">platform: PowerNV</literal_block>
                        <paragraph>If no output is displayed, then your POWER platform does not support KVM
                            based hardware virtualization.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>To list the loaded kernel modules and verify that the <literal>kvm</literal> modules are
                            loaded, run the following command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># lsmod | grep kvm</literal_block>
                        <paragraph>If the output includes <literal>kvm_hv</literal>, the <literal>kvm</literal> hardware virtualization
                            modules are loaded and your kernel meets the module requirements for
                            OpenStack Compute.</paragraph>
                        <paragraph>If the output does not show that the <literal>kvm</literal> module is loaded, run the
                            following command to load it:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe -a kvm</literal_block>
                        <paragraph>For PowerNV platform, run the following command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe -a kvm-hv</literal_block>
                        <paragraph>Because a KVM installation can change user group membership, you might need
                            to log in again for changes to take effect.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
        </section>
        <section ids="configure-compute-backing-storage" names="configure\ compute\ backing\ storage">
            <title>Configure Compute backing storage</title>
            <paragraph>Backing Storage is the storage used to provide the expanded operating system
                image, and any ephemeral storage. Inside the virtual machine, this is normally
                presented as two virtual hard disks (for example, <literal>/dev/vda</literal> and <literal>/dev/vdb</literal>
                respectively). However, inside OpenStack, this can be derived from one of these
                methods: <literal>lvm</literal>, <literal>qcow</literal>, <literal>rbd</literal> or <literal>flat</literal>, chosen using the
                <literal>images_type</literal> option in <literal>nova.conf</literal> on the compute node.</paragraph>
            <note>
                <paragraph>The option <literal>raw</literal> is acceptable but deprecated in favor of <literal>flat</literal>.  The
                    Flat back end uses either raw or QCOW2 storage. It never uses a backing
                    store, so when using QCOW2 it copies an image rather than creating an
                    overlay. By default, it creates raw files but will use QCOW2 when creating a
                    disk from a QCOW2 if <literal>force_raw_images</literal> is not set in configuration.</paragraph>
            </note>
            <paragraph>QCOW is the default backing store. It uses a copy-on-write philosophy to delay
                allocation of storage until it is actually needed. This means that the space
                required for the backing of an image can be significantly less on the real disk
                than what seems available in the virtual machine operating system.</paragraph>
            <paragraph>Flat creates files without any sort of file formatting, effectively creating
                files with the plain binary one would normally see on a real disk. This can
                increase performance, but means that the entire size of the virtual disk is
                reserved on the physical disk.</paragraph>
            <paragraph>Local <reference name="LVM volumes" refuri="https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)">LVM volumes</reference> can also be
                used. Set <literal>images_volume_group = nova_local</literal> where <literal>nova_local</literal> is the name
                of the LVM group you have created.</paragraph>
        </section>
        <section ids="specify-the-cpu-model-of-kvm-guests" names="specify\ the\ cpu\ model\ of\ kvm\ guests">
            <title>Specify the CPU model of KVM guests</title>
            <paragraph>The Compute service enables you to control the guest CPU model that is exposed
                to KVM virtual machines. Use cases include:</paragraph>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>To maximize performance of virtual machines by exposing new host CPU features
                        to the guest</paragraph>
                </list_item>
                <list_item>
                    <paragraph>To ensure a consistent default CPU across all machines, removing reliance of
                        variable QEMU defaults</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>In libvirt, the CPU is specified by providing a base CPU model name (which is a
                shorthand for a set of feature flags), a set of additional feature flags, and
                the topology (sockets/cores/threads).  The libvirt KVM driver provides a number
                of standard CPU model names.  These models are defined in the
                <literal>/usr/share/libvirt/cpu_map.xml</literal> file.  Check this file to determine which
                models are supported by your local installation.</paragraph>
            <paragraph>Two Compute configuration options in the <literal>[libvirt]</literal> group of <literal>nova.conf</literal>
                define which type of CPU model is exposed to the hypervisor when using KVM:
                <literal>cpu_mode</literal> and <literal>cpu_model</literal>.</paragraph>
            <paragraph>The <literal>cpu_mode</literal> option can take one of the following values: <literal>none</literal>,
                <literal>host-passthrough</literal>, <literal>host-model</literal>, and <literal>custom</literal>.</paragraph>
            <section ids="host-model-default-for-kvm-qemu" names="host\ model\ (default\ for\ kvm\ &amp;\ qemu)">
                <title>Host model (default for KVM &amp; QEMU)</title>
                <paragraph>If your <literal>nova.conf</literal> file contains <literal>cpu_mode=host-model</literal>, libvirt identifies
                    the CPU model in <literal>/usr/share/libvirt/cpu_map.xml</literal> file that most closely
                    matches the host, and requests additional CPU flags to complete the match. This
                    configuration provides the maximum functionality and performance and maintains
                    good reliability and compatibility if the guest is migrated to another host
                    with slightly different host CPUs.</paragraph>
            </section>
            <section ids="host-pass-through" names="host\ pass\ through">
                <title>Host pass through</title>
                <paragraph>If your <literal>nova.conf</literal> file contains <literal>cpu_mode=host-passthrough</literal>, libvirt
                    tells KVM to pass through the host CPU with no modifications.  The difference
                    to host-model, instead of just matching feature flags, every last detail of the
                    host CPU is matched. This gives the best performance, and can be important to
                    some apps which check low level CPU details, but it comes at a cost with
                    respect to migration.  The guest can only be migrated to a matching host CPU.</paragraph>
            </section>
            <section ids="custom" names="custom">
                <title>Custom</title>
                <paragraph>If your <literal>nova.conf</literal> file contains <literal>cpu_mode=custom</literal>, you can explicitly
                    specify one of the supported named models using the cpu_model configuration
                    option. For example, to configure the KVM guests to expose Nehalem CPUs, your
                    <literal>nova.conf</literal> file should contain:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[libvirt]
cpu_mode = custom
cpu_model = Nehalem</literal_block>
            </section>
            <section ids="none-default-for-all-libvirt-driven-hypervisors-other-than-kvm-qemu" names="none\ (default\ for\ all\ libvirt-driven\ hypervisors\ other\ than\ kvm\ &amp;\ qemu)">
                <title>None (default for all libvirt-driven hypervisors other than KVM &amp; QEMU)</title>
                <paragraph>If your <literal>nova.conf</literal> file contains <literal>cpu_mode=none</literal>, libvirt does not specify
                    a CPU model. Instead, the hypervisor chooses the default model.</paragraph>
            </section>
            <section ids="guest-agent-support" names="guest\ agent\ support">
                <title>Guest agent support</title>
                <paragraph>Use guest agents to enable optional access between compute nodes and guests
                    through a socket, using the QMP protocol.</paragraph>
                <paragraph>To enable this feature, you must set <literal>hw_qemu_guest_agent=yes</literal> as a metadata
                    parameter on the image you wish to use to create the guest-agent-capable
                    instances from. You can explicitly disable the feature by setting
                    <literal>hw_qemu_guest_agent=no</literal> in the image metadata.</paragraph>
            </section>
        </section>
        <section ids="kvm-performance-tweaks" names="kvm\ performance\ tweaks">
            <title>KVM performance tweaks</title>
            <paragraph>The <reference name="VHostNet" refuri="http://www.linux-kvm.org/page/VhostNet">VHostNet</reference><target ids="vhostnet" names="vhostnet" refuri="http://www.linux-kvm.org/page/VhostNet"></target> kernel module improves
                network performance. To load the kernel module, run the following command as
                root:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe vhost_net</literal_block>
        </section>
        <section ids="troubleshoot-kvm" names="troubleshoot\ kvm">
            <title>Troubleshoot KVM</title>
            <paragraph>Trying to launch a new virtual machine instance fails with the <literal>ERROR</literal> state,
                and the following error appears in the <literal>/var/log/nova/nova-compute.log</literal> file:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">libvirtError: internal error no supported architecture for os type 'hvm'</literal_block>
            <paragraph>This message indicates that the KVM kernel modules were not loaded.</paragraph>
            <paragraph>If you cannot start VMs after installation without rebooting, the permissions
                might not be set correctly. This can happen if you load the KVM module before
                you install <literal>nova-compute</literal>.  To check whether the group is set to <literal>kvm</literal>,
                run:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ls -l /dev/kvm</literal_block>
            <paragraph>If it is not set to <literal>kvm</literal>, run:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># udevadm trigger</literal_block>
        </section>
    </section>
</document>
