<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/nova/doc/source/user/cellsv2_layout.rst">
    <comment xml:space="preserve">Licensed under the Apache License, Version 2.0 (the "License"); you may
not use this file except in compliance with the License. You may obtain
a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.</comment>
    <section ids="cells-layout-v2" names="cells\ layout\ (v2)">
        <title>Cells Layout (v2)</title>
        <paragraph>This document describes the layout of a deployment with Cells
            version 2, including deployment considerations for security and
            scale. It is focused on code present in Pike and later, and while it
            is geared towards people who want to have multiple cells for whatever
            reason, the nature of the cellsv2 support in Nova means that it
            applies in some way to all deployments.</paragraph>
        <note>
            <paragraph>The concepts laid out in this document do not in any way
                relate to CellsV1, which includes the <literal>nova-cells</literal>
                service, and the <literal>[cells]</literal> section of the configuration
                file. For more information on the differences, see the main
                <reference internal="True" refuri="cells#cells"><inline classes="std std-ref">Cells</inline></reference> page.</paragraph>
        </note>
        <section ids="concepts" names="concepts">
            <title>Concepts</title>
            <paragraph>A basic Nova system consists of the following components:</paragraph>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>The nova-api service which provides the external REST API to users.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The nova-scheduler and placement services which are responsible
                        for tracking resources and deciding which compute node instances
                        should be on.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>An "API database" that is used primarily by nova-api and
                        nova-scheduler (called <emphasis>API-level services</emphasis> below) to track location
                        information about instances, as well as a temporary location for
                        instances being built but not yet scheduled.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The nova-conductor service which offloads long-running tasks for the
                        API-level service, as well as insulates compute nodes from direct
                        database access</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The nova-compute service which manages the virt driver and
                        hypervisor host.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>A "cell database" which is used by API, conductor and compute
                        services, and which houses the majority of the information about
                        instances.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>A "cell0 database" which is just like the cell database, but
                        contains only instances that failed to be scheduled.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>A message queue which allows the services to communicate with each
                        other via RPC.</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>All deployments have at least the above components. Small deployments
                likely have a single message queue that all services share, and a
                single database server which hosts the API database, a single cell
                database, as well as the required cell0 database. This is considered a
                "single-cell deployment" because it only has one "real" cell. The
                cell0 database mimics a regular cell, but has no compute nodes and is
                used only as a place to put instances that fail to land on a real
                compute node (and thus a real cell).</paragraph>
            <paragraph>The purpose of the cells functionality in nova is specifically to
                allow larger deployments to shard their many compute nodes into cells,
                each of which has a database and message queue. The API database is
                always and only global, but there can be many cell databases (where
                the bulk of the instance information lives), each with a portion of
                the instances for the entire deployment within.</paragraph>
            <paragraph>All of the nova services use a configuration file, all of which will
                at a minimum specify a message queue endpoint
                (i.e. <literal>[DEFAULT]/transport_url</literal>). Most of the services also require
                configuration of database connection information
                (i.e. <literal>[database]/connection</literal>). API-level services that need access
                to the global routing and placement information will also be
                configured to reach the API database
                (i.e. <literal>[api_database]/connection</literal>).</paragraph>
            <note>
                <paragraph>The pair of <literal>transport_url</literal> and <literal>[database]/connection</literal>
                    configured for a service defines what cell a service lives
                    in.</paragraph>
            </note>
            <paragraph>API-level services need to be able to contact other services in all of
                the cells. Since they only have one configured <literal>transport_url</literal> and
                <literal>[database]/connection</literal> they look up the information for the other
                cells in the API database, with records called <emphasis>cell mappings</emphasis>.</paragraph>
            <note>
                <paragraph>The API database must have cell mapping records that match
                    the <literal>transport_url</literal> and <literal>[database]/connection</literal>
                    configuration elements of the lower-level services. See the
                    <literal>nova-manage</literal> <reference internal="True" refuri="../cli/nova-manage#man-page-cells-v2"><inline classes="std std-ref">Nova Cells v2</inline></reference> commands for more
                    information about how to create and examine these records.</paragraph>
            </note>
        </section>
        <section ids="service-layout" names="service\ layout">
            <title>Service Layout</title>
            <paragraph>The services generally have a well-defined communication pattern that
                dictates their layout in a deployment. In a small/simple scenario, the
                rules do not have much of an impact as all the services can
                communicate with each other on a single message bus and in a single
                cell database. However, as the deployment grows, scaling and security
                concerns may drive separation and isolation of the services.</paragraph>
            <section ids="simple" names="simple">
                <title>Simple</title>
                <paragraph>This is a diagram of the basic services that a simple (single-cell)
                    deployment would have, as well as the relationships
                    (i.e. communication paths) between them:</paragraph>
                <graphviz code='digraph services {&#10;  graph [pad="0.35", ranksep="0.65", nodesep="0.55", concentrate=true];&#10;  node [fontsize=10 fontname="Monospace"];&#10;  edge [arrowhead="normal", arrowsize="0.8"];&#10;  labelloc=bottom;&#10;  labeljust=left;&#10;&#10;  { rank=same&#10;    api [label="nova-api"]&#10;    apidb [label="API Database" shape="box"]&#10;    scheduler [label="nova-scheduler"]&#10;  }&#10;  { rank=same&#10;    mq [label="MQ" shape="diamond"]&#10;    conductor [label="nova-conductor"]&#10;  }&#10;  { rank=same&#10;    cell0db [label="Cell0 Database" shape="box"]&#10;    celldb [label="Cell Database" shape="box"]&#10;    compute [label="nova-compute"]&#10;  }&#10;&#10;  api -&gt; mq -&gt; compute&#10;  conductor -&gt; mq -&gt; scheduler&#10;&#10;  api -&gt; apidb&#10;  api -&gt; cell0db&#10;  api -&gt; celldb&#10;&#10;  conductor -&gt; apidb&#10;  conductor -&gt; cell0db&#10;  conductor -&gt; celldb&#10;}' options="{}">
                </graphviz>
                <paragraph>All of the services are configured to talk to each other over the same
                    message bus, and there is only one cell database where live instance
                    data resides. The cell0 database is present (and required) but as no
                    compute nodes are connected to it, this is still a "single cell"
                    deployment.</paragraph>
            </section>
            <section ids="multiple-cells" names="multiple\ cells">
                <title>Multiple Cells</title>
                <paragraph>In order to shard the services into multiple cells, a number of things
                    must happen. First, the message bus must be split into pieces along
                    the same lines as the cell database. Second, a dedicated conductor
                    must be run for the API-level services, with access to the API
                    database and a dedicated message queue. We call this <emphasis>super conductor</emphasis>
                    to distinguish its place and purpose from the per-cell conductor nodes.</paragraph>
                <graphviz code='digraph services2 {&#10;  graph [pad="0.35", ranksep="0.65", nodesep="0.55", concentrate=true];&#10;  node [fontsize=10 fontname="Monospace"];&#10;  edge [arrowhead="normal", arrowsize="0.8"];&#10;  labelloc=bottom;&#10;  labeljust=left;&#10;&#10;  subgraph api {&#10;    api [label="nova-api"]&#10;    scheduler [label="nova-scheduler"]&#10;    conductor [label="super conductor"]&#10;    { rank=same&#10;      apimq [label="API MQ" shape="diamond"]&#10;      apidb [label="API Database" shape="box"]&#10;    }&#10;&#10;    api -&gt; apimq -&gt; conductor&#10;    api -&gt; apidb&#10;    conductor -&gt; apimq -&gt; scheduler&#10;    conductor -&gt; apidb&#10;  }&#10;&#10;  subgraph clustercell0 {&#10;    label="Cell 0"&#10;    color=green&#10;    cell0db [label="Cell Database" shape="box"]&#10;  }&#10;&#10;  subgraph clustercell1 {&#10;    label="Cell 1"&#10;    color=blue&#10;    mq1 [label="Cell MQ" shape="diamond"]&#10;    cell1db [label="Cell Database" shape="box"]&#10;    conductor1 [label="nova-conductor"]&#10;    compute1 [label="nova-compute"]&#10;&#10;    conductor1 -&gt; mq1 -&gt; compute1&#10;    conductor1 -&gt; cell1db&#10;&#10;  }&#10;&#10;  subgraph clustercell2 {&#10;    label="Cell 2"&#10;    color=red&#10;    mq2 [label="Cell MQ" shape="diamond"]&#10;    cell2db [label="Cell Database" shape="box"]&#10;    conductor2 [label="nova-conductor"]&#10;    compute2 [label="nova-compute"]&#10;&#10;    conductor2 -&gt; mq2 -&gt; compute2&#10;    conductor2 -&gt; cell2db&#10;  }&#10;&#10;  api -&gt; mq1 -&gt; conductor1&#10;  api -&gt; mq2 -&gt; conductor2&#10;  api -&gt; cell0db&#10;  api -&gt; cell1db&#10;  api -&gt; cell2db&#10;&#10;  conductor -&gt; cell0db&#10;  conductor -&gt; cell1db&#10;  conductor -&gt; mq1&#10;  conductor -&gt; cell2db&#10;  conductor -&gt; mq2&#10;}' options="{}">
                </graphviz>
                <paragraph>It is important to note that services in the lower cell boxes do not
                    have the ability to call back to the API-layer services via RPC, nor
                    do they have access to the API database for global visibility of
                    resources across the cloud. This is intentional and provides security
                    and failure domain isolation benefits, but also has impacts on some
                    things that would otherwise require this any-to-any communication
                    style. Check the release notes for the version of Nova you are using
                    for the most up-to-date information about any caveats that may be
                    present due to this limitation.</paragraph>
            </section>
            <section ids="caveats-of-a-multi-cell-deployment" names="caveats\ of\ a\ multi-cell\ deployment">
                <title>Caveats of a Multi-Cell deployment</title>
                <comment xml:space="preserve">note: This information is correct as of the Pike release.</comment>
                <section ids="cross-cell-instance-migrations" names="cross-cell\ instance\ migrations">
                    <title>Cross-cell instance migrations</title>
                    <paragraph>Currently it is not possible to migrate an instance from a host in one
                        cell to a host in another cell. This may be possible in the future,
                        but it is currently unsupported. This impacts cold migration,
                        resizes, live migrations, evacuate, and unshelve operations.</paragraph>
                </section>
                <section ids="quota-related-quirks" names="quota-related\ quirks">
                    <title>Quota-related quirks</title>
                    <paragraph>Quotas are now calculated live at the point at which an operation
                        would consume more resource, instead of being kept statically in the
                        database. This means that a multi-cell environment may incorrectly
                        calculate the usage of a tenant if one of the cells is unreachable, as
                        those resources cannot be counted. In this case, the tenant may be
                        able to consume more resource from one of the available cells, putting
                        them far over quota when the unreachable cell returns. In the future,
                        placement will provide us with a consistent way to calculate usage
                        independent of the actual cell being reachable.</paragraph>
                </section>
                <section ids="performance-of-listing-instances" names="performance\ of\ listing\ instances">
                    <title>Performance of listing instances</title>
                    <paragraph>With multiple cells, the instance list operation may not sort and
                        paginate results properly when crossing multiple cell
                        boundaries. Further, the performance of a sorted list operation will
                        be considerably slower than with a single cell.</paragraph>
                </section>
                <section ids="notifications" names="notifications">
                    <title>Notifications</title>
                    <paragraph>With a multi-cell environment with multiple message queues, it is
                        likely that operators will want to configure a separate connection to
                        a unified queue for notifications. This can be done in the
                        configuration file of all nodes. See the <reference name="oslo.messaging configuration" refuri="https://docs.openstack.org/oslo.messaging/latest/configuration/opts.html#oslo_messaging_notifications.transport_url">oslo.messaging configuration</reference><target ids="oslo-messaging-configuration" names="oslo.messaging\ configuration" refuri="https://docs.openstack.org/oslo.messaging/latest/configuration/opts.html#oslo_messaging_notifications.transport_url"></target>
                        documentation for more details.</paragraph>
                </section>
                <section ids="neutron-metadata-api-proxy" names="neutron\ metadata\ api\ proxy">
                    <title>Neutron Metadata API proxy</title>
                    <paragraph>The Neutron metadata API proxy should be global across all cells, and
                        thus be configured as an API-level service with access to the
                        <literal>[api_database]/connection</literal> information.</paragraph>
                </section>
                <section ids="consoleauth-service-and-console-proxies" names="consoleauth\ service\ and\ console\ proxies">
                    <title>Consoleauth service and console proxies</title>
                    <paragraph>The consoleauth service should be global across all cells and thus be
                        configured as an API-level service with access to the
                        <literal>[api_database]/connection</literal> information. The various console proxies
                        should also be global across all cells but they don't need access to the
                        API database.</paragraph>
                    <paragraph>Future work will deprecate the consoleauth service, store token
                        authorizations in the cell databases, and require console proxies running
                        per cell instead of globally.</paragraph>
                </section>
                <section ids="operations-requiring-upcalls" names="operations\ requiring\ upcalls">
                    <title>Operations Requiring upcalls</title>
                    <paragraph>If you deploy multiple cells with a superconductor as described above,
                        computes and cell-based conductors will not have the ability to speak
                        to the scheduler as they are not connected to the same MQ. This is by
                        design for isolation, but currently the processes are not in place to
                        implement some features without such connectivity. Thus, anything that
                        requires a so-called "upcall" will not function. This impacts the
                        following:</paragraph>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>Instance reschedules during boot</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>Instance affinity reporting from the compute nodes to scheduler</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>The late anti-affinity check</paragraph>
                        </list_item>
                    </bullet_list>
                    <paragraph>The first is simple: if you boot an instance, it gets scheduled to a
                        compute node, fails, it would normally be re-scheduled to another
                        node. That requires scheduler intervention and thus it will not work
                        in Pike with a multi-cell layout. If you do not rely on reschedules
                        for covering up transient compute-node failures, then this will not
                        affect you. To ensure you do not make futile attempts at rescheduling,
                        you should set <literal>[scheduler]/max_attempts=1</literal> in <literal>nova.conf</literal>.</paragraph>
                    <paragraph>The second two are related. The summary is that some of the facilities
                        that Nova has for ensuring that affinty/anti-affinity is preserved
                        between instances does not function in Pike with a multi-cell
                        layout. If you don't use affinity operations, then this will not
                        affect you. To make sure you don't make futile attempts at the
                        affinity check, you should set
                        <literal>[workarounds]/disable_group_policy_check_upcall=True</literal> and
                        <literal>[filter_scheduler]/track_instance_changes=False</literal> in <literal>nova.conf</literal>.</paragraph>
                </section>
            </section>
        </section>
    </section>
</document>
