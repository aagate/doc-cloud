<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/nova/doc/source/admin/networking-nova.rst">
    <section ids="networking-with-nova-network" names="networking\ with\ nova-network">
        <title>Networking with nova-network</title>
        <important>
            <paragraph><literal>nova-network</literal> was deprecated in the OpenStack Newton release.  In Ocata
                and future releases, you can start <literal>nova-network</literal> only with a cells v1
                configuration. This is not a recommended configuration for deployment.</paragraph>
        </important>
        <paragraph>Understanding the networking configuration options helps you design the best
            configuration for your Compute instances.</paragraph>
        <paragraph>You can choose to either install and configure <literal>nova-network</literal> or use the
            OpenStack Networking service (neutron). This section contains a brief overview
            of <literal>nova-network</literal>. For more information about OpenStack Networking, see
            <reference name="Networking" refuri="https://docs.openstack.org/admin-guide/networking.html">Networking</reference><target ids="networking" names="networking" refuri="https://docs.openstack.org/admin-guide/networking.html"></target>.</paragraph>
        <section ids="networking-concepts" names="networking\ concepts">
            <title>Networking concepts</title>
            <paragraph>Compute assigns a private IP address to each VM instance. Compute makes a
                distinction between fixed IPs and floating IP. Fixed IPs are IP addresses that
                are assigned to an instance on creation and stay the same until the instance is
                explicitly terminated. Floating IPs are addresses that can be dynamically
                associated with an instance. A floating IP address can be disassociated and
                associated with another instance at any time. A user can reserve a floating IP
                for their project.</paragraph>
            <note>
                <paragraph>Currently, Compute with <literal>nova-network</literal> only supports Linux bridge
                    networking that allows virtual interfaces to connect to the outside network
                    through the physical interface.</paragraph>
            </note>
            <paragraph>The network controller with <literal>nova-network</literal> provides virtual networks to
                enable compute servers to interact with each other and with the public network.
                Compute with <literal>nova-network</literal> supports the following network modes, which are
                implemented as Network Manager types:</paragraph>
            <definition_list>
                <definition_list_item>
                    <term>Flat Network Manager</term>
                    <definition>
                        <paragraph>In this mode, a network administrator specifies a subnet. IP addresses for VM
                            instances are assigned from the subnet, and then injected into the image on
                            launch. Each instance receives a fixed IP address from the pool of available
                            addresses. A system administrator must create the Linux networking bridge
                            (typically named <literal>br100</literal>, although this is configurable) on the systems
                            running the <literal>nova-network</literal> service. All instances of the system are
                            attached to the same bridge, which is configured manually by the network
                            administrator.</paragraph>
                    </definition>
                </definition_list_item>
            </definition_list>
            <note>
                <paragraph>Configuration injection currently only works on Linux-style systems that
                    keep networking configuration in <literal>/etc/network/interfaces</literal>.</paragraph>
            </note>
            <definition_list>
                <definition_list_item>
                    <term>Flat DHCP Network Manager</term>
                    <definition>
                        <paragraph>In this mode, OpenStack starts a DHCP server (dnsmasq) to allocate IP
                            addresses to VM instances from the specified subnet, in addition to manually
                            configuring the networking bridge. IP addresses for VM instances are assigned
                            from a subnet specified by the network administrator.</paragraph>
                        <paragraph>Like flat mode, all instances are attached to a single bridge on the compute
                            node. Additionally, a DHCP server configures instances depending on
                            single-/multi-host mode, alongside each <literal>nova-network</literal>.  In this mode,
                            Compute does a bit more configuration. It attempts to bridge into an Ethernet
                            device (<literal>flat_interface</literal>, eth0 by default). For every instance, Compute
                            allocates a fixed IP address and configures dnsmasq with the MAC ID and IP
                            address for the VM.  Dnsmasq does not take part in the IP address allocation
                            process, it only hands out IPs according to the mapping done by Compute.
                            Instances receive their fixed IPs with the <literal_strong classes="command">dhcpdiscover</literal_strong> command.
                            These IPs are not assigned to any of the host's network interfaces, only to
                            the guest-side interface for the VM.</paragraph>
                        <paragraph>In any setup with flat networking, the hosts providing the <literal>nova-network</literal>
                            service are responsible for forwarding traffic from the private network. They
                            also run and configure dnsmasq as a DHCP server listening on this bridge,
                            usually on IP address 10.0.0.1 (see <reference internal="True" refid="compute-dnsmasq"><inline classes="std std-ref">DHCP server: dnsmasq</inline></reference>). Compute can
                            determine the NAT entries for each network, although sometimes NAT is not
                            used, such as when the network has been configured with all public IPs, or if
                            a hardware router is used (which is a high availability option). In this
                            case, hosts need to have <literal>br100</literal> configured and physically connected to any
                            other nodes that are hosting VMs. You must set the <literal>flat_network_bridge</literal>
                            option or create networks with the bridge parameter in order to avoid raising
                            an error. Compute nodes have iptables or ebtables entries created for each
                            project and instance to protect against MAC ID or IP address spoofing and ARP
                            poisoning.</paragraph>
                    </definition>
                </definition_list_item>
            </definition_list>
            <note>
                <paragraph>In single-host Flat DHCP mode you will be able to ping VMs through their
                    fixed IP from the <literal>nova-network</literal> node, but you cannot ping them from the
                    compute nodes. This is expected behavior.</paragraph>
            </note>
            <definition_list>
                <definition_list_item>
                    <term>VLAN Network Manager</term>
                    <definition>
                        <paragraph>This is the default mode for OpenStack Compute. In this mode, Compute creates
                            a VLAN and bridge for each project. For multiple-machine installations, the
                            VLAN Network Mode requires a switch that supports VLAN tagging (IEEE 802.1Q).
                            The project gets a range of private IPs that are only accessible from inside
                            the VLAN.  In order for a user to access the instances in their project, a
                            special VPN instance (code named <literal>cloudpipe</literal>) needs to be created.  Compute
                            generates a certificate and key for the user to access the VPN and starts the
                            VPN automatically. It provides a private network segment for each project's
                            instances that can be accessed through a dedicated VPN connection from the
                            internet. In this mode, each project gets its own VLAN, Linux networking
                            bridge, and subnet.</paragraph>
                        <paragraph>The subnets are specified by the network administrator, and are assigned
                            dynamically to a project when required. A DHCP server is started for each
                            VLAN to pass out IP addresses to VM instances from the subnet assigned to the
                            project. All instances belonging to one project are bridged into the same
                            VLAN for that project. OpenStack Compute creates the Linux networking bridges
                            and VLANs when required.</paragraph>
                    </definition>
                </definition_list_item>
            </definition_list>
            <paragraph>These network managers can co-exist in a cloud system. However, because you
                cannot select the type of network for a given project, you cannot configure
                multiple network types in a single Compute installation.</paragraph>
            <paragraph>All network managers configure the network using network drivers. For example,
                the Linux L3 driver (<literal>l3.py</literal> and <literal>linux_net.py</literal>), which makes use of
                <literal>iptables</literal>, <literal>route</literal> and other network management facilities, and the
                libvirt <reference name="network filtering facilities" refuri="http://libvirt.org/formatnwfilter.html">network filtering facilities</reference>. The driver is not tied to any
                particular network manager; all network managers use the same driver. The
                driver usually initializes only when the first VM lands on this host node.</paragraph>
            <paragraph>All network managers operate in either single-host or multi-host mode.  This
                choice greatly influences the network configuration. In single-host mode, a
                single <literal>nova-network</literal> service provides a default gateway for VMs and hosts a
                single DHCP server (dnsmasq). In multi-host mode, each compute node runs its
                own <literal>nova-network</literal> service. In both cases, all traffic between VMs and the
                internet flows through <literal>nova-network</literal>. Each mode has benefits and drawbacks.
                For more on this, see the Network Topology section in the <reference name="OpenStack Operations Guide" refuri="https://docs.openstack.org/ops-guide/arch-network-design.html#network-topology">OpenStack Operations
                    Guide</reference>.</paragraph>
            <paragraph>All networking options require network connectivity to be already set up
                between OpenStack physical nodes. OpenStack does not configure any physical
                network interfaces. All network managers automatically create VM virtual
                interfaces. Some network managers can also create network bridges such as
                <literal>br100</literal>.</paragraph>
            <paragraph>The internal network interface is used for communication with VMs. The
                interface should not have an IP address attached to it before OpenStack
                installation, it serves only as a fabric where the actual endpoints are VMs and
                dnsmasq. Additionally, the internal network interface must be in
                <literal>promiscuous</literal> mode, so that it can receive packets whose target MAC address
                is the guest VM, not the host.</paragraph>
            <paragraph>All machines must have a public and internal network interface (controlled by
                these options: <literal>public_interface</literal> for the public interface, and
                <literal>flat_interface</literal> and <literal>vlan_interface</literal> for the internal interface with flat
                or VLAN managers). This guide refers to the public network as the external
                network and the private network as the internal or project network.</paragraph>
            <paragraph>For flat and flat DHCP modes, use the <literal_strong classes="command">nova network-create</literal_strong> command to
                create a network:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nova network-create vmnet \
  --fixed-range-v4 10.0.0.0/16 --fixed-cidr 10.0.20.0/24 --bridge br100</literal_block>
            <paragraph>This example uses the following parameters:</paragraph>
            <definition_list>
                <definition_list_item>
                    <term><literal>--fixed-range-v4</literal></term>
                    <definition>
                        <paragraph>Specifies the network subnet.</paragraph>
                    </definition>
                </definition_list_item>
                <definition_list_item>
                    <term><literal>--fixed-cidr</literal></term>
                    <definition>
                        <paragraph>Specifies a range of fixed IP addresses to allocate, and can be a subset of
                            the <literal>--fixed-range-v4</literal> argument.</paragraph>
                    </definition>
                </definition_list_item>
                <definition_list_item>
                    <term><literal>--bridge</literal></term>
                    <definition>
                        <paragraph>Specifies the bridge device to which this network is connected on every
                            compute node.</paragraph>
                    </definition>
                </definition_list_item>
            </definition_list>
            <target refid="compute-dnsmasq"></target>
        </section>
        <section ids="dhcp-server-dnsmasq compute-dnsmasq" names="dhcp\ server:\ dnsmasq compute-dnsmasq">
            <title>DHCP server: dnsmasq</title>
            <paragraph>The Compute service uses <reference name="dnsmasq" refuri="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</reference> as the DHCP server when
                using either Flat DHCP Network Manager or VLAN Network Manager. For Compute to
                operate in IPv4/IPv6 dual-stack mode, use at least dnsmasq v2.63. The
                <literal>nova-network</literal> service is responsible for starting dnsmasq processes.</paragraph>
            <paragraph>The behavior of dnsmasq can be customized by creating a dnsmasq configuration
                file. Specify the configuration file using the <literal>dnsmasq_config_file</literal>
                configuration option:</paragraph>
            <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">dnsmasq_config_file=/etc/dnsmasq-nova.conf</literal_block>
            <paragraph>For more information about creating a dnsmasq configuration file, see the
                <reference internal="True" refuri="../configuration/config"><inline classes="doc">Configuration Options</inline></reference>, and <reference name="the dnsmasq documentation" refuri="http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq.conf.example">the dnsmasq documentation</reference>.</paragraph>
            <paragraph>Dnsmasq also acts as a caching DNS server for instances. You can specify the
                DNS server that dnsmasq uses by setting the <literal>dns_server</literal> configuration option
                in <literal>/etc/nova/nova.conf</literal>. This example configures dnsmasq to use Google's
                public DNS server:</paragraph>
            <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">dns_server=8.8.8.8</literal_block>
            <paragraph>Dnsmasq logs to syslog (typically <literal>/var/log/syslog</literal> or <literal>/var/log/messages</literal>,
                depending on Linux distribution). Logs can be useful for troubleshooting,
                especially in a situation where VM instances boot successfully but are not
                reachable over the network.</paragraph>
            <paragraph>Administrators can specify the starting point IP address to reserve with the
                DHCP server (in the format n.n.n.n) with this command:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nova-manage fixed reserve --address IP_ADDRESS</literal_block>
            <paragraph>This reservation only affects which IP address the VMs start at, not the fixed
                IP addresses that <literal>nova-network</literal> places on the bridges.</paragraph>
        </section>
        <section ids="configure-compute-to-use-ipv6-addresses" names="configure\ compute\ to\ use\ ipv6\ addresses">
            <title>Configure Compute to use IPv6 addresses</title>
            <paragraph>If you are using OpenStack Compute with <literal>nova-network</literal>, you can put Compute
                into dual-stack mode, so that it uses both IPv4 and IPv6 addresses for
                communication. In dual-stack mode, instances can acquire their IPv6 global
                unicast addresses by using a stateless address auto-configuration mechanism
                [RFC 4862/2462]. IPv4/IPv6 dual-stack mode works with both <literal>VlanManager</literal> and
                <literal>FlatDHCPManager</literal> networking modes.</paragraph>
            <paragraph>In <literal>VlanManager</literal> networking mode, each project uses a different 64-bit global
                routing prefix. In <literal>FlatDHCPManager</literal> mode, all instances use one 64-bit
                global routing prefix.</paragraph>
            <paragraph>This configuration was tested with virtual machine images that have an IPv6
                stateless address auto-configuration capability. This capability is required
                for any VM to run with an IPv6 address. You must use an EUI-64 address for
                stateless address auto-configuration. Each node that executes a <literal>nova-*</literal>
                service must have <literal>python-netaddr</literal> and <literal>radvd</literal> installed.</paragraph>
            <rubric>Switch into IPv4/IPv6 dual-stack mode</rubric>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>For every node running a <literal>nova-*</literal> service, install <literal>python-netaddr</literal>:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># apt-get install python-netaddr</literal_block>
                </list_item>
                <list_item>
                    <paragraph>For every node running <literal>nova-network</literal>, install <literal>radvd</literal> and configure
                        IPv6 networking:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># apt-get install radvd
# echo 1 &gt; /proc/sys/net/ipv6/conf/all/forwarding
# echo 0 &gt; /proc/sys/net/ipv6/conf/all/accept_ra</literal_block>
                </list_item>
                <list_item>
                    <paragraph>On all nodes, edit the <literal>nova.conf</literal> file and specify <literal>use_ipv6 = True</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Restart all <literal>nova-*</literal> services.</paragraph>
                </list_item>
            </enumerated_list>
            <rubric>IPv6 configuration options</rubric>
            <paragraph>You can use the following options with the <literal_strong classes="command">nova network-create</literal_strong>
                command:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>Add a fixed range for IPv6 addresses to the <literal_strong classes="command">nova network-create</literal_strong>
                        command. Specify <literal>public</literal> or <literal>private</literal> after the <literal>network-create</literal>
                        parameter.</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nova network-create public --fixed-range-v4 FIXED_RANGE_V4 \
  --vlan VLAN_ID --vpn VPN_START --fixed-range-v6 FIXED_RANGE_V6</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Set the IPv6 global routing prefix by using the <literal>--fixed_range_v6</literal>
                        parameter. The default value for the parameter is <literal>fd00::/48</literal>.</paragraph>
                    <paragraph>When you use <literal>FlatDHCPManager</literal>, the command uses the original
                        <literal>--fixed_range_v6</literal> value. For example:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nova network-create public  --fixed-range-v4 10.0.2.0/24 \
  --fixed-range-v6 fd00:1::/48</literal_block>
                </list_item>
                <list_item>
                    <paragraph>When you use <literal>VlanManager</literal>, the command increments the subnet ID to create
                        subnet prefixes. Guest VMs use this prefix to generate their IPv6 global
                        unicast addresses. For example:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nova network-create public --fixed-range-v4 10.0.1.0/24 --vlan 100 \
  --vpn 1000 --fixed-range-v6 fd00:1::/48</literal_block>
                </list_item>
            </bullet_list>
            <table ids="id8">
                <title>Description of IPv6 configuration options</title>
                <tgroup cols="2">
                    <colspec colwidth="50"></colspec>
                    <colspec colwidth="50"></colspec>
                    <thead>
                        <row>
                            <entry>
                                <paragraph>Configuration option = Default value</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Description</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>[DEFAULT]</paragraph>
                            </entry>
                            <entry>
                            </entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                <paragraph>fixed_range_v6 = fd00::/48</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) Fixed IPv6 address block</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>gateway_v6 = None</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) Default IPv6 gateway</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>ipv6_backend = rfc2462</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) Backend to use for IPv6 generation</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>use_ipv6 = False</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(BoolOpt) Use IPv6</paragraph>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <target refid="metadata-service"></target>
        </section>
        <section ids="metadata-service id1" names="metadata\ service metadata-service">
            <title>Metadata service</title>
            <paragraph>Compute uses a metadata service for virtual machine instances to retrieve
                instance-specific data. Instances access the metadata service at
                <literal>http://169.254.169.254</literal>. The metadata service supports two sets of APIs: an
                OpenStack metadata API and an EC2-compatible API. Both APIs are versioned by
                date.</paragraph>
            <paragraph>To retrieve a list of supported versions for the OpenStack metadata API, make a
                GET request to <literal>http://169.254.169.254/openstack</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/openstack
2012-08-10
2013-04-04
2013-10-17
latest</literal_block>
            <paragraph>To list supported versions for the EC2-compatible metadata API, make a GET
                request to <literal>http://169.254.169.254</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254
1.0
2007-01-19
2007-03-01
2007-08-29
2007-10-10
2007-12-15
2008-02-01
2008-09-01
2009-04-04
latest</literal_block>
            <paragraph>If you write a consumer for one of these APIs, always attempt to access the
                most recent API version supported by your consumer first, then fall back to an
                earlier version if the most recent one is not available.</paragraph>
            <paragraph>Metadata from the OpenStack API is distributed in JSON format. To retrieve the
                metadata, make a GET request to
                <literal>http://169.254.169.254/openstack/2012-08-10/meta_data.json</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/openstack/2012-08-10/meta_data.json</literal_block>
            <literal_block highlight_args="{}" language="json" linenos="False" xml:space="preserve">{
   "uuid": "d8e02d56-2648-49a3-bf97-6be8f1204f38",
   "availability_zone": "nova",
   "hostname": "test.novalocal",
   "launch_index": 0,
   "meta": {
      "priority": "low",
      "role": "webserver"
   },
   "project_id": "f7ac731cc11f40efbc03a9f9e1d1d21f",
   "public_keys": {
       "mykey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKV\
                 VRNCRX6BlnNbI+USLGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTH\
                 bsiyPCIDOKyeHba4MUJq8Oh5b2i71/3BISpyxTBH/uZDHdslW2a+SrPDCe\
                 uMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated by Nova\n"
   },
   "name": "test"
}</literal_block>
            <paragraph>Instances also retrieve user data (passed as the <literal>user_data</literal> parameter in the
                API call or by the <literal>--user_data</literal> flag in the <literal_strong classes="command">openstack server
                    create</literal_strong> command) through the metadata service, by making a GET request to
                <literal>http://169.254.169.254/openstack/2012-08-10/user_data</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/openstack/2012-08-10/user_data
#!/bin/bash
echo 'Extra user data here'</literal_block>
            <paragraph>The metadata service has an API that is compatible with version 2009-04-04 of
                the <reference name="Amazon EC2 metadata service" refuri="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html">Amazon EC2 metadata service</reference>.
                This means that virtual machine images designed for EC2 will work properly with
                OpenStack.</paragraph>
            <paragraph>The EC2 API exposes a separate URL for each metadata element. Retrieve a
                listing of these elements by making a GET query to
                <literal>http://169.254.169.254/2009-04-04/meta-data/</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/2009-04-04/meta-data/
ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
hostname
instance-action
instance-id
instance-type
kernel-id
local-hostname
local-ipv4
placement/
public-hostname
public-ipv4
public-keys/
ramdisk-id
reservation-id
security-groups</literal_block>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/2009-04-04/meta-data/block-device-mapping/
ami</literal_block>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/2009-04-04/meta-data/placement/
availability-zone</literal_block>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/2009-04-04/meta-data/public-keys/
0=mykey</literal_block>
            <paragraph>Instances can retrieve the public SSH key (identified by keypair name when a
                user requests a new instance) by making a GET request to
                <literal>http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKVVRNCRX6BlnNbI+US\
LGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTHbsiyPCIDOKyeHba4MUJq8Oh5b2i71/3B\
ISpyxTBH/uZDHdslW2a+SrPDCeuMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated\
by Nova</literal_block>
            <paragraph>Instances can retrieve user data by making a GET request to
                <literal>http://169.254.169.254/2009-04-04/user-data</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ curl http://169.254.169.254/2009-04-04/user-data
#!/bin/bash
echo 'Extra user data here'</literal_block>
            <paragraph>The metadata service is implemented by either the <literal>nova-api</literal> service or the
                <literal>nova-api-metadata</literal> service. Note that the <literal>nova-api-metadata</literal> service is
                generally only used when running in multi-host mode, as it retrieves
                instance-specific metadata. If you are running the <literal>nova-api</literal> service, you
                must have <literal>metadata</literal> as one of the elements listed in the <literal>enabled_apis</literal>
                configuration option in <literal>/etc/nova/nova.conf</literal>. The default <literal>enabled_apis</literal>
                configuration setting includes the metadata service, so you do not need to
                modify it.</paragraph>
            <paragraph>Hosts access the service at <literal>169.254.169.254:80</literal>, and this is translated to
                <literal>metadata_host:metadata_port</literal> by an iptables rule established by the
                <literal>nova-network</literal> service. In multi-host mode, you can set <literal>metadata_host</literal> to
                <literal>127.0.0.1</literal>.</paragraph>
            <paragraph>For instances to reach the metadata service, the <literal>nova-network</literal> service must
                configure iptables to NAT port <literal>80</literal> of the <literal>169.254.169.254</literal> address to the
                IP address specified in <literal>metadata_host</literal> (this defaults to <literal>$my_ip</literal>, which
                is the IP address of the <literal>nova-network</literal> service) and port specified in
                <literal>metadata_port</literal> (which defaults to <literal>8775</literal>) in <literal>/etc/nova/nova.conf</literal>.</paragraph>
            <note>
                <paragraph>The <literal>metadata_host</literal> configuration option must be an IP address, not a host
                    name.</paragraph>
            </note>
            <paragraph>The default Compute service settings assume that <literal>nova-network</literal> and
                <literal>nova-api</literal> are running on the same host. If this is not the case, in the
                <literal>/etc/nova/nova.conf</literal> file on the host running <literal>nova-network</literal>, set the
                <literal>metadata_host</literal> configuration option to the IP address of the host where
                <literal>nova-api</literal> is running.</paragraph>
            <table ids="id9">
                <title>Description of metadata configuration options</title>
                <tgroup cols="2">
                    <colspec colwidth="50"></colspec>
                    <colspec colwidth="50"></colspec>
                    <thead>
                        <row>
                            <entry>
                                <paragraph>Configuration option = Default value</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Description</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>[DEFAULT]</paragraph>
                            </entry>
                            <entry>
                            </entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                <paragraph>metadata_cache_expiration = 15</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(IntOpt) Time in seconds to cache metadata; 0 to disable metadata
                                    caching entirely (not recommended). Increasing this should improve
                                    response times of the metadata API when under heavy load. Higher values
                                    may increase memory usage and result in longer times for host metadata
                                    changes to take effect.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>metadata_host = $my_ip</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) The IP address for the metadata API server</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>metadata_listen = 0.0.0.0</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) The IP address on which the metadata API will listen.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>metadata_listen_port = 8775</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(IntOpt) The port on which the metadata API will listen.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>metadata_manager = nova.api.manager.MetadataManager</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) OpenStack metadata service manager</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>metadata_port = 8775</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(IntOpt) The port for the metadata API port</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>metadata_workers = None</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(IntOpt) Number of workers for metadata service. The default will be the number of CPUs available.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>vendordata_driver = nova.api.metadata.vendordata_json.JsonFileVendorData</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) Driver to use for vendor data</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>vendordata_jsonfile_path = None</paragraph>
                            </entry>
                            <entry>
                                <paragraph>(StrOpt) File to load JSON formatted vendor data from</paragraph>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </section>
        <section ids="enable-ping-and-ssh-on-vms" names="enable\ ping\ and\ ssh\ on\ vms">
            <title>Enable ping and SSH on VMs</title>
            <paragraph>You need to enable <literal>ping</literal> and <literal>ssh</literal> on your VMs for network access.  This
                can be done with either the <literal_strong classes="command">nova</literal_strong> or <literal_strong classes="command">euca2ools</literal_strong> commands.</paragraph>
            <note>
                <paragraph>Run these commands as root only if the credentials used to interact with
                    <literal>nova-api</literal> are in <literal>/root/.bashrc</literal>. If the EC2 credentials in the
                    <literal>.bashrc</literal> file are for an unprivileged user, you must run these commands
                    as that user instead.</paragraph>
            </note>
            <paragraph>Enable ping and SSH with <literal_strong classes="command">openstack security group rule create</literal_strong>
                commands:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack security group rule create --protocol icmp default
$ openstack security group rule create --protocol tcp --dst-port 22:22 default</literal_block>
            <paragraph>Enable ping and SSH with <literal>euca2ools</literal>:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default
$ euca-authorize -P tcp -p 22 -s 0.0.0.0/0 default</literal_block>
            <paragraph>If you have run these commands and still cannot ping or SSH your instances,
                check the number of running <literal>dnsmasq</literal> processes, there should be two. If not,
                kill the processes and restart the service with these commands:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># killall dnsmasq
# service nova-network restart</literal_block>
        </section>
        <section ids="configure-public-floating-ip-addresses" names="configure\ public\ (floating)\ ip\ addresses">
            <title>Configure public (floating) IP addresses</title>
            <paragraph>This section describes how to configure floating IP addresses with
                <literal>nova-network</literal>. For information about doing this with OpenStack Networking,
                see <reference name="L3-routing-and-NAT" refuri="https://docs.openstack.org/neutron/latest/admin/archives/adv-features.html#l3-routing-and-nat">L3-routing-and-NAT</reference><target ids="l3-routing-and-nat" names="l3-routing-and-nat" refuri="https://docs.openstack.org/neutron/latest/admin/archives/adv-features.html#l3-routing-and-nat"></target>.</paragraph>
            <section ids="private-and-public-ip-addresses" names="private\ and\ public\ ip\ addresses">
                <title>Private and public IP addresses</title>
                <paragraph>In this section, the term floating IP address is used to refer to an IP
                    address, usually public, that you can dynamically add to a running virtual
                    instance.</paragraph>
                <paragraph>Every virtual instance is automatically assigned a private IP address.  You can
                    choose to assign a public (or floating) IP address instead.  OpenStack Compute
                    uses network address translation (NAT) to assign floating IPs to virtual
                    instances.</paragraph>
                <paragraph>To be able to assign a floating IP address, edit the <literal>/etc/nova/nova.conf</literal>
                    file to specify which interface the <literal>nova-network</literal> service should bind public
                    IP addresses to:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">public_interface=VLAN100</literal_block>
                <paragraph>If you make changes to the <literal>/etc/nova/nova.conf</literal> file while the
                    <literal>nova-network</literal> service is running, you will need to restart the service to
                    pick up the changes.</paragraph>
                <note>
                    <paragraph>Floating IPs are implemented by using a source NAT (SNAT rule in iptables),
                        so security groups can sometimes display inconsistent behavior if VMs use
                        their floating IP to communicate with other VMs, particularly on the same
                        physical host. Traffic from VM to VM across the fixed network does not have
                        this issue, and so this is the recommended setup. To ensure that traffic
                        does not get SNATed to the floating range, explicitly set:</paragraph>
                    <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">dmz_cidr=x.x.x.x/y</literal_block>
                    <paragraph>The <literal>x.x.x.x/y</literal> value specifies the range of floating IPs for each pool of
                        floating IPs that you define. This configuration is also required if the VMs
                        in the source group have floating IPs.</paragraph>
                </note>
            </section>
            <section ids="enable-ip-forwarding" names="enable\ ip\ forwarding">
                <title>Enable IP forwarding</title>
                <paragraph>IP forwarding is disabled by default on most Linux distributions. You will need
                    to enable it in order to use floating IPs.</paragraph>
                <note>
                    <paragraph>IP forwarding only needs to be enabled on the nodes that run
                        <literal>nova-network</literal>. However, you will need to enable it on all compute nodes
                        if you use <literal>multi_host</literal> mode.</paragraph>
                </note>
                <paragraph>To check if IP forwarding is enabled, run:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cat /proc/sys/net/ipv4/ip_forward
0</literal_block>
                <paragraph>Alternatively, run:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 0</literal_block>
                <paragraph>In these examples, IP forwarding is disabled.</paragraph>
                <paragraph>To enable IP forwarding dynamically, run:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># sysctl -w net.ipv4.ip_forward=1</literal_block>
                <paragraph>Alternatively, run:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># echo 1 &gt; /proc/sys/net/ipv4/ip_forward</literal_block>
                <paragraph>To make the changes permanent, edit the <literal>/etc/sysctl.conf</literal> file and update
                    the IP forwarding setting:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">net.ipv4.ip_forward = 1</literal_block>
                <paragraph>Save the file and run this command to apply the changes:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># sysctl -p</literal_block>
                <paragraph>You can also apply the changes by restarting the network service:</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>on Ubuntu, Debian:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># /etc/init.d/networking restart</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>on RHEL, Fedora, CentOS, openSUSE and SLES:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># service network restart</literal_block>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="create-a-list-of-available-floating-ip-addresses" names="create\ a\ list\ of\ available\ floating\ ip\ addresses">
                <title>Create a list of available floating IP addresses</title>
                <paragraph>Compute maintains a list of floating IP addresses that are available for
                    assigning to instances. Use the <literal_strong classes="command">nova-manage floating</literal_strong> commands to
                    perform floating IP operations:</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Add entries to the list:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># nova-manage floating create --pool nova --ip_range 68.99.26.170/31</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>List the floating IP addresses in the pool:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># openstack floating ip list</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create specific floating IPs for either a single address or a subnet:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># nova-manage floating create --pool POOL_NAME --ip_range CIDR</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Remove floating IP addresses using the same parameters as the create command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># openstack floating ip delete CIDR</literal_block>
                    </list_item>
                </bullet_list>
                <paragraph>For more information about how administrators can associate floating IPs with
                    instances, see <reference name="ip floating" refuri="https://docs.openstack.org/python-openstackclient/latest/cli/command-objects/ip-floating.html">ip floating</reference> in the python-openstackclient User Documentation.</paragraph>
            </section>
            <section ids="automatically-add-floating-ips" names="automatically\ add\ floating\ ips">
                <title>Automatically add floating IPs</title>
                <paragraph>You can configure <literal>nova-network</literal> to automatically allocate and assign a
                    floating IP address to virtual instances when they are launched. Add this line
                    to the <literal>/etc/nova/nova.conf</literal> file:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">auto_assign_floating_ip=True</literal_block>
                <paragraph>Save the file, and restart <literal>nova-network</literal></paragraph>
                <note>
                    <paragraph>If this option is enabled, but all floating IP addresses have already been
                        allocated, the <literal_strong classes="command">openstack server create</literal_strong> command will fail.</paragraph>
                </note>
            </section>
        </section>
        <section ids="remove-a-network-from-a-project" names="remove\ a\ network\ from\ a\ project">
            <title>Remove a network from a project</title>
            <paragraph>You cannot delete a network that has been associated to a project. This section
                describes the procedure for dissociating it so that it can be deleted.</paragraph>
            <paragraph>In order to disassociate the network, you will need the ID of the project it
                has been associated to. To get the project ID, you will need to be an
                administrator.</paragraph>
            <paragraph>Disassociate the network from the project using the <literal_strong classes="command">nova-manage
                    project scrub</literal_strong> command, with the project ID as the final parameter:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># nova-manage project scrub --project ID</literal_block>
        </section>
        <section ids="multiple-interfaces-for-instances-multinic" names="multiple\ interfaces\ for\ instances\ (multinic)">
            <title>Multiple interfaces for instances (multinic)</title>
            <paragraph>The multinic feature allows you to use more than one interface with your
                instances. This is useful in several scenarios:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph>SSL Configurations (VIPs)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Services failover/HA</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Bandwidth Allocation</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Administrative/Public access to your instances</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>Each VIP represents a separate network with its own IP block. Every network
                mode has its own set of changes regarding multinic usage:</paragraph>
            <figure>
                <image candidates="{'*': u'admin/figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg'}" uri="admin/figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg" width="600"></image>
            </figure>
            <figure>
                <image candidates="{'*': u'admin/figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg'}" uri="admin/figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg" width="600"></image>
            </figure>
            <figure>
                <image candidates="{'*': u'admin/figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg'}" uri="admin/figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg" width="600"></image>
            </figure>
            <section ids="using-multinic" names="using\ multinic">
                <title>Using multinic</title>
                <paragraph>In order to use multinic, create two networks, and attach them to the project
                    (named <literal>project</literal> on the command line):</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nova network-create first-net --fixed-range-v4 20.20.0.0/24 --project-id $your-project
$ nova network-create second-net --fixed-range-v4 20.20.10.0/24 --project-id $your-project</literal_block>
                <paragraph>Each new instance will now receive two IP addresses from their respective DHCP
                    servers:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server list
+---------+----------+--------+-----------------------------------------+------------+
|ID       | Name     | Status | Networks                                | Image Name |
+---------+----------+--------+-----------------------------------------+------------+
| 1234... | MyServer | ACTIVE | network2=20.20.0.3; private=20.20.10.14 | cirros     |
+---------+----------+--------+-----------------------------------------+------------+</literal_block>
                <note>
                    <paragraph>Make sure you start the second interface on the instance, or it won't be
                        reachable through the second IP.</paragraph>
                </note>
                <paragraph>This example demonstrates how to set up the interfaces within the instance.
                    This is the configuration that needs to be applied inside the image.</paragraph>
                <paragraph>Edit the <literal>/etc/network/interfaces</literal> file:</paragraph>
                <literal_block highlight_args="{}" language="bash" linenos="False" xml:space="preserve"># The loopback network interface
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet dhcp</literal_block>
                <paragraph>If the Virtual Network Service Neutron is installed, you can specify the
                    networks to attach to the interfaces by using the <literal>--nic</literal> flag with the
                    <literal_strong classes="command">openstack server create</literal_strong> command:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server create --image ed8b2a37-5535-4a5f-a615-443513036d71 \
  --flavor 1 --nic net-id=NETWORK1_ID --nic net-id=NETWORK2_ID test-vm1</literal_block>
            </section>
        </section>
        <section ids="troubleshooting-networking" names="troubleshooting\ networking">
            <title>Troubleshooting Networking</title>
            <section ids="cannot-reach-floating-ips" names="cannot\ reach\ floating\ ips">
                <title>Cannot reach floating IPs</title>
            </section>
            <section dupnames="problem" ids="problem">
                <title>Problem</title>
                <paragraph>You cannot reach your instances through the floating IP address.</paragraph>
            </section>
            <section dupnames="solution" ids="solution">
                <title>Solution</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Check that the default security group allows ICMP (ping) and SSH (port 22),
                            so that you can reach the instances:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack security group rule list default
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 22:22           | None                  |
| e9d3200f-647a-4293-a9fc-e65ceee189ae | icmp        | 0.0.0.0/0 | type=1:code=-1  | None                  |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Check the NAT rules have been added to iptables on the node that is running
                            <literal>nova-network</literal>:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># iptables -L -nv -t nat \
    -A nova-network-PREROUTING -d 68.99.26.170/32 -j DNAT --to-destination 10.0.0.3 \
    -A nova-network-floating-snat -s 10.0.0.3/32 -j SNAT --to-source 68.99.26.170</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Check that the public address (<literal>68.99.26.170</literal> in this example), has been
                            added to your public interface. You should see the address in the listing
                            when you use the <literal_strong classes="command">ip addr</literal_strong> command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ ip addr
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
link/ether xx:xx:xx:17:4b:c2 brd ff:ff:ff:ff:ff:ff
inet 13.22.194.80/24 brd 13.22.194.255 scope global eth0
inet 68.99.26.170/32 scope global eth0
inet6 fe80::82b:2bf:fe1:4b2/64 scope link
valid_lft forever preferred_lft forever</literal_block>
                        <note>
                            <paragraph>You cannot use <literal>SSH</literal> to access an instance with a public IP from within
                                the same server because the routing configuration does not allow it.</paragraph>
                        </note>
                    </list_item>
                    <list_item>
                        <paragraph>Use <literal>tcpdump</literal> to identify if packets are being routed to the inbound
                            interface on the compute host. If the packets are reaching the compute hosts
                            but the connection is failing, the issue may be that the packet is being
                            dropped by reverse path filtering. Try disabling reverse-path filtering on
                            the inbound interface. For example, if the inbound interface is <literal>eth2</literal>,
                            run:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># sysctl -w net.ipv4.conf.ETH2.rp_filter=0</literal_block>
                        <paragraph>If this solves the problem, add the following line to <literal>/etc/sysctl.conf</literal> so
                            that the reverse-path filter is persistent:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">net.ipv4.conf.rp_filter=0</literal_block>
                    </list_item>
                </bullet_list>
            </section>
            <section ids="temporarily-disable-firewall" names="temporarily\ disable\ firewall">
                <title>Temporarily disable firewall</title>
            </section>
            <section dupnames="problem" ids="id2">
                <title>Problem</title>
                <paragraph>Networking issues prevent administrators accessing or reaching VM's through
                    various pathways.</paragraph>
            </section>
            <section dupnames="solution" ids="id3">
                <title>Solution</title>
                <paragraph>You can disable the firewall by setting this option in <literal>/etc/nova/nova.conf</literal>:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">firewall_driver=nova.virt.firewall.NoopFirewallDriver</literal_block>
                <warning>
                    <paragraph>We strongly recommend you remove this line to re-enable the firewall once
                        your networking issues have been resolved.</paragraph>
                </warning>
            </section>
            <section ids="packet-loss-from-instances-to-nova-network-server-vlanmanager-mode" names="packet\ loss\ from\ instances\ to\ nova-network\ server\ (vlanmanager\ mode)">
                <title>Packet loss from instances to nova-network server (VLANManager mode)</title>
            </section>
            <section dupnames="problem" ids="id4">
                <title>Problem</title>
                <paragraph>If you can access your instances with <literal>SSH</literal> but the network to your instance
                    is slow, or if you find that running certain operations are slower than they
                    should be (for example, <literal>sudo</literal>), packet loss could be occurring on the
                    connection to the instance.</paragraph>
                <paragraph>Packet loss can be caused by Linux networking configuration settings related to
                    bridges. Certain settings can cause packets to be dropped between the VLAN
                    interface (for example, <literal>vlan100</literal>) and the associated bridge interface (for
                    example, <literal>br100</literal>) on the host running <literal>nova-network</literal>.</paragraph>
            </section>
            <section dupnames="solution" ids="id5">
                <title>Solution</title>
                <paragraph>One way to check whether this is the problem is to open three terminals and run
                    the following commands:</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>In the first terminal, on the host running <literal>nova-network</literal>, use <literal>tcpdump</literal>
                            on the VLAN interface to monitor DNS-related traffic (UDP, port 53). As
                            root, run:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># tcpdump -K -p -i vlan100 -v -vv udp port 53</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>In the second terminal, also on the host running <literal>nova-network</literal>, use
                            <literal>tcpdump</literal> to monitor DNS-related traffic on the bridge interface.  As
                            root, run:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># tcpdump -K -p -i br100 -v -vv udp port 53</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>In the third terminal, use <literal>SSH</literal> to access the instance and generate DNS
                            requests by using the <literal_strong classes="command">nslookup</literal_strong> command:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ nslookup www.google.com</literal_block>
                        <paragraph>The symptoms may be intermittent, so try running <literal_strong classes="command">nslookup</literal_strong>
                            multiple times. If the network configuration is correct, the command should
                            return immediately each time. If it is not correct, the command hangs for
                            several seconds before returning.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>If the <literal_strong classes="command">nslookup</literal_strong> command sometimes hangs, and there are packets
                            that appear in the first terminal but not the second, then the problem may
                            be due to filtering done on the bridges. Try disabling filtering, and
                            running these commands as root:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># sysctl -w net.bridge.bridge-nf-call-arptables=0
# sysctl -w net.bridge.bridge-nf-call-iptables=0
# sysctl -w net.bridge.bridge-nf-call-ip6tables=0</literal_block>
                        <paragraph>If this solves your issue, add the following line to <literal>/etc/sysctl.conf</literal> so
                            that the changes are persistent:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">net.bridge.bridge-nf-call-arptables=0
net.bridge.bridge-nf-call-iptables=0
net.bridge.bridge-nf-call-ip6tables=0</literal_block>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="kvm-network-connectivity-works-initially-then-fails" names="kvm:\ network\ connectivity\ works\ initially,\ then\ fails">
                <title>KVM: Network connectivity works initially, then fails</title>
            </section>
            <section dupnames="problem" ids="id6">
                <title>Problem</title>
                <paragraph>With KVM hypervisors, instances running Ubuntu 12.04 sometimes lose network
                    connectivity after functioning properly for a period of time.</paragraph>
            </section>
            <section dupnames="solution" ids="id7">
                <title>Solution</title>
                <paragraph>Try loading the <literal>vhost_net</literal> kernel module as a workaround for this issue (see
                    <reference name="bug #997978" refuri="https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/997978/">bug #997978</reference>) . This
                    kernel module may also <reference name="improve network performance" refuri="http://www.linux-kvm.org/page/VhostNet">improve network performance</reference> on KVM. To load the kernel module:</paragraph>
                <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># modprobe vhost_net</literal_block>
                <note>
                    <paragraph>Loading the module has no effect on running instances.</paragraph>
                </note>
            </section>
        </section>
    </section>
</document>
