<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/nova/doc/source/reference/scheduler-evolution.rst">
    <comment xml:space="preserve">Licensed under the Apache License, Version 2.0 (the "License"); you may
not use this file except in compliance with the License. You may obtain
a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.</comment>
    <section ids="scheduler-evolution" names="scheduler\ evolution">
        <title>Scheduler Evolution</title>
        <paragraph>Evolving the scheduler has been a priority item over several
            releases: <reference refuri="http://specs.openstack.org/openstack/nova-specs/#priorities">http://specs.openstack.org/openstack/nova-specs/#priorities</reference></paragraph>
        <paragraph>The scheduler has become tightly coupled with the rest of nova,
            limiting its capabilities, accuracy, flexibility and maintainability.
            The goal of scheduler evolution is to bring about a better separation of
            concerns between scheduling functionality and the rest of nova.</paragraph>
        <paragraph>Once this effort has completed, its conceivable that the nova-scheduler could
            become a separate git repo, outside of nova but within the compute project.
            This is not the current focus.</paragraph>
        <section ids="problem-use-cases" names="problem\ use\ cases">
            <title>Problem Use Cases</title>
            <paragraph>Many users are wanting to do more advanced things with the scheduler, but the
                current architecture is not ready to support those use cases in a maintainable way.
                A few examples will help to illustrate where the scheduler falls
                short:</paragraph>
            <section ids="cross-project-affinity" names="cross\ project\ affinity">
                <title>Cross Project Affinity</title>
                <paragraph>It can be desirable, when booting from a volume, to use a compute node
                    that is close to the shared storage where that volume is. Similarly, for
                    the sake of performance, it can be desirable to use a compute node that
                    is in a particular location in relation to a pre-created port.</paragraph>
            </section>
            <section ids="accessing-aggregates-in-filters-and-weights" names="accessing\ aggregates\ in\ filters\ and\ weights">
                <title>Accessing Aggregates in Filters and Weights</title>
                <paragraph>Any DB access in a filter or weight slows down the scheduler. Until the
                    end of kilo, there was no way to deal with the scheduler accessing
                    information about aggregates without querying the DB in every call to
                    host_passes() in a filter.</paragraph>
            </section>
            <section ids="filter-scheduler-alternatives" names="filter\ scheduler\ alternatives">
                <title>Filter Scheduler Alternatives</title>
                <paragraph>For certain use cases, radically different schedulers may perform much better
                    than the filter scheduler. We should not block this innovation. It is
                    unreasonable to assume a single scheduler will work for all use cases.</paragraph>
                <paragraph>However, to enable this kind of innovation in a maintainable way, a
                    single strong scheduler interface is required.</paragraph>
            </section>
            <section ids="project-scale-issues" names="project\ scale\ issues">
                <title>Project Scale issues</title>
                <paragraph>There are many interesting ideas for new schedulers, like the solver scheduler,
                    and frequent requests to add new filters and weights to the scheduling system.
                    The current nova team does not have the bandwidth to deal with all these
                    requests. A dedicated scheduler team could work on these items independently
                    of the rest of nova.</paragraph>
                <paragraph>The tight coupling that currently exists makes it impossible to work
                    on the scheduler in isolation. A stable interface is required before
                    the code can be split out.</paragraph>
            </section>
        </section>
        <section ids="key-areas-we-are-evolving" names="key\ areas\ we\ are\ evolving">
            <title>Key areas we are evolving</title>
            <paragraph>Here we discuss, at a high level, areas that are being addressed as part of
                the scheduler evolution work.</paragraph>
            <section ids="fixing-the-scheduler-db-model" names="fixing\ the\ scheduler\ db\ model">
                <title>Fixing the Scheduler DB model</title>
                <paragraph>We need the nova and scheduler data models to be independent of each other.</paragraph>
                <paragraph>The first step is breaking the link between the ComputeNode and Service
                    DB tables. In theory where the Service information is stored should be
                    pluggable through the service group API, and should be independent of the
                    scheduler service. For example, it could be managed via zookeeper rather
                    than polling the nova DB.</paragraph>
                <paragraph>There are also places where filters and weights call into the nova DB to
                    find out information about aggregates. This needs to be sent to the
                    scheduler, rather than reading directly from the nova database.</paragraph>
            </section>
            <section ids="versioning-scheduler-placement-interfaces" names="versioning\ scheduler\ placement\ interfaces">
                <title>Versioning Scheduler Placement Interfaces</title>
                <paragraph>At the start of kilo, the scheduler is passed a set of dictionaries across
                    a versioned RPC interface. The dictionaries can create problems with the
                    backwards compatibility needed for live-upgrades.</paragraph>
                <paragraph>Luckily we already have the oslo.versionedobjects infrastructure we can use
                    to model this data in a way that can be versioned across releases.</paragraph>
                <paragraph>This effort is mostly focusing around the request_spec. See, for
                    example, <reference name="this spec" refuri="http://specs.openstack.org/openstack/nova-specs/specs/kilo/approved/sched-select-destinations-use-request-spec-object.html">this spec</reference>.</paragraph>
            </section>
            <section ids="sending-host-and-node-stats-to-the-scheduler" names="sending\ host\ and\ node\ stats\ to\ the\ scheduler">
                <title>Sending host and node stats to the scheduler</title>
                <paragraph>Periodically nova-compute updates the scheduler state stored in
                    the database.</paragraph>
                <paragraph>We need a good way to model the data that is being sent from the compute
                    nodes into the scheduler, so over time, the scheduler can move to having
                    its own database.</paragraph>
                <paragraph>This is linked to the work on the resource tracker.</paragraph>
            </section>
            <section ids="updating-the-scheduler-about-other-data" names="updating\ the\ scheduler\ about\ other\ data">
                <title>Updating the Scheduler about other data</title>
                <paragraph>For things like host aggregates, we need the scheduler to cache information
                    about those, and know when there are changes so it can update its cache.</paragraph>
                <paragraph>Over time, its possible that we need to send cinder and neutron data, so
                    the scheduler can use that data to help pick a nova-compute host.</paragraph>
            </section>
            <section ids="resource-tracker" names="resource\ tracker">
                <title>Resource Tracker</title>
                <paragraph>The recent work to add support for NUMA and PCI pass through have shown we
                    have no good pattern to extend the resource tracker. Ideally we want to keep
                    the innovation inside the nova tree, but we also need it to be easier.</paragraph>
                <paragraph>This is very related to the effort to re-think how we model resources, as
                    covered by discussion about <reference name="resource providers" refuri="https://blueprints.launchpad.net/nova/+spec/resource-providers">resource providers</reference>.</paragraph>
            </section>
            <section ids="parallelism-and-concurrency" names="parallelism\ and\ concurrency">
                <title>Parallelism and Concurrency</title>
                <paragraph>The current design of the nova-scheduler is very racy, and can lead to
                    excessive numbers of build retries before the correct host is found. The
                    recent NUMA features are particularly impacted by how the scheduler
                    works. All this has lead to many people running only a single
                    nova-scheduler process configured to use a very small greenthread pool.</paragraph>
                <paragraph>The work on cells v2 will mean that we soon need the scheduler to scale for
                    much larger problems. The current scheduler works best with less than 1k nodes
                    but we will need the scheduler to work with at least 10k nodes.</paragraph>
                <paragraph>Various ideas have been discussed to reduce races when running multiple
                    nova-scheduler processes. One idea is to use two-phase commit "style"
                    resource tracker claims. Another idea involves using incremental updates
                    so it is more efficient to keep the scheduler's state up to date,
                    potentially using Kafka.</paragraph>
                <paragraph>For more details, see the <reference name="backlog spec" refuri="http://specs.openstack.org/openstack/nova-specs/specs/backlog/approved/parallel-scheduler.html">backlog spec</reference> that describes more of the details
                    around this problem.</paragraph>
                <target ids="this-spec" names="this\ spec" refuri="http://specs.openstack.org/openstack/nova-specs/specs/kilo/approved/sched-select-destinations-use-request-spec-object.html"></target>
                <target ids="resource-providers" names="resource\ providers" refuri="https://blueprints.launchpad.net/nova/+spec/resource-providers"></target>
                <target ids="backlog-spec" names="backlog\ spec" refuri="http://specs.openstack.org/openstack/nova-specs/specs/backlog/approved/parallel-scheduler.html"></target>
            </section>
        </section>
    </section>
</document>
